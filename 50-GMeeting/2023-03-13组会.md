---
Date: 2023-03-13
Presenter: 张若彤, 应涵倩
DESC: 知识蒸馏；FFNet;
---
tags:  #组会 

## 分享一
***
### Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer

  [论文原文](https://arxiv.org/abs/1612.03928)


### Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks

  ICLR: [论文原文](https://arxiv.org/pdf/2101.05930.pdf)

  后门攻击：对训练的数据扰乱——作为学生模型
  后门攻击后的网络在干净数据集上训练后， fituning 之后——作为老师模型
  


## 分享二
***
### FFNet: Simple and Efficient Architectures for Semantic Segmentation
  
  [论文原文](https://arxiv.org/abs/2206.08236)
  
  语义分割，只用卷积层，encoder+decoder，没有多尺度特征融合网络
  预训练：backbone 在 imagenet 上训练，然后后面那个部分从头开始训练
  容易部署到移动端


## 分享三：19 级毕设
***

## Transformer

《transformer is all your need》

#### contribution
1. 并行度更高：RNN 是时序，输入 t 输出 h(t)
2. 训练时间更少
3. 可以用于其他 task，但是本文是用于机器翻译

  具体看[[Attention is All you Need]]