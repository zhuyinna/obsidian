# 一面
- RCNN系列的发展过程
  - RCNN(Regions with CNN features): 提出了region proposal的概念，通过selective search提取候选区域，然后用CNN提取特征，最后用SVM分类
    - 什么是Selective Search: 一种region proposal的方法，通过合并相似的区域来生成候选区域
    - SVM: 支持向量机，用于分类: 通过找到一个超平面，将数据分为两类, 使得两类数据之间的间隔最大
  - Fast RCNN: 通过RoI pooling将region proposal和CNN特征提取合并到一起，提高了速度
    - RoI pooling: 将不同大小的RoI映射到固定大小的特征图上，保证了后续全连接层的输入大小一致
    - RoI: Region of Interest, 一种region proposal的方法
  - Faster RCNN: 提出了RPN(Region Proposal Network), 用于生成region proposal, 使得整个网络可以端到端训练
    - RPN: 通过anchor生成region proposal, 通过分类和回归两个分支来预测region proposal的类别和位置
  - Mask RCNN: 在Faster RCNN的基础上增加了mask分支，用于预测物体的mask
    - mask分支: 通过RoIAlign将RoI映射到固定大小的特征图上，然后通过卷积层预测mask
    - RoIAlign: 与RoI pooling类似，但是更精确，通过双线性插值来保证映射后的特征图的信息不丢失

- Anchor Free是什么意思？
  - Anchor Free是一种目标检测的方法，与Anchor-based方法相对应
  - Anchor-based方法: 通过预定义的一些anchor来预测目标的位置和类别, 例如Faster RCNN, SSD, YOLO
  - Anchor Free方法: 不需要预定义的anchor, 直接通过网络来预测目标的位置和类别, 例如CenterNet, CenterNet2, CornerNet, 从而避免了anchor的设计和调参, 以及锚框引起的复杂性和重叠问题
- SSD的特点了解吗？
  - SSD(Single Shot MultiBox Detector): 一种目标检测的方法
  - 特点:
    - 多尺度特征图: 通过多层特征图来检测不同大小的目标
    - 多长宽比的密集锚点: 通过不同长宽比的锚点来检测不同形状的目标
    - 特征金字塔: 通过不同层的特征图来检测不同大小的目标, 底层特征图用于检测小目标, 高层特征图用于检测大目标
- PR曲线怎么画？F-score是什么？mAP怎么求？
  - PR曲线: Precision-Recall curve, 用于评估二分类器的性能
    - 横轴: Recall, 纵轴: Precision
    - 通过改变分类器的阈值, 绘制不同的PR曲线
  - F-score: 综合考虑Precision和Recall的指标, 一般是Precision和Recall的调和平均
    - F1-score: Precision和Recall的调和平均, F1 = 2 * Precision * Recall / (Precision + Recall)
  - mAP: mean Average Precision, 用于评估目标检测的性能
    - 先计算每个类别的AP(Average Precision), 然后对所有类别的AP取平均
    - AP: Precision-Recall曲线下的面积, 用于评估目标检测的准确率和召回率
- 说下你了解的激活函数及它们的优缺点？
  - 激活函数: 用于引入非线性因素, 使得神经网络可以拟合非线性函数
  - Sigmoid: 将输入映射到(0, 1)之间, 用于二分类问题
    - 优点: 输出范围有限, 易于求导
    - 缺点: 容易出现梯度消失, 输出不是以0为中心, 导致梯度更新不稳定
  - Relu: f(x) = max(0, x), 用于解决梯度消失问题
    - 优点: 计算简单, 收敛速度快
    - 缺点: Dead Relu问题, x < 0时梯度为0, 导致神经元无法更新
  - Leaky Relu: f(x) = max(0.01x, x), 解决Dead Relu问题
    - 优点: 解决了Dead Relu问题
    - 缺点: 需要调整斜率, 不是自适应的
  - Tanh: 将输入映射到(-1, 1)之间
    - 优点: 输出以0为中心, 易于求导
    - 缺点: 梯度消失问题
  - Softmax: 将输入映射到(0, 1)之间, 用于多分类问题
    - 优点: 输出概率分布, 易于求导, 适用于多分类问题的输出层
    - 缺点: 梯度消失问题
- BN 在做什么？有什么作用？
  - BN(Batch Normalization): 用于加速神经网络的训练, 提高模型的收敛速度和稳定性
  - 作用:
    - 加速收敛: 通过将每个batch的数据进行归一化, 使得每层的输入分布稳定, 从而加速收敛
    - 减少梯度消失: 通过将每层的输入归一化, 避免了梯度消失问题
    - 正则化: 通过对每个batch的数据进行归一化, 使得模型对输入数据的变化更加稳定, 从而起到正则化的作用
- 缓解过拟合的常见方法
  - Dropout: 随机丢弃一些神经元, 避免模型对某些特征过于依赖
  - 正则化: L1正则化, L2正则化, 通过对模型的参数进行惩罚, 避免模型过拟合
  - 数据增强: 通过对训练数据进行旋转, 翻转, 剪裁等操作, 增加训练数据的多样性
  - 早停: 在验证集上监控模型的性能, 当性能开始下降时停止训练
  - Batch Normalization: 通过对每个batch的数据进行归一化, 避免模型对输入数据的变化过于敏感
- Linux管道
  - Linux管道: 用于将一个命令的输出作为另一个命令的输入, 通过管道符`|`连接多个命令, 例如`ls | grep "txt"`
- git合并分支操作
  - `git checkout master`
  - `git merge dev`
- coding: 3sum, 二叉树非递归中序遍历

```python
# 3sum： 找出数组中三个数之和为0的所有组合， 不重复
# 思路： 先排序，然后遍历数组，固定一个数，然后使用双指针找另外两个数
```

```python
# 二叉树非递归中序遍历
# 思路： 使用栈来模拟递归过程
# 将根节点入栈，然后将左子树入栈，直到左子树为空，然后出栈，访问节点，然后将右子树入栈，重复上述过程
```


# 二面

- 模型转换? ncnn框架?
  - 模型转换是指将一个深度学习框架训练的模型转换为另一个深度学习框架可用的模型. 这通常涉及从原始训练框架（如 TensorFlow 或 PyTorch）导出到一个通用格式（如 ONNX），然后将其转换为目标框架的格式（如 TensorFlow Lite, CoreML, 或 NCNN）。
  - NCNN 是一个为移动端优化的高性能神经网络前向计算框架，由腾讯优图实验室开源。它特别适用于移动端设备，因为它轻量级，不依赖第三方库，支持多平台，并且提供了针对 ARM CPU 优化的速度。
- baseline选择?
  - baseline 选择 VGG16, 是一个经典的卷积神经网络模型，由于其简单的网络结构和较好的性能，适合用作基准模型。此外，VGG16 在 ImageNet 数据集上取得了不错的性能，因此也适合用于图像分类任务。选择其他模型比如ResNet或者EfficientNet, 取决于性能需求和计算资源
  - Loss设计: 应该与项目的目标和数据集有关，比如对于分类任务，可以使用交叉熵损失函数，对于回归任务，可以使用均方误差损失函数
    - 分类任务和回归任务的区别: 分类任务是将输入映射到离散的类别，回归任务是将输入映射到连续的数值
  - 离线数据增强: 数据增强是一种常用的数据预处理技术，通过对训练数据进行旋转、翻转、剪裁等操作，增加训练数据的多样性，从而提高模型的泛化能力。离线数据增强是指在训练之前对数据进行增强，而不是在训练过程中动态增强数据
    - ps: 训练中动态增强数据指的是 在每个epoch中对数据进行增强，比如随机旋转、翻转等
- 年龄预测网络的输出是什么? MAE怎么计算的?
  - 年龄预测网络的输出通常是一个数值，表示预测的年龄。可以是一个连续值，也可以是一个离散值。
  - 平均绝对误差MAE（Mean Absolute Error）是一种常用的回归任务的评价指标，用于衡量预测值与真实值之间的平均绝对误差。计算公式如下：
    - MAE = 1/n * Σ|y_pred - y_true|
    - 其中，y_pred 是模型的预测值，y_true 是真实值，n 是样本数量
    - 通常应用于回归任务，值越小表示模型的预测越准确
- 数据不平衡的处理方法?
  - 数据不平衡是指在训练数据中不同类别的样本数量差异较大，这可能导致模型对少数类别的预测性能较差。常见的处理方法包括：
    - 过采样（Oversampling）：增加少数类别的样本数量，使得不同类别的样本数量接近
    - 欠采样（Undersampling）：减少多数类别的样本数量，使得不同类别的样本数量接近
    - 权重调整（Weighted Loss）：调整损失函数中不同类别的权重，使得模型更关注少数类别的样本
    - 生成新样本（Synthetic Data Generation）：通过生成新的合成样本来平衡数据集，比如SMOTE算法
    - 集成学习（Ensemble Learning）：通过组合多个模型的预测结果来提高性能，可以减少不平衡数据的影响
- 深度学习中常用的trick
  - BN: 提高训练稳定性和加速收敛
  - Dropout或者其他正则化技术来防止过拟合
  - 学习率调度: 衰减/预热
    - 学习率预热: 在训练开始时，先使用较小的学习率，然后逐渐增加学习率，以加速收敛. 为什么? 避免模型在训练初期因为学习率过大导致的不稳定性
    - 学习率衰减: 在训练过程中逐渐减小学习率，以提高模型的泛化能力. 避免模型在训练后期因为学习率过大导致的震荡
  - 梯度裁剪: 防止梯度爆炸
- SGD和Adam
  - SGD随机梯度下降: 是一种基本的优化算法，每次更新参数时只考虑一个样本的梯度，速度较慢，容易陷入局部最优解.
  - Adam: 是一种自适应学习率的优化算法，结合了动量和自适应学习率的优点，可以加速收敛，适用于大规模数据和复杂模型的训练.
  - 举例来说, 训练就是在下山, SGD就是一个盲人下山, 虽然能够到山底, 但是可能会走很多弯路, 有点事

了解模型转换吗？ncnn框架了解吗？
baseline怎么选的？Loss怎么设计的？为什么要做离线数据增强？
年龄预测网络的输出是什么？MAE怎么计算的？
问比赛：
baseline选的VGG16？为什么要选这个？有没有尝试其它的？
borderline smote具体怎么实现的？
还有没有其它方法缓解数据不平衡？
基础知识：
深度学习中常见的trick你知道哪些？
SGD和Adam介绍一下？你认为哪个好，为什么？
Pytorch 里面的 Batch Normalization 的底层实现
Pytorch 里面的通道 Concat 怎么实现？
开放题：如何看待CV行业的内卷