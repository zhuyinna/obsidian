- RCNN系列的发展过程
  - RCNN(Regions with CNN features): 提出了region proposal的概念，通过selective search提取候选区域，然后用CNN提取特征，最后用SVM分类
    - 什么是Selective Search: 一种region proposal的方法，通过合并相似的区域来生成候选区域
    - SVM: 支持向量机，用于分类: 通过找到一个超平面，将数据分为两类, 使得两类数据之间的间隔最大
  - Fast RCNN: 通过RoI pooling将region proposal和CNN特征提取合并到一起，提高了速度
    - RoI pooling: 将不同大小的RoI映射到固定大小的特征图上，保证了后续全连接层的输入大小一致
    - RoI: Region of Interest, 一种region proposal的方法
  - Faster RCNN: 提出了RPN(Region Proposal Network), 用于生成region proposal, 使得整个网络可以端到端训练
    - RPN: 通过anchor生成region proposal, 通过分类和回归两个分支来预测region proposal的类别和位置
  - Mask RCNN: 在Faster RCNN的基础上增加了mask分支，用于预测物体的mask
    - mask分支: 通过RoIAlign将RoI映射到固定大小的特征图上，然后通过卷积层预测mask
    - RoIAlign: 与RoI pooling类似，但是更精确，通过双线性插值来保证映射后的特征图的信息不丢失

- Anchor Free是什么意思？
  - Anchor Free是一种目标检测的方法，与Anchor-based方法相对应
  - Anchor-based方法: 通过预定义的一些anchor来预测目标的位置和类别, 例如Faster RCNN, SSD, YOLO
  - Anchor Free方法: 不需要预定义的anchor, 直接通过网络来预测目标的位置和类别, 例如CenterNet, CenterNet2, CornerNet, 从而避免了anchor的设计和调参, 以及锚框引起的复杂性和重叠问题
- SSD的特点了解吗？
  - SSD(Single Shot MultiBox Detector): 一种目标检测的方法
  - 特点:
    - 多尺度特征图: 通过多层特征图来检测不同大小的目标
    - 多长宽比的密集锚点: 通过不同长宽比的锚点来检测不同形状的目标
    - 特征金字塔: 通过不同层的特征图来检测不同大小的目标, 底层特征图用于检测小目标, 高层特征图用于检测大目标
- PR曲线怎么画？F-score是什么？mAP怎么求？
  - PR曲线: Precision-Recall curve, 用于评估二分类器的性能
    - 横轴: Recall, 纵轴: Precision
    - 通过改变分类器的阈值, 绘制不同的PR曲线
  - F-score: 综合考虑Precision和Recall的指标, 一般是Precision和Recall的调和平均
    - F1-score: Precision和Recall的调和平均, F1 = 2 * Precision * Recall / (Precision + Recall)
  - mAP: mean Average Precision, 用于评估目标检测的性能
    - 先计算每个类别的AP(Average Precision), 然后对所有类别的AP取平均
    - AP: Precision-Recall曲线下的面积, 用于评估目标检测的准确率和召回率
- 说下你了解的激活函数及它们的优缺点？
  - 激活函数: 用于引入非线性因素, 使得神经网络可以拟合非线性函数
  - Sigmoid: 将输入映射到(0, 1)之间, 用于二分类问题
    - 优点: 输出范围有限, 易于求导
    - 缺点: 容易出现梯度消失, 输出不是以0为中心, 导致梯度更新不稳定
  - Relu: f(x) = max(0, x), 用于解决梯度消失问题
    - 优点: 计算简单, 收敛速度快
    - 缺点: Dead Relu问题, x < 0时梯度为0, 导致神经元无法更新
  - Leaky Relu: f(x) = max(0.01x, x), 解决Dead Relu问题
    - 优点: 解决了Dead Relu问题
    - 缺点: 需要调整斜率, 不是自适应的
  - Tanh: 将输入映射到(-1, 1)之间
    - 优点: 输出以0为中心, 易于求导
    - 缺点: 梯度消失问题
  - Softmax: 将输入映射到(0, 1)之间, 用于多分类问题
    - 优点: 输出概率分布, 易于求导, 适用于多分类问题的输出层
    - 缺点: 梯度消失问题
- BN 在做什么？有什么作用？
  - BN(Batch Normalization): 用于加速神经网络的训练, 提高模型的收敛速度和稳定性
  - 作用:
    - 加速收敛: 通过将每个batch的数据进行归一化, 使得每层的输入分布稳定, 从而加速收敛
    - 减少梯度消失: 通过将每层的输入归一化, 避免了梯度消失问题
    - 正则化: 通过对每个batch的数据进行归一化, 使得模型对输入数据的变化更加稳定, 从而起到正则化的作用
- 缓解过拟合的常见方法
  - Dropout: 随机丢弃一些神经元, 避免模型对某些特征过于依赖
  - 正则化: L1正则化, L2正则化, 通过对模型的参数进行惩罚, 避免模型过拟合
  - 数据增强: 通过对训练数据进行旋转, 翻转, 剪裁等操作, 增加训练数据的多样性
  - 早停: 在验证集上监控模型的性能, 当性能开始下降时停止训练
  - Batch Normalization: 通过对每个batch的数据进行归一化, 避免模型对输入数据的变化过于敏感
- Linux管道
  - Linux管道: 用于将一个命令的输出作为另一个命令的输入, 通过管道符`|`连接多个命令, 例如`ls | grep "txt"`
- git合并分支操作
  - `git checkout master`
  - `git merge dev`
- coding: 3sum, 二叉树非递归中序遍历

```python
# 3sum： 找出数组中三个数之和为0的所有组合， 不重复
# 思路： 


```

```python
# 二叉树非递归中序遍历
```