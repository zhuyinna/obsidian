# 一面

- 问实习做的项目以及具体的细节（搭了哪些backbone，有没有用迁移学习，MAE怎么计算...）
  - 可能会使用不同的backbone网络，例如VGG、ResNet、Inception等，作为模型的特征提取部分。迁移学习可能会被用来利用预训练模型在大型数据集（如ImageNet）上学到的知识，以加速训练过程并提高模型的泛化能力。MAE（平均绝对误差）通常是通过计算预测值和真实值之间差的绝对值的平均来计算的。
- 问自己的小项目的细节（数据合成怎么做的，用了哪些数据增强，了解mixup吗...）
  - 数据合成可能涉及使用图像合成技术来生成训练数据，例如通过将前景对象叠加到不同的背景中。数据增强可能包括随机裁剪、旋转、缩放、颜色变换等技术。Mixup是一种数据增强技术，它通过在样本对之间进行线性插值来创建新样本。
  - Mixup代码实现:
    ```python
    def mixup_data(x, y, alpha=1.0, use_cuda=True):
        if alpha > 0:
            lam = np.random.beta(alpha, alpha)
        else:
            lam = 1
        batch_size = x.size()[0]
        if use_cuda:
            index = torch.randperm(batch_size).cuda()
        else:
            index = torch.randperm(batch_size)
        mixed_x = lam * x + (1 - lam) * x[index, :]  # 对输入数据进行线性插值
        y_a, y_b = y, y[index]
        return mixed_x, y_a, y_b, lam
    ```
- detection的经典的baseline了解哪些，说一说？
  - R-CNN系列: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN
  - YOLO系列: YOLOv1, YOLOv2, YOLOv3, YOLOv4
  - SSD: Single Shot MultiBox Detector
- Mask RCNN 如何解决 unalignment?
  - 通过引入一个额外的分支来预测目标的mask，ROI Align通过双线性插值精确地对齐感兴趣区域（RoI）和特征图，避免了ROI Pooling中由于量化而导致的不精确对齐。
  > ROI pooling是将RoI映射到固定大小的特征图上，但由于量化的原因，可能导致RoI和特征图之间的不精确对齐，从而影响了mask的精度。ROI Align通过双线性插值来解决这个问题，保证了RoI和特征图之间的精确对齐。
- RFCN里的 position sensitive score map 的原理？解决了什么问题？
  - RFCN(Region-based Fully Convolutional Networks)通过将RoI划分为多个位置敏感区域，然后对每个区域进行分类和回归，从而提高了目标检测的精度。这种方法可以更好地对齐目标的位置，提高了检测的准确性。
- 说说 Anchor-based 和 Anchor Free 的区别
  - Anchor-based方法（如Faster R-CNN）依赖于预定义的锚框来预测目标的位置，而Anchor Free方法（如CenterNet、FCOS）直接在特征图上预测目标的位置和大小，不依赖于预定义的锚框。
  - Anchor-based方法更快, 但是需要设计和调参锚框, 容易出现重叠问题; Anchor Free方法不需要设计锚框, 直接预测目标的位置和大小, 但是速度较慢.
- ROC 和 AUC
  - ROC(Receiver Operating Characteristic)曲线是用于评估**二分类器的性能**，横轴是假阳性率（False Positive Rate），纵轴是真阳性率（True Positive Rate）。AUC（Area Under Curve）是ROC曲线下的面积，用于衡量分类器的性能，AUC值越大，分类器的性能越好。
- mAP 和 AUC 各自的优缺点
  - mAP（mean Average Precision）是目标检测任务中常用的评价指标，它综合考虑了不同类别的AP（Average Precision），可以更全面地评估模型的性能。AUC（Area Under Curve）是用于评估二分类器的性能，它只考虑了真阳性率和假阳性率，不能直接用于目标检测任务。

# 二面

- 你说你实习用过ShuffleNet，那ShuffleNet-v1,v2各有什么优点，motivation是什么？
  - ShuffleNet-v1
    - 通过Channel Shuffle来增加通道之间的信息交流，利用分组卷积减少参数量和计算量，提高模型的效率。
    >分组卷积: 将输入通道分成多个组，每个组进行卷积操作，然后将结果拼接在一起。这样可以减少参数量和计算量，提高模型的效率。
    - motivation: 减少参数量和计算量，提高模型的效率。
  - ShuffleNet-v2
    - 在v1的基础上引入了瓶颈结构，先降维再升维，进一步减少了计算量，提高了模型的性能, 减少了内存访问成本(MAC)
    - motivatin: 进一步减少计算量，提高模型的性能。
- Channel Shuffle Pytorch里面是怎么实现的？如果用 Tensorflow 呢？
```python
def channel_shuffle(x, groups):
    batchsize, num_channels, height, width = x.data.size()
    channels_per_group = num_channels // groups
    # reshape
    x = x.view(batchsize, groups, channels_per_group, height, width)
    x = torch.transpose(x, 1, 2).contiguous()  
    # flatten
    x = x.view(batchsize, -1, height, width)
    return x
```
```python
def channel_shuffle(x, groups):
    batchsize, height, width, num_channels = x.shape.as_list()
    channels_per_group = num_channels // groups
    # reshape
    x = tf.reshape(x, [-1, height, width, groups, channels_per_group])
    x = tf.transpose(x, [0, 1, 2, 4, 3])
    # flatten
    x = tf.reshape(x, [-1, height, width, num_channels])
```
> transpose操作是将张量的维度进行交换, 通过transpose操作可以实现张量的维度变换, 从而实现通道之间的交换。

- 你还用过Mobilenet，问个最简单的，Pytorch里怎么实现DwConv？
  - Depthwise Convolution（深度卷积）是MobileNet中的一个重要组件，它通过对每个输入通道进行单独的卷积操作，然后将结果进行通道拼接，从而减少参数量和计算量。
  - 实现思路: 先对输入通道进行分组，然后对每个分组进行卷积操作，最后将结果拼接在一起。
  ```python
  def dw_conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1):
      return nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, out_channels, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
      )
  ```
- 现在有一 NCHW 的 feature map，经过一个3*3的DwConv，计算量和参数量怎么算？
  - 参数量: N * C * K * K
  - 计算量: N * C * H * W * K * K
- v2的瓶颈块里为什么要先升维再降维？这不是增加了计算量吗？
  - 先升维再降维的设计是为了在保持轻量级的同时增加网络的表达能力，同时通过分组卷积和channel shuffle减少计算量的增加，并优化内存访问成本。
- 说说 v3 里的 h-swish比普通的swish激活函数好在哪里？通道注意力机制的原理？
  - h-swish: 是对swish激活函数的硬件友好近似，它通过使用ReLU6和线性函数的组合来近似swish，减少计算复杂度，提高运行效率。
  - 通道注意力机制: 通过学习每个通道的重要性，动态调整通道的特征响应，增强重要特征，抑制不重要特征，常见于SENet等网络结构中。
- FPN的上采样怎么做的？会带来什么后果？模型中是怎么解决的？
  - FPN(全称: Feature Pyramid Networks)通过自顶向下和自底向上的特征融合来生成多尺度的特征图，通过直接加法或双线性插值的方式进行上采样，提高了模型的分辨率。带来的后果: 可能会导致特征图的分辨率不够高，影响模型的检测性能。解决方法: 通过引入更多的特征金字塔层来提高特征图的分辨率，提高模型的检测性能。
- FCN的上采样又是怎么做的？你认为双线性插值和反卷积哪个效果会更好？
  - FCN(全卷积网络)中的上采样是通过反卷积操作来实现的，反卷积操作可以将特征图的尺寸恢复到原始大小，提高了模型的分辨率。
  - 双线性插值和反卷积各有优缺点，双线性插值简单高效，但可能会导致特征图的模糊，反卷积操作可以学习上采样的参数，提高了特征图的质量，但计算量较大。
- SSD的基本原理讲一讲？Anchor怎么选的？NMS怎么做的？
  - single shot multibox detector(SSD)是一种目标检测算法，通过在不同尺度和长宽比的特征图上预测目标的位置和类别，实现了端到端的目标检测。 结合了不同尺度的特征图，通过预测不同尺度的锚框来检测目标。
- U-NET的特征融合怎么做的？
  - 通过跳跃连接, 将编码器和解码器的特征图进行拼接, 从而实现特征融合, 提高了模型的性能。增强了网络对细节的捕获能力, 适合图像分割
- Linux cat 命令的作用？
  - cat用于查看, 创建和连接文件的内容, 常用于查看
- Docker磁盘映射的参数是什么？
  - docker使用-v或者--volume参数来进行磁盘映射, 例如`docker run -v /host/path:/container/path`, 来将宿主机的目录或文件映射到容器底部
- 给一个特定的任务，针对这个任务设计Loss，说说思路？
  - 针对不同的任务可以设计不同的损失函数，例如交叉熵损失用于分类任务，均方误差用于回归任务，Dice Loss用于分割任务等。设计损失函数的思路可以根据任务的特点和目标来确定，例如对于不平衡数据可以使用加权损失，对于多任务学习可以设计多任务损失函数等。
  - 考虑任务的特性, 比如是否需要平衡类别,是否关注边界框的精确度,  常见的做法包括结合多个loss, 引入注意力机制, 使用自适应权重等
- 现有数以十万计的两个图片文件夹，其中一个被另一个包含，最快找出两个文件夹的差集，说说思路？
  - Tip: comm命令可以用来比较两个文件的差异, -3参数可以输出两个文件的差集
```bash
# 生成文件列表
# 排序文件列表： 更快
# 比较文件列表
find /path/to/folderA -type f -exec basename {} \; | sort > listA.txt
find /path/to/folderB -type f -exec basename {} \; | sort > listB.txt
comm -3 listA.txt listB.txt > diff.txt
```
> -exec参数用于执行指定的命令, {}表示查找到的文件, \;表示命令结束
> 为什么\;而不是直接; ? 因为;是shell的保留字符, 需要转义

# coding
#TODO: 完善代码
coding: 一面
1. 连续子数组最大和
- 思路1: 暴力, 罗列出所有子数组
- 思路2: 动态规划: dp[i] 表示 nums[i]结尾的最大子数组和  dp[i] = max(dp[i-1] + nums[i], nums[i])
  更新dp[i]的时候, 需要用ans记录当前之前的dp[0..i]的最大值
- 思路3: 贪心: 
```cpp
int sum = nums[0];
for ( int i=1; i<nums.size(); i++ ) {
  if ( sum < 0 ) {
    sum = nums[i];
  } else {
    sum += nums[i];
  }
  ans = max( ans, sum );
}
return ans;

```
<img src=https://s2.loli.net/2024/05/21/693XUN5RSxI7taH.png width='100%'>
2. 计算IOU

coding: 二面
1. 反转链表, 不用递归
- 递归
```cpp
reverseList(TreeNode* root)
```
- 迭代
2. TopK，面试官说写一个解决办法就行，然后让我讲我知道的思路
反问
