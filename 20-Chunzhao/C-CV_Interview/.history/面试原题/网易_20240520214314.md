
- 你这个borderline smote + mixup 怎么做的？
  - Borderline SMOTE：这是一种过采样技术，用于处理不平衡数据集。它特别关注那些在决策边界附近的少数类样本，并只对这些样本进行SMOTE(Synthetic Minority Over-Sampling Technique, 合成少数过采样技术)
  - Mixup：这是一种数据增强技术，通过线性地混合两个样本及其标签来生成新的训练样本。
  - 结合这两种方法可以生成新的样本，既考虑到了类别平衡也增加了数据多样性。
- 为啥要对近邻数据做类别判断？
  - 对近邻数据进行类别判断通常是在执行**基于近邻的分类或数据增强技术**时进行的。例如，在SMOTE（Synthetic Minority Over-sampling Technique）算法中，对近邻的类别进行判断是为了只对少数类样本进行过采样。这样做可以帮助改善分类器在不平衡数据集上的性能，通过生成接近决策边界的合成样本来提高分类器的泛化能力。
- ResNet有什么优点？解决了什么问题？
  - 残差结构: 环节梯度消失和梯度爆炸, 增加网络深度
- Inception-v1的创新点？v3中3*3卷积分解怎么做的？
  - 引入Inception模块,它允许网络在同一个层级上学习多种尺度的特征。
  - Inception-v3中的创新点包括将较大的卷积核（如3x3）分解成较小的卷积（如1x3和3x1），这样做可以减少参数数量，提高网络效率。
- 3D卷积
  - 3D卷积通常用于处理视频或体积数据，它可以同时捕获空间和时间维度上的特征。
- Batch Normalization的具体过程？为什么要有两个可学习参数？
  - 目标: 加速深度网络训练
  - **它通过规范化层的输入来减少内部协变量偏移。具体过程包括计算批次数据的均值和方差，然后使用这些统计数据来规范化数据，最后通过可学习的参数γ（尺度）和β（偏移）来恢复和保持网络的表达能力。**
- 说说它和其他几种Normalization的区别以及各自的应用场景？
  - Batch Normalization：适用于批量数据处理，但在小批量数据上可能效果不佳。
  - Layer Normalization：不依赖于批次大小，适用于RNN和transformer
  - Instance Normalization：常用于风格迁移任务。
  - Group Normalization：在批次大小很小的情况下是一个好的选择
- Anchor Free的目标检测网络有哪些？具体说说各自的基本原理？
    Anchor Free目标检测网络是一类不依赖于预定义锚框的目标检测方法。典型的Anchor Free网络包括：
  - **CenterNet**：基于关键点检测的方法，它检测目标的中心点，并预测每个目标的宽度和高度。
  - **FCOS (Fully Convolutional One-Stage Object Detection)**：直接在特征图上预测每个像素点是否为目标的中心，并为每个中心点预测边界框的大小。
- 目标检测领域的常见Loss？
  - 分类loss: 交叉熵，用于目标检测中的分类任务。
  - 定位loss: Smooth L1 Loss, 用于回归任务, 即预测目标的边界框。
  - IoU loss: 用于优化预测边界框与真实边界框之间的交并比（Intersection over Union）
- 讲讲逻辑回归的思路
  - 逻辑回归是一种统计模型，用于二分类问题。它通过使用逻辑函数（Sigmoid函数）将线性回归的输出映射到概率空间（0到1之间），然后基于阈值进行分类。
- smooth L1 loss 的优点？
  - Smooth L1 Loss结合了L1 Loss和L2 Loss的优点：在误差较小时表现得像L2 Loss，减少抖动，在误差较大时表现得像L1 Loss，对异常值不敏感。
  > 为什么L1 loss对异常值不敏感? L1 loss是绝对值损失，它对异常值的影响较小，因为异常值的绝对值较大，不会对损失函数产生较大的影响。 为什么L2 loss对异常值敏感? L2 loss是平方损失，它对异常值的影响较大，因为异常值的平方值较大，会对损失函数产生较大的影响。
- IoU Loss的表达式？有什么缺点？如何改进？
  - $IoU Loss=1-IoU$，其中IoU是预测边界框和真实边界框之间的交并比。
  - 缺点：IoU Loss不是可导的，不适合直接用于优化。 且当IoU接近1时, 即预测和真实没有重叠时, 梯度为0, 无法进行优化。
- 讲讲SVD的过程
  - SVD（奇异值分解）是一种矩阵分解方法，将一个矩阵分解为三个矩阵的乘积：$A=UΣV^T$，其中U和V是正交矩阵，Σ是对角矩阵。SVD可以用于降维、矩阵逆、矩阵近似等。
- SVD怎么用到PCA降维里面？
  - PCA（主成分分析）是一种常用的降维方法，它通过SVD来实现。具体来说，PCA通过计算数据的协方差矩阵，然后对协方差矩阵进行SVD分解，得到特征向量和特征值，从而实现数据的降维。从协方差矩阵中提取主成分, 这些主成分是数据变异性最大的方向，可以用于降低数据的维度。
- PCA降维的过程，和 LDA 有什么联系和区别，应用场景分别是什么？
  - PCA降维的过程包括计算数据的协方差矩阵，提取主成分，并将数据投影到这些成分上。LDA（线性判别分析）则旨在找到**最大化类间分离的特征子空间**。PCA主要用于降维和数据压缩，而LDA用于分类。
- one-stage和two-stage的区别，以及各自的典型网络有哪些？
  - One-stage目标检测方法直接在特征图上预测目标的位置和类别，如YOLO、SSD, 速度较快, 但精度低。Two-stage方法则先生成候选框，再对候选框进行分类和回归，如Faster R-CNN, 精度高速度慢.
- FCOS 的模型结构和基本原理？
  - FCOS（Fully Convolutional One-Stage Object Detection）是一种Anchor Free目标检测方法，它直接在特征图上预测每个像素点是否为目标的中心，并为每个中心点预测边界框的大小。FCOS通过特征金字塔网络（FPN）来提取多尺度特征，然后通过分类头、回归头和中心点头来预测目标的类别、位置和中心点。
- CV三大领域的SOTA算法
  - 图像分类: EfficientNet, ResNeXt
  - 目标检测: YOLOv4, EfficientDet, Faster



coding
# TODO
1. 合并k个有序链表，保持合并后的链表有序，要求最低的时间复杂度内完成（写得很艰难）
```cpp
// 定义一个priority queue用来保存所有头节点
struct ListNode {
    int val;
    ListNode* next;
    ListNode(int x) : val(x), next(nullptr) {}
}
ListNode* sortKLists (vector<ListNode*>& lists){
    // 自定义排序函数： 优先队列中较小的节点优先
    auto cmp = [](ListNode* a, ListNode* b) {
        return a->val > b->val;
    }
    priority_queue<ListNode*, vector<ListNode*>, decltype(cmp)> pq;
    // 把所有头节点加入
    for ( auto list : lists ) {
        if ( list ) pq.push(list);
    }
    ListNode dummy(0); // 哑节点
    ListNode *tail = &dummy;
    while ( !pq.empty() ) {
        tail->next = pq.top();
        pq.pop();
        tail = tail->next;

        if ( tail->next ) {
            pq.push(tail->next);
        }
    }
    return dummy.next;
}
```
2. 数组中只有两个数出现了一次，其它数都出现两次，常数空间复杂度内找出这两个数


