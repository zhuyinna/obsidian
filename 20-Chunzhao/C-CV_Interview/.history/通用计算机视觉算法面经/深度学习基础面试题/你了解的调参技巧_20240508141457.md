简述一下你常用的网络模型训练技巧？
1. 使用更大的 batch size。使用更大的 batch size 可以加快训练的进度。但是对于凸优化问题，收敛速度会随着 batch size 的增加而降低。所以在相同的 epoch 下，使用更大的 batch size 可能会导致验证集的 acc更低。所以可以使用以下技巧来解决问题。
    > 什么是凸优化问题? 
    凸优化问题是指目标函数是凸函数，约束条件是凸集的优化问题。凸优化问题有很多优良的性质，比如局部最优解就是全局最优解，凸优化问题的解是唯一的等等。凸优化问题是数学规划中的一个重要分支，也是机器学习中的一个重要分支。

    > 有哪些典型的凸优化问题? 
    凸优化问题有很多典型的问题，比如线性规划、二次规划、半定规划、凸二次规划、凸规划、非线性规划等等。

- (1)  linear scaling learning rate。使用更大的学习率，例如，当我们选择初始学习率为 0.1， batch size 为 256，当将 batch size 增大至 b 时，需要将初始学习率增加至 0.1 * b / 256

- (2) learning rate warm up。选择前 $\mathrm{n}$ 个 epoch 进行 warm up, 在这 $\mathrm{n}$ 个 epoch 中线性地增加学习率至初始学习率, 在正常地进行 decay。
    > 为什么要进行warm up? 
    在训练的开始阶段，模型的参数是随机初始化的，此时模型的输出是不稳定的，可能会导致梯度爆炸或者梯度消失。为了避免这种情况，可以在训练的开始阶段，使用较小的学习率，逐渐增大学习率，直到达到初始学习率。

- (3) zero $\gamma$ 。在 residual block 中的 $\mathrm{BN}$ 中, 首先进行标准化输入 $\mathrm{x}$, 得 到 $\hat{x}$, 再进行线性变化: $\gamma \hat{x}+\beta$, 其中 $\gamma$ 和 $\beta$ 都是可以学习的参数, 其 值被初始化为 1 和 0 , 而在这里, $\gamma$ 被初始化为 0 。
    > 为什么$\gamma$被初始化为0?
    在训练的开始阶段，模型的参数是随机初始化的，此时模型的输出是不稳定的，可能会导致梯度爆炸或者梯度消失。为了避免这种情况，可以在训练的开始阶段，将$\gamma$初始化为0，这样可以避免梯度爆炸或者梯度消失。
- (4) no bias decay。为了避免过拟合, 对于权重 weight 和 bias, 通常会 使用 weight decay。但是在这里, 仅对 weight 使用 decay, 而不对 bias 进行 decay。
    > 什么是weight decay? 
    weight decay是一种正则化方法，通过在损失函数中添加一个正则项，来减小模型的复杂度，从而避免过拟合。weight decay的目的是为了使模型的参数更加平滑，从而提高模型的泛化能力。其等价于$L_2$范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使得学出的模型参数较小，通常接近于0。

1. 使用更低的数值精度。
2. cosine learning rate decay 。将学习率随着 epoch 的增大而不断衰减。
$$\eta_{t}=\frac{1}{2}\left(1+\cos \left(\frac{t \pi}{T}\right)\right) \eta$$

> 为什么又有学习率warm up, 又有随着epoch增大而衰减?在训练的开始阶段，模型的参数是随机初始化的，此时模型的输出是不稳定的，可能会导致梯度爆炸或者梯度消失。为了避免这种情况，可以在训练的开始阶段，使用较小的学习率，逐渐增大学习率，直到达到初始学习率。而随着epoch的增大，模型的参数会逐渐稳定，此时可以逐渐减小学习率，以提高模型的泛化能力。
1. label smoothing。
    > 什么是label smoothing?
    神经网络会促使自身往正确标签和错误标签差值最大的方向学习，在训练数据较少，不足以表征所有的样本特征的情况下，会导致网络过拟合。
    label smoothing可以解决上述问题，这是一种正则化策略，主要是通过soft one-hot来加入噪声，减少了真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。
<img src=https://s2.loli.net/2024/05/08/JfVuSWQ2EmCqdKl.png width='40
0%'>

2. knowledge distillation。
3. mixup training, cutout, random erase, data augmentation 等数据增强 方法。

