## 原因
没有对数据进行归一化
忘记检查输入和输出
没有对数据进行预处理
没有对数据正则化
使用过大的样本
使用不正确的学习率
在输出层使用错误的激活函数
网络中包含坏梯度
初始化权重错误
过深的网络
隐藏单元数量错误

对应的解决办法分别是：

1. 对数据进行归一化，常用的归一化包括**零均值归一化**和**线性函数归一化**方法；
    > 原因: 这是由于不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。
    此外，大部分神经网络流程都假设输入输出是在0附近的分布，从权值初始化到激活函数、从训练到训练网络的优化算法。
2. 检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；
    > 有时候损失函数的明显减小并不意味着训练工作的完成, 在你的代码中，在数据预处理、训练代码等都有可能出现错误，而损失函数的下降并不意味着网络学习到了有用的东西. 这是由于与传统的编程不同，机器学习对于某些错误不能够及时有效的反馈错误信息，以便我们回过头来对代码BUG进行检查。
3. 对数据进行预处理，包括做一些简单的转换；
4. 采用正则化方法，比如 L2 正则，或者 dropout；
5. 在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。
6. **不采用梯度裁剪**。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。
7. 如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；
8. 如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。
9. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
10. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
11. 从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字