## 目标

加速推理

很多深度模型采用BN层（Batch Normalization）被很多深度模型来提升泛化能力。在模型推理时，BN层要从训练状态切换到测试状态，此时采用模型训练中近似的均值和方差。BN层最酷的地方是它可以用一个1x1卷积等效替换，更进一步地，我们可以将BN层合并到前面的卷积层中。

融合之后, 推理结果相同,但是时间减少.

## Conv与BN的融合过程

卷积层的计算公式为$Y=WX + B$,假设batchnorm的均值与方差分别的是$\mu$和 $\sigma^{2}$。线性变换的参数为$\gamma$和$\beta$。求合并后的卷积操作中的$W_{merged}$和$B_{merged}$。


<img src="https://files.mdnice.com/user/6935/84b35586-b6ca-4253-abfc-567e0ef9cf8f.png" style="zoom:50%;" />

## 融合方案

BN等效成1x1卷积:

<img src=https://s2.loli.net/2024/05/08/DqoVJF3PxpnQ4i5.png width='100%'>

可以看出这就是一个1x1卷积的形式.

**代码实现**:
```python
import torch
    import torchvision
    
    def fuse(conv, bn):
    
        fused = torch.nn.Conv2d(
            conv.in_channels,
            conv.out_channels,
            kernel_size=conv.kernel_size,
            stride=conv.stride,
            padding=conv.padding,
            bias=True
        )
    
        # setting weights
        w_conv = conv.weight.clone().view(conv.out_channels, -1)
        w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps+bn.running_var)))
        fused.weight.copy_( torch.mm(w_bn, w_conv).view(fused.weight.size()) )
        
        # setting bias
        if conv.bias is not None:
            b_conv = conv.bias
        else:
            b_conv = torch.zeros( conv.weight.size(0) )
        b_conv = torch.mm(w_bn, b_conv.view(-1, 1)).view(-1)
        b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(
                              torch.sqrt(bn.running_var + bn.eps)
                            )
        fused.bias.copy_( b_conv + b_bn )
    
        return fused
    
    # Testing
    # we need to turn off gradient calculation because we didn't write it
    torch.set_grad_enabled(False)
    x = torch.randn(16, 3, 256, 256)
    resnet18 = torchvision.models.resnet18(pretrained=True)
    # removing all learning variables, etc
    resnet18.eval()
    model = torch.nn.Sequential(
        resnet18.conv1,
        resnet18.bn1
    )
    f1 = model.forward(x)
    fused = fuse(model[0], model[1])
    f2 = fused.forward(x)
    d = (f1 - f2).mean().item()
    print("error:",d)
```