归一化（normalization，有些文献也称为“正则化”或“规范化”）操作已经被广泛认为是能够提升算法性能的必备操作，该操作将数据“拽”到相同的分布水平。在CV领域，应用最为广泛、甚至说是“标配”的归一化方式为批归一化（BatchNormlization，以下简称为“BatchNorm”）。而在NLP领域，公认有效的归一化方式却是层归一化（LayerNormlization，以下简称为“LayerNorm”）。

## 1. BN与LN区别
BN: 将不同样本的相同维度的特征处理为相同的分布










