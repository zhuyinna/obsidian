归一化（normalization，有些文献也称为“正则化”或“规范化”）操作已经被广泛认为是能够提升算法性能的必备操作，该操作将数据“拽”到相同的分布水平。在CV领域，应用最为广泛、甚至说是“标配”的归一化方式为批归一化（BatchNormlization，以下简称为“BatchNorm”）。而在NLP领域，公认有效的归一化方式却是层归一化（LayerNormlization，以下简称为“LayerNorm”）。

## 1. BN与LN区别
BN: 将不同样本的相同维度的特征处理为相同的分布.
BN公式: 
$y={x-E[x]}/\sqrt{Var[x]+\epsilon}*\gamma+\beta$

LN: 对于一个NLP模型, 由于每个句子(一个句子可以看成一个样本)的长度不同,无法将不同样本的相同维度的特征处理为相同的分布,因此LN是对每个样本的每个维度的特征进行归一化
<img src=https://s2.loli.net/2024/05/07/waneKjYpbkIy4Ft.png width='100%'>

左: BN, 右: LN

## 位置
BN层往往用在深度神经网络的卷积层之后、激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。

## 总结

归根结底，两种归一化方法的差异主要体现在二者因面对任务的不同而引起的作用对象差异，以及其由此引发的一系列使用方法的不同。首先，最根本的不同即BatchNorm和LayerNorm的作用对象不同——BatchNorm认为相同维的特征具有相同分布，因此在特征维度上开展归一化操作，归一化的结果保持样本之间的可比较性。而LayerNorm认为每个样本内的特征具有相同分布，因此针对每一个样本进行归一化处理，保持相同样本内部不同对象的可比较性。由于上述根本差异的存在，引出了一系列使用方法的不同：BatchNorm在批次中执行跨样本的归一化操作，这就意味着批次的构成和规模会直接影响BatchNorm的效果。BatchNorm需要平衡小批次统计量和整体样本统计量之间的关系，还需要考虑利用批次统计量更新全局统计量的方法，这也涉及训练和测试阶段使用的统计量有“批次版”和“全局版”的问题…等等。而这些问题到了LayerNorm就都不再是问题——LayerNorm的归一化操作只在样本内部独立开展，因此实际可以完全忽略批次的存在。因此也不用考虑保存和更新的问题且训练和测试应用模式完全一致，均值和标准差随算随用。









