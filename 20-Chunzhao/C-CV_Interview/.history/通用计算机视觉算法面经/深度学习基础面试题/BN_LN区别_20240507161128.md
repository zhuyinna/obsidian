归一化（normalization，有些文献也称为“正则化”或“规范化”）操作已经被广泛认为是能够提升算法性能的必备操作，该操作将数据“拽”到相同的分布水平。在CV领域，应用最为广泛、甚至说是“标配”的归一化方式为批归一化（BatchNormlization，以下简称为“BatchNorm”）。而在NLP领域，公认有效的归一化方式却是层归一化（LayerNormlization，以下简称为“LayerNorm”）。

## 1. BN与LN区别
BN: 将不同样本的相同维度的特征处理为相同的分布

LN: 对于一个NLP模型, 由于每个句子(一个句子可以看成一个样本)的长度不同,无法将不同样本的相同维度的特征处理为相同的分布,因此LN是对每个样本的每个维度的特征进行归一化
<img src=https://s2.loli.net/2024/05/07/waneKjYpbkIy4Ft.png width='100%'>

左: BN, 右: LN











