## CNN

- 局部连接：不是全连接，而是使用size相对input小的kernel在局部感受视野内进行连接（点积运算）
- 权值共享：在一个卷积核运算中，每次都运算一个感受视野，通过滑动遍历的把整个输入都卷积完成，而不是每移动一次就更换卷积核参数



两者目的都是减少参数。通过局部感受视野，通过卷积操作获取高阶特征，能达到比较好的效果。

### 内部计算
先看卷积（卷积核与感受视野的点积）与池化示意图：

![](https://files.mdnice.com/user/6935/022ed39e-a493-4f7b-8a3a-9476dcfc3c3f.gif)

最大池化层:

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvOTkxNDcwLzIwMTkwMi85OTE0NzAtMjAxOTAyMDgyMDE1MDg3MDQtMzY4NjQ0NzkyLnBuZw?x-oss-process=image/format,png)

- 输入、卷积、池化形状定义:
   输入为（长，宽，RGB通道数）=（7*7*3）的图片，即inputs = (batch_size, 7, 7, 3)
  卷积核为（长，宽，卷积通道数，卷积核个数），即filter = (3, 3, 3, 32)。
  池化为（长，宽，步长）
  - 标准卷积运算过程：
    1. 对于输入没有疑问，对于卷积操作，定义的32个卷积核，每一个卷积核都有3个通道，**相同的卷积核在三个通道的卷积核参数不相同，只有尺寸相同，** 每个卷积核都有3个尺寸但参数不一样的变量，最后该卷积核的结果为每个通道的卷积结果相加，得到卷积后的新通道。卷积核的个数为卷积完成后新的通道数。
    2. 参数个数为 32*3， 参数最后的size为3x3x3x32。

- 池化的意义

  1.特征不变形：池化操作是模型更加关注是否存在某些特征而不是特征具体的位置。

　　2.特征降维：池化相当于在空间范围内做了维度约减，从而使模型可以抽取更加广范围的特征。同时减小了下一层的输入大小，进而减少计算量和参数个数。

　　3.在一定程度上防止过拟合，更方便优化。



## 卷积和池化后的大小计算 

给定的值：原大小 $(H, W)$, 卷积核大小 $(h, w)$, 池化大小 $(h, w)$, padding(填充)=a, strids (滑动步长） $=\mathrm{b}$, 公式通用 计算公式: $\left\{\begin{array}{c}H^{\prime}=\frac{H+2 * a-h}{b}+1 \\ W^{\prime}=\frac{W+2 * a-w}{b}+1\end{array}\right.$





假设：输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：

1. 卷积后: $H^{\prime}=\frac{200+2 * 1-5}{2}+1=\frac{197}{2}+1 \approx 99$
2. 池化后: $H^{\prime}=\frac{99+2 * 0-3}{1}+1=\frac{96}{1}+1 \approx 97$
3. 又一层卷积后: $H^{\prime}=\frac{97+2 * 1-3}{1}+1=\frac{96}{1}+1=97$
最后大小为 $(97,97)$

## 传统图像处理中常见的滤波器
>  卷积核和滤波器?
> 卷积核是卷积操作的参数，滤波器是卷积核的集合，卷积核是滤波器的一部分。一个滤波器就对应一个特征图.

1. 均值滤波和高斯滤波
- 滤波器中元素之和为1，输出亮度与输入基本一致；
- 均为低通滤波器，主要用于图像模糊/平滑处理、消除噪点；
- 核越大，模糊程度越大；
e.g. 均值滤波器
$$
\begin{array}{|c|c|c|}
\hline 1/9 & 1/9 & 1/9 \\ \hline 1/9 & 1/9 & 1/9 \\ \hline 1/9 & 1/9 & 1/9 \\ 
\hline
\end{array}
$$

e.g.高斯滤波器
$$
\begin{array}{|c|c|c|}
\hline 1/16 & 1/8 & 1/16 \\ \hline 1/8 & 1/4 & 1/8 \\ \hline 1/16 & 1/8 & 1/16 \\
\hline
\end{array}
$$

2. 锐化卷积核
- 锐化卷积核是一种高通滤波器，用于增强图像的边缘和细节；
e.g.
$$
\begin{array}{|c|c|c|}
\hline -1 & -1 & -1 \\ \hline -1 & 9 & -1 \\ \hline -1 & -1 & -1 \\
\hline
\end{array}
$$

3. 一阶微分算子
<img src=https://s2.loli.net/2024/05/08/euprsJljtx4bR8i.png width='100%'>

4. 二阶微分算子
<img src=https://s2.loli.net/2024/05/08/VpDYNwA5rJX2vta.png width='100%'>


## 卷积神经网络中常见卷积

比如，在传统图像处理中，人们通过设定不同的算子来提取诸如边缘、水平、垂直等固定的特征。而在卷积神经网络中，仅需要随机初始化一个固定卷积核大小的滤波器，并通过诸如反向传播的技术来实现卷积核参数的自动更新即可。

### 1. 原始卷积
特性:
- 权值共享
- 平移不变性: 比较脆弱. 当图像中的目标发生偏移时网络仍然能够输出同源图像一致的结果。
- 平移等变性: 当输入发生偏移时网络的输出结果也应该发生相应的偏移。这种特性比较适用于目标检测和语义分割等任务。

### 2. 组卷积
2.1 背景
GPU算力瓶颈

2.2 原理

<img src=https://s2.loli.net/2024/05/08/3ignjM4sQmkdfCE.png width='100%'>

2.3 特性
2.3.1 降低参数量
参数量为原始卷积的1/g，其中g为分组数。

2.3.2 提高训练效率
通过将卷积运算按通道划分为多个路径，可以尽可能地利用分布式的计算资源进行并行运算，有利于大规模深度神经网络的训练。

2.3.3 提高泛化性能
组卷积可以看成是对原始卷积操作的一种解耦，改善原始卷积操作中滤波器之间的稀疏性，在一定程度上起到正则化的作用。

2.4 改进
中间过程显然缺乏信息的交互（考虑到不同滤波器可提取到不同的特征）
为了解决此问题，ShuffleNet结合了逐点组卷积(Pointwise Group Convolution, PGC)和通道混洗(channel shuffle)，来实现一个高效轻量化的移动端网络设计。


### 3. 转置卷积
<img src=https://s2.loli.net/2024/05/08/daugpmbFnGhiXwM.png width='100%'>

**特性**

- 特征上采样

利用转置卷积，可以引入参数让网络自动学习卷积核的权重以更好地恢复空间分辨率。一般来说，利用转置卷积来替代常规的上采样操作（最近邻插值、双线性插值即双立方插值）会取得更好的效果（在没有过拟合的情况下），弊端是增大了参数量，且容易出现网格效应

- 特征可视化

利用转置卷积还可以对特征图进行可视化。有时间的强烈推荐大家去阅读原论文《Visualizing and Understanding Convolutional Networks》[6]，有助于帮助大家理解不同深度的各个特征图究竟学到了什么特征。比如，增加网络的深度有利于提取更加抽象的高级语义特征，而增加网络的宽度有利于增强特征多样性的表达。或者是小的卷积核有利于特征的学习，而小的步长则有利于保留更多的空间细节信息。

### 4. 1x1卷积

1×1卷积最初提出的目的是用于增强模型对特定感受野下的局部区域的判定能力。后续也被GoogleNet和ResNet进一步的应用。

详细见： [[1x1]]

**特性**
  - 增强特征表达能力
  卷积之后的非线性激活函数
  - 升维和降维
  - 跨通道的信息交互

### 5. 空洞卷积

空洞卷积，也称为扩张卷积(Dilated Convolution)，最早是针对语义分割任务所提出来的。由于语义分割是一种像素级的分类，经过编码器所提取出的高级特征图最终需要上采样到原始输入特征图的空间分辨率。因此，为了限制网络整体的计算效率，通常会采用池化和插值等上/下采样操作，但这对语义分割这种稠密预测任务来说是非常致命的，主要体现在以下三方面：

不可学习：由于上采样操作（如双线性插值法）是固定的即不可学习的，所以并不能重建回原始的空间信息。
损失空间信息：引入池化操作不可避免的会导致内部数据结构丢失，导致空间细节信息严重丢失。
丢失小目标：经过N次池化（每次下采样2倍），原则上小于
个像素点的目标信息将不可重建，这对于语义分割这种密集型预测任务来说是致命的。

<img src=https://s2.loli.net/2024/05/08/zoFi8pQ3uRvdc4V.png width='100%'>

空洞卷积可看成是原始卷积更进一步的扩展，通过在原始卷积的基础上引入空洞率这个超参数，用于调节卷积核的间隔数量。比如，原始卷积核其空洞率为1，而对于空洞率为k的卷积则用0去填充空白的区域。

优点: 增大感受野, 表征多尺度信息
缺点:不好优化(速度方面), 引入网格/棋盘效应

### 6. 深度可分离卷积
逐深度卷积+逐点卷积

**特性**

- 降低模型参数量和计算量
- 降低模型容量
在应用时没有使用激活函数. 此外，虽然深度可分离卷积可以显著的降低模型的计算量，但同时也会导致模型的容量显著降低，从而导致模型精度的下降。
  > 为什么? 还不清楚, 只查到了加入激活函数和BN会导致性能下降


### 7. 可变形卷积
需要注意的是，可变形卷积并不是真正意义上的学习可变形的卷积核，而是利用额外的卷积层去学习相应的偏移量，将得到的偏移量叠加到输入特征图中相应位置的像素点中。但由于偏移量的生成会产生浮点数类型，而偏移量又必须转换为整形，如果直接取整的话是无法进行反向传播的，因此原文是利用双线性插值的方式来间接的计算对应的像素值。

**特性**

- 自适应感受野
但实际上同一个物体由于在不同位置上可能对应着不同的尺度或者变形，因此自适应感受野是进行精确定位所需要的，特别是对于密集型预测任务来说。变形卷积基于一个平行的网络来学习偏移，让卷积核在输入特征图能够发散采样，使网络能够聚焦目标中心，从而提高对物体形变的建模能力。

- 难以部署
  卷积核过大

### 8. 空间可分离卷积
与深度可分离卷积一样，空间可分离卷积也属于因式分离卷积的一种，其核心思想是从图像空间维度（宽度和高度）进行卷积运算。

<img src=https://s2.loli.net/2024/05/08/s34rzS5gUNanw6L.png width='100%'>

**特性**

- 降低运算量

  如上图所示，以5×5的输入特征图为例，如果我们直接用一个3×3的卷积核去卷积，共需要9×9=81次乘法运算。而如果换成空间可分离卷积，那么计算量为15×3+9×3=72次乘法运算。共节省了约11%的计算量

**应用**

可以在空间上分离的最著名的卷积之一是Sobel算子，用于检测边缘。

局限性:空间可分离卷积在实际当中很少被广泛应用，最主要的一个原因是并不是所有的卷积核都能够有效的拆分成小的卷积核。

### 9. 图卷积

<img src=https://s2.loli.net/2024/05/08/Ryk5PZLVK19Tgab.png width='100%'>

图中的核心思想是利用边的信息对节点信息进行聚合，从而生成新的节点表示。简而言之，CNNs中的卷积运算是卷积核对应位置的加权求和，扩展到GCNs就是利用边的信息不断的汇聚邻间节点的信息，以更新原节点的参数。

### 10. 