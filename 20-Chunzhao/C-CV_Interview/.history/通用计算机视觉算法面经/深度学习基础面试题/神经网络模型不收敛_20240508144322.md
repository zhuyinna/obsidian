## 原因
没有对数据进行归一化
忘记检查输入和输出
没有对数据进行预处理
没有对数据正则化
使用过大的样本
使用不正确的学习率
在输出层使用错误的激活函数
网络中包含坏梯度
初始化权重错误
过深的网络
隐藏单元数量错误

对应的解决办法分别是：

1. 对数据进行归一化，常用的归一化包括**零均值归一化**和**线性函数归一化**方法；
    > 原因: 这是由于不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。
    此外，大部分神经网络流程都假设输入输出是在0附近的分布，从权值初始化到激活函数、从训练到训练网络的优化算法。
2. 检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；
    > 有时候损失函数的明显减小并不意味着训练工作的完成, 在你的代码中，在数据预处理、训练代码等都有可能出现错误，而损失函数的下降并不意味着网络学习到了有用的东西. 这是由于与传统的编程不同，机器学习对于某些错误不能够及时有效的反馈错误信息，以便我们回过头来对代码BUG进行检查。
3. 对数据进行预处理，包括做一些简单的转换；
4. 采用正则化方法，比如 L2 正则，或者 dropout；
    > 正则化的基本方法是在网络层之间添加dropout，设置从中到高的训练概率。例如0.75或0.9。如果你仍然认为不太可能出现过拟合，可以将此参数设置为较高的值例如0.99。如果你认为过拟合是一个问题，可以尝试将此参数设置为0.5或更低。
5. 在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。
    > 大样本会破坏梯度下降的随机性。 使用小样本与更为随机的权重。这样做有两大好处。首先，这样可以帮助训练跳出鞍点。其次，可以使训练在更为平缓的局部最小值停止。一般而言，后者具备更好的泛化性能。
6. **不采用梯度裁剪**。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。
    > 很多深度学习框架会启用梯度裁剪（Clipping Gradient）。这可以防止训练过程中出现的梯度爆炸。它会在每一步中强制改变权重，让权重发生最大限度的改变。这在数据中含有大量异常时尤为有效。但是，开启这个选项也会让用户难以手动找到最佳的学习率。大多数用户由于梯度裁剪将学习率设置的过高，使整体训练行为变慢、也使结果不可预测。因此，关闭梯度裁剪，找到最佳学习率，然后再开启梯度裁剪是一个更好的选择。
    ```python
7. 如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；
8. 如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。
9. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
10. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
11. 从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字