## 原因

数据方面
1. 没有对数据进行预处理
2. 忘记对你的数据进行归一化

网络方面
1. 忘记对你的数据进行归一化
2. 
3. 忘记对你的数据进行归一化

4. 忘记检查输出结果

5. 没有使用任何的正则化方法

6. 使用了一个太大的 batch size

7. 使用一个错误的学习率

8. 在最后一层使用错误的激活函数

9.  网络包含坏的梯度

10. 网络权重没有正确的初始化

11. 使用了一个太深的神经网络

12. 隐藏层神经元数量设置不正确

对应的解决办法分别是：

1. 对数据进行归一化，常用的归一化包括**零均值归一化**和**线性函数归一化**方法；
2. 检测训练过程中每个阶段的数据结果，如果是图像数据可以考虑使用可视化的方法；
3. 对数据进行预处理，包括做一些简单的转换；
4. 采用正则化方法，比如 L2 正则，或者 dropout；
5. 在训练的时候，找到一个可以容忍的最小的 batch 大小。可以让 GPU 并行使用最优的 batch 大小并不一定可以得到最好的准确率，因为更大的 batch 可能需要训练更多时间才能达到相同的准确率。所以大胆的从一个很小的 batch 大小开始训练，比如 16，8，甚至是 1。
6. **不采用梯度裁剪**。找出在训练过程中不会导致误差爆炸的最大学习率。将学习率设置为比这个低一个数量级，这可能是非常接近最佳学习率。
7. 如果是在做回归任务，大部分情况下是不需要在最后一层使用任何激活函数；如果是分类任务，一般最后一层是用 sigmoid 激活函数；
8. 如果你发现你的训练误差没有随着迭代次数的增加而变化，那么很可能就是出现了因为是 ReLU 激活函数导致的神经元死亡的情况。可以尝试使用如 leaky ReLU 或者 ELUs 等激活函数，看看是否还出现这种情况。
9. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
10. 目前比较常用而且在任何情况下效果都不错的初始化方式包括了“he”，“xaiver”和“lecun”。所以可以任意选择其中一种，但是可以先进行实验来找到最适合你的任务的权值初始化方式。
11. 从256到1024个隐藏神经元数量开始。然后，看看其他研究人员在相似应用上使用的数字