简述一下你常用的网络模型训练技巧？
1. 使用更大的 batch size。使用更大的 batch size 可以加快训练的进度。但是对于凸优化问题，收敛速度会随着 batch size 的增加而降低。所以在相同的 epoch 下，使用更大的 batch size 可能会导致验证集的 acc更低。所以可以使用以下技巧来解决问题。
    > 什么是凸优化问题? 凸优化问题是指目标函数是凸函数，约束条件是凸集的优化问题。凸优化问题有很多优良的性质，比如局部最优解就是全局最优解，凸优化问题的解是唯一的等等。凸优化问题是数学规划中的一个重要分支，也是机器学习中的一个重要分支。

    > 有哪些典型的凸优化问题? 凸优化问题有很多典型的问题，比如线性规划、二次规划、半定规划、凸二次规划、凸规划、非线性规划等等。

- (1)  linear scaling learning rate。使用更大的学习率，例如，当我们选择初始学习率为 0.1， batch size 为 256，当将 batch size 增大至 b 时，需要将初始学习率增加至 0.1 * b / 256

- (2) learning rate warm up。选择前 $\mathrm{n}$ 个 epoch 进行 warm up, 在这 $\mathrm{n}$ 个 epoch 中线性地增加学习率至初始学习率, 在正常地进行 decay。
    > 为什么要进行warm up? 在训练的开始阶段，模型的参数是随机初始化的，此时模型的输出是不稳定的，可能会导致梯度爆炸或者梯度消失。为了避免这种情况，可以在训练的开始阶段，使用较小的学习率，逐渐增大学习率，直到达到初始学习率。

- (3) zero $\gamma$ 。在 residual block 中的 $\mathrm{BN}$ 中, 首先进行标准化输入 $\mathrm{x}$, 得 到 $\hat{x}$, 再进行线性变化: $\gamma \hat{x}+\beta$, 其中 $\gamma$ 和 $\beta$ 都是可以学习的参数, 其 值被初始化为 1 和 0 , 而在这里, $\gamma$ 被初始化为 0 。(
- (4) no bias decay。为了避免过拟合, 对于权重 weight 和 bias, 通常会 使用 weight decay。但是在这里, 仅对 weight 使用 decay, 而不对 bias 进行 decay。

1. 使用更低的数值精度。
2. cosine learning rate decay 。将学习率随着 epoch 的增大而不断衰减。
$$
\eta_{t}=\frac{1}{2}\left(1+\cos \left(\frac{t \pi}{T}\right)\right) \eta
$$
1. label smoothing。
2. knowledge distillation。
3. mixup training, cutout, random erase, data augmentation 等数据增强 方法。

