简述一下你常用的网络模型训练技巧？
1. 使用更大的 batch size。使用更大的 batch size 可以加快训练的进度。但是对于凸优化问题，收敛速度会随着 batch size 的增加而降低。所以在相同的 epoch 下，使用更大的 batch size 可能会导致验证集的 acc更低。所以可以使用以下技巧来解决问题。

- (1)  linear scaling learning rate。使用更大的学习率，例如，当我们选择初始学习率为 0.1， batch size 为 256，当将 batch size 增大至 b 时，需要将初始学习率增加至 0.1 * b / 256

- (2) learning rate warm up。选择前 $\mathrm{n}$ 个 epoch 进行 warm up, 在这 $\mathrm{n}$ 个 epoch 中线性地增加学习率至初始学习率, 在正常地进行 decay。

- (3) zero $\gamma$ 。在 residual block 中的 $\mathrm{BN}$ 中, 首先进行标准化输入 $\mathrm{x}$, 得 到 $\hat{x}$, 再进行线性变化: $\gamma \hat{x}+\beta$, 其中 $\gamma$ 和 $\beta$ 都是可以学习的参数, 其 值被初始化为 1 和 0 , 而在这里, $\gamma$ 被初始化为 0 。(
- (4) no bias decay。为了避免过拟合, 对于权重 weight 和 bias, 通常会 使用 weight decay。但是在这里, 仅对 weight 使用 decay, 而不对 bias 进行 decay。

2. 使用更低的数值精度。
3. cosine learning rate decay 。将学习率随着 epoch 的增大而不断衰减。
$$
\eta_{t}=\frac{1}{2}\left(1+\cos \left(\frac{t \pi}{T}\right)\right) \eta
$$
4. label smoothing。
5. knowledge distillation。
6. mixup training, cutout, random erase, data augmentation 等数据增强 方法。

