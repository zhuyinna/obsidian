# 词向量的历史

- 2003年，Bengio等人提出了神经网络语言模型（NNLM），并在2008年提出了词嵌入模型（Word Embedding）。
- 2013年，Mikolov等人提出了word2vec模型，该模型是一个高效的词向量学习模型，可以学习出高质量的词向量。
    > 不足之处: word2vec模型只考虑了局部词频统计，没有考虑全局词频统计。其是一个静态的训练模型，无法表示出一词多义，你说“苹果”是指苹果公司呢，还是水果呢？

# ELMO
word2vec无法解决一词多义的问题, 静态指的是训练好之后这个单词的表达就固定住了, 无论新的句子上下文单词是什么, 这个单词的Word Embedding不会跟着上下文场景的变化而改变.

2018年，Peters等人提出了ELMO模型，ELMO模型是一个双层双向LSTM模型，可以学习出动态的词向量。ELMO模型的主要思想是：一个词的意思是由上下文决定的，因此一个词的词向量应该是上下文的函数。ELMO模型的输入是一个词序列，输出是每个词的词向量。ELMO模型的输出是一个词的词向量是上下文的函数，因此一个词的词向量是动态的。




