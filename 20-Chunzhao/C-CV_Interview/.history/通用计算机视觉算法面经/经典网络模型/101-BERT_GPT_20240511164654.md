# TL;DR
<img src=https://s2.loli.net/2024/05/11/mg9fL4yVOoFJ1NG.png width='100%'>

# 二阶段模型

## pretrain训练方式
很多机器学习任务都需要带标签的数据集作为输入完成。但是我们身边存在大量没有标注的数据，例如文本、图片、代码等等。标注这些数据需要花费大量的人力和时间，标注的速度远远不及数据产生的速度，所以带有标签的数据往往只占有总数据集很小的一部分。随着算力的不断提高，计算机能够处理的数据量逐渐增大。如果不能很好利用这些无标签的数据就显得很浪费。

所以半监督学习和预训练+微调的二阶段模式整变得越来越受欢迎。最常见的二阶段方法就是Word2Vec，使用大量无标记的文本训练出带有一定语义信息的词向量，然后将这些词向量作为下游机器学习任务的输入，就能够大大提高下游模型的泛化能力。


## finetune

接下来就进入模型训练的第二步，运用少量的带标签数据对模型参数进行微调。

上一步中最后一个词的输出我们没有用到，在这一步中就要使用这一个输出来作为下游监督学习的输入。

<img src=https://s2.loli.net/2024/05/11/lUBjn13qXMDQxm7.png width='100%'>

> 看不懂...

# word2vec

2013年，Mikolov等人提出了word2vec模型，该模型是一个高效的词向量学习模型，可以学习出高质量的词向量。

**不足之处**
word2vec模型只考虑了局部词频统计，没有考虑全局词频统计。其是一个静态的训练模型，无法表示出一词多义，你说“苹果”是指苹果公司呢，还是水果呢？

# ELMO
word2vec无法解决一词多义的问题, 静态指的是训练好之后这个单词的表达就固定住了, 无论新的句子上下文单词是什么, 这个单词的Word Embedding不会跟着上下文场景的变化而改变.

2018年，Peters等人提出了ELMO模型，ELMO模型是一个双层双向LSTM模型，可以学习出动态的词向量。ELMO模型的主要思想是：一个词的意思是由上下文决定的，因此一个词的词向量应该是上下文的函数。ELMO模型的输入是一个词序列，输出是每个词的词向量。ELMO模型的输出是一个词的词向量是上下文的函数，因此一个词的词向量是动态的。

## 第一阶段Pretrain

<img src=https://s2.loli.net/2024/05/11/yS8jeYUKW4NLr16.png width='100%'>

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。

前向LSTM的对数似然概率:
<img src=https://s2.loli.net/2024/05/11/JCREWsPwGkz15mF.png width='100%'>
后向:
<img src=https://s2.loli.net/2024/05/11/oM4b9pV1gfa5xUs.png width='100%'>
前向和后向结合:
<img src=https://s2.loli.net/2024/05/11/Z43CxPUHYTyg7Ko.png width='100%'>
其中Θx和Θs表示词向量矩阵和softmat层的参数，在前向和后向LSTM中都是共享的。

biLM的输出有2L+1个输出向量, 因为每一层LSTM都会有前向和后向两个向量输出，而每个词汇自己有embedding层的向量，因此总共是2L+1,表示如下：
<img src=https://s2.loli.net/2024/05/11/oZAfnuSPh6ar4CD.png width='100%'>




## 第二阶段Finetune

对于第一阶段已经预训练好的ELMO模型，下面进入第二阶段，如何在具体的nlp任务场景中使用，假如我们的下游任务是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding(假如是双层网络)，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。

**具体的NLP任务**: ELMo会训练一个权重向量对每一个词汇的输出向量进行线性加权，表示如下
<img src=https://s2.loli.net/2024/05/11/96yOQjo7btH3n2R.png width='100%'>

其中$s^{task}$是softmax规范化后的权重向量，$\gamma^{task}$
是一个放缩参数。由于biLM每一层的向量输出分布可能不同，因此，在进行线性加权之前，也可以考虑对每个向量进行Layer Normalization。

## 不足之处
1. LSTM特征抽取能力和训练速度弱于transformer
2. 拼接方式双向融合特征融合能力偏弱，ELMo在模型层上就是一个stacked bi-lstm（严格来说是训练了两个单向的stacked lstm），ELMo有用双向RNN来做encoding，但是这两个方向的RNN其实是分开训练的，只是在最后在loss层做了个简单相加。这样就导致对于每个方向上的单词来说，在被encoding的时候始终是看不到它另一侧的单词的。而显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的。

# GPT
Generative Pre-Training

## 第一阶段Pretrain

<img src=https://s2.loli.net/2024/05/11/DsLnjrZfuokWlMG.png width='100%'>

和ELMO的不同点:
1. 将LSTM替换为transformer: 特征提取能力增强
2. 单向: 只用了上文context-before, 没有用context-after,限制了应用场景比如阅读理解, 浪费了很多信息

## 第二阶段Finetune

<img src=https://s2.loli.net/2024/05/11/PijYb3wZorxVU2A.png width='100%'>

Classification：对于分类问题，不需要做什么修改
Entailment：对于推理问题，可以将先验与假设使用一个分隔符分开
Similarity：对于相似度问题，由于模型是单向的，但相似度与顺序无关。所以需要将两个句子顺序颠倒后两次输入的结果相加来做最后的推测
Multiple Choice：对于问答问题，则是将上下文、问题放在一起与答案分隔开，然后进行预测

## 不足之处

1. 单向

# BERT

如果做双向的语言模型,会导致要预测的词已经被看到了. 所以BERT提出了新的训练方式
## 第一阶段Pretrain
## Masked LM

但是，直接将大量的词替换为<MASK>标签可能会造成一些问题，模型可能会认为只需要预测<MASK>相应的输出就行，其他位置的输出就无所谓。同时Fine-Tuning阶段的输入数据中并没有<MASK>标签，也有数据分布不同的问题。为了减轻这样训练带来的影响，BERT采用了如下的方式：

输入数据中随机选择15%的词用于预测，这15%的词中:
- 80%的词向量输入时被替换为<MASK>
- 10%的词的词向量在输入时被替换为其他词的词向量
- 另外10%保持不动

> 这样一来就相当于告诉模型，我可能给你答案，也可能不给你答案，也可能给你错误的答案，有<MASK>的地方我会检查你的答案，没<MASK>的地方我也可能检查你的答案，所以<MASK>标签对你来说没有什么特殊意义，所以无论如何，你都要好好预测所有位置的输出。

## encoder
采用了transformer encoder block:
1. 并行性更好
2. (个人感觉)更加免受mask影响, 因为通过attetnion能够削弱mask标记, 针对性削弱匹配权重
3. 位置信息? 额外训练position embedding, 直接和word embedding相加

## 句子级负采样
首先给定的一个句子（相当于word2vec中给定context），它下一个句子即为正例（相当于word2vec的中心词），随机采样一个句子作为负例（相当于word2vec中随机采样的词），然后在该sentence-level上来做二分类（即判断句子是当前句子的下一句还是噪声）.

**句子级表示**
BERT这里并没有像下游监督任务中的普遍做法一样，在encoding的基础上再搞个全局池化之类的，它首先在每个sequence（对于句子对任务来说是两个拼起来的句子，对于其他任务来说是一个句子）前面加了一个特殊的token，记为[CLS]，如图
<img src=https://s2.loli.net/2024/05/11/5HoxTXa8eMNhGOJ.png width='100%'>

然后让encoder对[CLS]进行深度encoding，深度encoding的最高隐层即为整个句子/句对的表示啦。这个做法乍一看有点费解，不过别忘了，Transformer是可以无视空间和距离的把全局信息encoding进每个位置的，而[CLS]作为句子/句对的表示是直接跟分类器的输出层连接的，因此其作为梯度反传路径上的“关卡”，当然会想办法学习到分类相关的上层特征啦。
另外，为了让模型能够区分里面的每个词是属于“左句子”还是“右句子”，作者这里引入了“segment embedding”的概念来区分句子。对于句对来说，就用embedding A和embedding B来分别代表左句子和右句子；而对于句子来说，就只有embedding A啦。这个embedding A和B也是随模型训练出来的。
所以最终BERT每个token的表示由token原始的词向量token embedding、前文提到的position embedding和这里的segment embedding三部分相加而成，如图：
<img src=https://s2.loli.net/2024/05/11/5HoxTXa8eMNhGOJ.png width='100%'>

segment embedding: 用来区分左句子和右句子

<img src=https://s2.loli.net/2024/05/11/hWmkSwHJEj5eVTa.png width='100%'>

## 第二阶段Finetune
简洁的迁移策略
**a: 文本匹配任务**（文本匹配其实也是一种文本分类任务，只不过输入是文本对）
**b: 文本分类任务**
只需要用得到的表示（即encoder在[CLS]词位的顶层输出）加上一层MLP

**d: 序列标注任务**
**c: 问答任务**



<img src=https://s2.loli.net/2024/05/11/RzaimpKbfFtA19Y.png width='100%'>


