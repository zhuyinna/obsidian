# TL;DR
<img src=https://s2.loli.net/2024/05/11/mg9fL4yVOoFJ1NG.png width='100%'>

# word2vec

2013年，Mikolov等人提出了word2vec模型，该模型是一个高效的词向量学习模型，可以学习出高质量的词向量。

## 不足之处
word2vec模型只考虑了局部词频统计，没有考虑全局词频统计。其是一个静态的训练模型，无法表示出一词多义，你说“苹果”是指苹果公司呢，还是水果呢？

# ELMO
word2vec无法解决一词多义的问题, 静态指的是训练好之后这个单词的表达就固定住了, 无论新的句子上下文单词是什么, 这个单词的Word Embedding不会跟着上下文场景的变化而改变.

2018年，Peters等人提出了ELMO模型，ELMO模型是一个双层双向LSTM模型，可以学习出动态的词向量。ELMO模型的主要思想是：一个词的意思是由上下文决定的，因此一个词的词向量应该是上下文的函数。ELMO模型的输入是一个词序列，输出是每个词的词向量。ELMO模型的输出是一个词的词向量是上下文的函数，因此一个词的词向量是动态的。

## 第一阶段Pretrain

<img src=https://s2.loli.net/2024/05/11/yS8jeYUKW4NLr16.png width='100%'>

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。

前向LSTM的对数似然概率:
<img src=https://s2.loli.net/2024/05/11/JCREWsPwGkz15mF.png width='100%'>
后向:
<img src=https://s2.loli.net/2024/05/11/oM4b9pV1gfa5xUs.png width='100%'>
前向和后向结合:
<img src=https://s2.loli.net/2024/05/11/Z43CxPUHYTyg7Ko.png width='100%'>
其中Θx和Θs表示词向量矩阵和softmat层的参数，在前向和后向LSTM中都是共享的。

biLM的输出有2L+1个输出向量, 因为每一层LSTM都会有前向和后向两个向量输出，而每个词汇自己有embedding层的向量，因此总共是2L+1,表示如下：
<img src=https://s2.loli.net/2024/05/11/oZAfnuSPh6ar4CD.png width='100%'>




## 第二阶段Finetune

对于第一阶段已经预训练好的ELMO模型，下面进入第二阶段，如何在具体的nlp任务场景中使用，假如我们的下游任务是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding(假如是双层网络)，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。

**具体的NLP任务**: ELMo会训练一个权重向量对每一个词汇的输出向量进行线性加权，表示如下
<img src=https://s2.loli.net/2024/05/11/96yOQjo7btH3n2R.png width='100%'>

其中$s^{task}$是softmax规范化后的权重向量，$\gamma^{task}$
是一个放缩参数。由于biLM每一层的向量输出分布可能不同，因此，在进行线性加权之前，也可以考虑对每个向量进行Layer Normalization。

## 不足之处
1. LSTM特征抽取能力和训练速度弱于transformer
2. 拼接方式双向融合特征融合能力偏弱，ELMo在模型层上就是一个stacked bi-lstm（严格来说是训练了两个单向的stacked lstm），ELMo有用双向RNN来做encoding，但是这两个方向的RNN其实是分开训练的，只是在最后在loss层做了个简单相加。这样就导致对于每个方向上的单词来说，在被encoding的时候始终是看不到它另一侧的单词的。而显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的。

# GPT
Generative Pre-Training

## 第一阶段Pretrain

<img src=https://s2.loli.net/2024/05/11/DsLnjrZfuokWlMG.png width='100%'>

和ELMO的不同点:
1. 将LSTM替换为transformer: 特征提取能力增强
2. 单向: 只用了上文context-before, 没有用context-after,限制了应用场景比如阅读理解, 浪费了很多信息

## 第二阶段Finetune

<img src=https://s2.loli.net/2024/05/11/PijYb3wZorxVU2A.png width='100%'>

GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。

## 不足之处

1. 单向

# BERT

如果做双向的语言模型,会导致要预测的词已经被看到了. 所以BERT提出了新的训练方式
## 第一阶段Pretrain
## Masked LM

随机mask句子的部分单词, 类似于完形填空. 但是会导致mask部分也被encode进去, 这些mask在下游任务中是不存在的, 因此最终训练设计为:
对于被mask的单词:

- 有80%的概率用“[mask]”标记来替换
- 有10%的概率用随机采样的一个单词来替换
- 有10%的概率不做替换（虽然不做替换，但是还是要预测哈）

## encoder
采用了transformer encoder block:
1. 并行性更好
2. (个人感觉)更加免受mask影响, 因为通过attetnion能够削弱mask标记, 针对性削弱匹配权重
3. 位置信息? 额外训练position embedding, 直接和word embedding相加

## 句子级负采样
首先给定的一个句子（相当于word2vec中给定context），它下一个句子即为正例（相当于word2vec的中心词），随机采样一个句子作为负例（相当于word2vec中随机采样的词），然后在该sentence-level上来做二分类（即判断句子是当前句子的下一句还是噪声）.

**句子级表示**
BERT这里并没有像下游监督任务中的普遍做法一样，在encoding的基础上再搞个全局池化之类的，它首先在每个sequence（对于句子对任务来说是两个拼起来的句子，对于其他任务来说是一个句子）前面加了一个特殊的token，记为[CLS]，如图
<img src=https://s2.loli.net/2024/05/11/5HoxTXa8eMNhGOJ.png width='100%'>

然后让encoder对[CLS]进行深度encoding，深度encoding的最高隐层即为整个句子/句对的表示啦。这个做法乍一看有点费解，不过别忘了，Transformer是可以无视空间和距离的把全局信息encoding进每个位置的，而[CLS]作为句子/句对的表示是直接跟分类器的输出层连接的，因此其作为梯度反传路径上的“关卡”，当然会想办法学习到分类相关的上层特征啦。
另外，为了让模型能够区分里面的每个词是属于“左句子”还是“右句子”，作者这里引入了“segment embedding”的概念来区分句子。对于句对来说，就用embedding A和embedding B来分别代表左句子和右句子；而对于句子来说，就只有embedding A啦。这个embedding A和B也是随模型训练出来的。
所以最终BERT每个token的表示由token原始的词向量token embedding、前文提到的position embedding和这里的segment embedding三部分相加而成，如图：
<img src=https://s2.loli.net/2024/05/11/5HoxTXa8eMNhGOJ.png width='100%'>

segment embedding: 用来区分左句子和右句子

<img src=https://s2.loli.net/2024/05/11/hWmkSwHJEj5eVTa.png width='100%'>

## 第二阶段Finetune
简洁的迁移策略
**a: 文本匹配任务**（文本匹配其实也是一种文本分类任务，只不过输入是文本对）
**b: 文本分类任务**
只需要用得到的表示（即encoder在[CLS]词位的顶层输出）加上一层MLP

**d: 序列标注任务**
**c: 问答任务**



<img src=https://s2.loli.net/2024/05/11/RzaimpKbfFtA19Y.png width='100%'>


