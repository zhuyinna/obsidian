- [UNet变体](#unet变体)
  - [1. UNet++(Nested Unet)](#1-unetnested-unet)
  - [2. Attention Unet](#2-attention-unet)
  - [3. 3D Unet](#3-3d-unet)
  - [4. Vnet](#4-vnet)
  - [5. ResUnet](#5-resunet)
  - [6. Inception-Unet](#6-inception-unet)
  - [7. RU-Net and R2U-Net](#7-ru-net-and-r2u-net)


# UNet变体

## 1. UNet++(Nested Unet)

UNet++是UNet的改进版本，通过增加更多的跳跃连接来提高模型的性能。UNet++的结构如下图所示：

<img src=https://s2.loli.net/2024/05/11/FV1jw9CuKiAz4Rr.png width='100%'>

图1：（a）UNet++由编码器和解码器组成，它们通过一系列嵌套的密集卷积块连接。UNet++背后的主要思想是在融合之前结合编码器和解码器特征图之间的语义差距。例如，（X0，0，X1，3）之间的语义差距是使用具有三个卷积层的密集卷积块来结合的。在上图中，黑色表示原始U-Net，绿色和蓝色表示跳过路径上的密集卷积块，红色表示深度监督。 红色、绿色和蓝色组件将UNet++与U-Net区分开来。（b） 对UNet++的第一个跳跃途径的详细分析。（c） UNet++如果在深度监督下进行训练，可以在推理时进行修剪。

**代码**:
```python
#For nested 3 channels are required

class conv_block_nested(nn.Module):
    
    def __init__(self, in_ch, mid_ch, out_ch):
        super(conv_block_nested, self).__init__()
        self.activation = nn.ReLU(inplace=True) #进行覆盖运算
        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)
        self.bn1 = nn.BatchNorm2d(mid_ch)
        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)
        self.bn2 = nn.BatchNorm2d(out_ch)

    def forward(self, x): #看起来是平平无奇的卷积-BN-ReLU组成的块
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.activation(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        output = self.activation(x)

        return output
    
#Nested Unet

class NestedUNet(nn.Module):
    """
    Implementation of this paper:
    https://arxiv.org/pdf/1807.10165.pdf
    """
    def __init__(self, in_ch=3, out_ch=1):
        super(NestedUNet, self).__init__()

        n1 = 64
        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16] #通道数除了第一次，下采样每次加倍，上采样减半

        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)#池化
        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)#上采样
        #这是左边半个U,通道数分别是从0->1,1->2,2->3,3->4,其他的看图上结构即可，对应图上相应的编号
        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])
        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])
        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])
        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])
        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])

        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])
        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])
        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])
        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])

        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])
        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])
        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])

        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])
        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])

        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])

        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)


    def forward(self, x):
        
        x0_0 = self.conv0_0(x)
        x1_0 = self.conv1_0(self.pool(x0_0))#0->1,1->2,2->3,3->4的时候都有pool池化，减小图片大小
        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))#当0层的图片与1层图片结合的时候，1层图片要做上采样使之与0层图片大小相同，x0_1代表图上第0层第1列的⚪

        x2_0 = self.conv2_0(self.pool(x1_0))
        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))#x0_2是前面x0_0,x0_1,x1_1上采样结合，dense 连接，其他的类似

        x3_0 = self.conv3_0(self.pool(x2_0))
        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))
        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))

        x4_0 = self.conv4_0(self.pool(x3_0))
        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))
        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))
        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))
        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))

        output = self.final(x0_4)
        return output

````

## 2. Attention Unet
Attention-Unet模型是以Unet模型为基础的，可以从上图看出，Attention-Unet和U-net的区别就在于decoder时，从encoder提取的部分进行了Attention Gate再进行decoder。
<img src=https://s2.loli.net/2024/05/11/RqdnWIEMVivlLKN.png width='100%'>

代码:
```python
class AttU_Net(nn.Module):
    def __init__(self,img_ch=3,output_ch=1):
        super(AttU_Net,self).__init__()
        
        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)

        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)
        self.Conv2 = conv_block(ch_in=64,ch_out=128)
        self.Conv3 = conv_block(ch_in=128,ch_out=256)
        self.Conv4 = conv_block(ch_in=256,ch_out=512)
        self.Conv5 = conv_block(ch_in=512,ch_out=1024)

        self.Up5 = up_conv(ch_in=1024,ch_out=512)
        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)
        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)

        self.Up4 = up_conv(ch_in=512,ch_out=256)
        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)
        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)
        
        self.Up3 = up_conv(ch_in=256,ch_out=128)
        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)
        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)
        
        self.Up2 = up_conv(ch_in=128,ch_out=64)
        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)
        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)

        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)


    def forward(self,x):
        # encoding path
        x1 = self.Conv1(x)

        x2 = self.Maxpool(x1)
        x2 = self.Conv2(x2)
        
        x3 = self.Maxpool(x2)
        x3 = self.Conv3(x3)

        x4 = self.Maxpool(x3)
        x4 = self.Conv4(x4)

        x5 = self.Maxpool(x4)
        x5 = self.Conv5(x5)

        # decoding + concat path
        d5 = self.Up5(x5)
        x4 = self.Att5(g=d5,x=x4)
        d5 = torch.cat((x4,d5),dim=1)        
        d5 = self.Up_conv5(d5)
        
        d4 = self.Up4(d5)
        x3 = self.Att4(g=d4,x=x3)
        d4 = torch.cat((x3,d4),dim=1)
        d4 = self.Up_conv4(d4)

        d3 = self.Up3(d4)
        x2 = self.Att3(g=d3,x=x2)
        d3 = torch.cat((x2,d3),dim=1)
        d3 = self.Up_conv3(d3)

        d2 = self.Up2(d3)
        x1 = self.Att2(g=d2,x=x1)
        d2 = torch.cat((x1,d2),dim=1)
        d2 = self.Up_conv2(d2)

        d1 = self.Conv_1x1(d2)

        return d1
```

## 3. 3D Unet
3D图像可以提供额外的上下文信息。
3D U-net是U-net框架的基本拓展，支持3D立体分割。核心结构和U-net一样还是包含收缩和扩张路径，只是所有的2D操作都被相应的3D操作，即3D Conv、3D max pooling 和 3D upconvolutions所替代，从而产生三维分割图像。其中3D Conv与2DConv的区别的如下图，3D Conv包含了深度信息。

## 4. Vnet

V-Net是一种基于3D卷积神经网络的体积分割方法，它是U-Net的一个扩展，用于医学图像分割。V-Net的网络结构如下图所示，它包含了一个编码器和一个解码器，编码器用于提取特征，解码器用于还原分割结果。V-Net的编码器和解码器都是由多个3D卷积层和3D池化层组成，其中3D卷积层用于提取特征，3D池化层用于降低特征图的尺寸。V-Net的编码器和解码器之间还有一个跳跃连接，用于将编码器提取的特征图与解码器还原的特征图进行融合。V-Net的输出是一个二值图像，用于表示分割结果。

<img src=https://s2.loli.net/2024/05/11/UmgXuYyRtKHpAda.png width='100%'>

 从图像上可以看出，Vnet和Unet确实是很像的。但是由于论文是针对三维图像提出的，所以图中用方格代表feature map。另外，还可以看出，Vnet也借用了Unet从压缩路径叠加feature map，从而补充损失信息的方法(橙色线路)。这里需要特别说明的，也是Vnet和Unet最大的不同，
- **Vnet采用了ResNet的短路连接方式(灰色路线)。相当于在Unet中引入ResBlock**。这是Vnet最大的改进之处
- 另外,Vnet第一个stage在短路部分只进行了一次卷积操作，在第二个stage进行了两次…这和Unet每个stage卷积操作次数相同的结构是略有不同的。


## 5. ResUnet

在encoder block中引入残差: 
举例:LinkNet
<img src=https://s2.loli.net/2024/05/11/9hw5RWJAmupTzgY.png width='100%'>
编码块:
<img src=https://s2.loli.net/2024/05/11/oTnHpdZF4I38axl.png width='50%'>


## 6. Inception-Unet
大多数图像处理算法倾向于使用固定大小的filters进行卷积，但是调整模型以找到正确的筛选器大小通常很麻烦；此外，固定大小的filters仅适用于突出部分大小相似的图像，不适用于突出部分的形状大小变化较大的图像。一种解决方法是用更深的网络，另一种是Inception network。
<img src=https://s2.loli.net/2024/05/11/gAoe3qlQTc7zbdM.png width='100%'>


## 7. RU-Net and R2U-Net
循环U-net模型和循环残差U-net模型

<img src=https://s2.loli.net/2024/05/11/buaUdrgVRw9BljA.png width='100%'>

具体结构:
<img src=https://s2.loli.net/2024/05/11/C1hELkbVxAZfFu2.png width='100%'>

