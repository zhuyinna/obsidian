# 词向量的历史

- 2003年，Bengio等人提出了神经网络语言模型（NNLM），并在2008年提出了词嵌入模型（Word Embedding）。
- 2013年，Mikolov等人提出了word2vec模型，该模型是一个高效的词向量学习模型，可以学习出高质量的词向量。
    > 不足之处: word2vec模型只考虑了局部词频统计，没有考虑全局词频统计。其是一个静态的训练模型，无法表示出一词多义，你说“苹果”是指苹果公司呢，还是水果呢？

# ELMO
word2vec无法解决一词多义的问题, 静态指的是训练好之后这个单词的表达就固定住了, 无论新的句子上下文单词是什么, 这个单词的Word Embedding不会跟着上下文场景的变化而改变.

2018年，Peters等人提出了ELMO模型，ELMO模型是一个双层双向LSTM模型，可以学习出动态的词向量。ELMO模型的主要思想是：一个词的意思是由上下文决定的，因此一个词的词向量应该是上下文的函数。ELMO模型的输入是一个词序列，输出是每个词的词向量。ELMO模型的输出是一个词的词向量是上下文的函数，因此一个词的词向量是动态的。

<img src=https://s2.loli.net/2024/05/11/yS8jeYUKW4NLr16.png width='100%'>

图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。

前向LSTM的对数似然概率:
<img src=https://s2.loli.net/2024/05/11/JCREWsPwGkz15mF.png width='100%'>
后向:
<img src=https://s2.loli.net/2024/05/11/oM4b9pV1gfa5xUs.png width='100%'>
前向和后向结合:
<img src=https://s2.loli.net/2024/05/11/Z43CxPUHYTyg7Ko.png width='100%'>
其中Θx和Θs表示词向量矩阵和softmat层的参数，在前向和后向LSTM中都是共享的。

biLM的输出有2L+1个输出向量, 因为每一层LSTM都会有前向和后向两个向量输出，而每个词汇自己有embedding层的向量，因此总共是2L+1,表示如下：
<img src=https://s2.loli.net/2024/05/11/oZAfnuSPh6ar4CD.png width='100%'>


**具体的NLP任务**: ELMo会训练一个权重向量对每一个词汇的输出向量进行线性加权，表示如下
<img src=https://s2.loli.net/2024/05/11/96yOQjo7btH3n2R.png width='100%'>

其中$s^{task}$
 是softmax规范化后的权重向量，$\gamma^{task}$
 是一个放缩参数。由于biLM每一层的向量输出分布可能不同，因此，在进行线性加权之前，也可以考虑对每个向量进行Layer Normalization。


#


