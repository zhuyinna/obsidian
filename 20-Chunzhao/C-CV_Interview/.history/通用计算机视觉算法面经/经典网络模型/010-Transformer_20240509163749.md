# Transformer
Transformer架构最早是由谷歌在 2017 年的论文《Attention Is All You Need》中引入的，抛弃了以往深度学习任务里面使用到的CNN和RNN。它受欢迎的主要原因是其架构引入了并行化，Transformer 利用了强大的 TPU 和并行训练，从而减少了训练时间。此后在此基础上又出现了GPT、Bert等优秀模型，这些优秀模型都是在Transformer的基础上衍生出来的。

## Transformer中的三种注意力
<img src=https://s2.loli.net/2024/05/09/CHh3I2QxrvTyLDE.png width='100%'>