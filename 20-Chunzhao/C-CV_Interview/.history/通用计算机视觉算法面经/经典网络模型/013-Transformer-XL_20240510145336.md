**前言**
目前在NLP领域中，处理语言建模问题有两种最先进的架构：RNN和Transformer。RNN按照序列顺序逐个学习输入的单词或字符之间的关系，而Transformer则接收一整段序列，然后使用self-attention机制来学习它们之间的依赖关系。这两种架构目前来看都取得了令人瞩目的成就，但它们都局限在捕捉长期依赖性上。

为了解决这一问题，CMU联合Google Brain在2019年1月推出的一篇新论文《Transformer-XL：Attentive Language Models beyond a Fixed-Length Context》同时结合了RNN序列建模和Transformer自注意力机制的优点，在输入数据的每个段上使用Transformer的注意力模块，并使用循环机制来学习连续段之间的依赖关系。Transformer-XL在多种语言建模数据集（如单词级别的enwik8和字符级别的text8）上实现了目前的SoTA效果，且该模型在推理阶段速度更快，比之前最先进的利用Transformer进行语言建模的方法快300～1800倍。 同时，该论文也放出了其配套源码（包括TensorFlow和PyTorch的）、预训练模型及在各个数据集上训练的超参数，可以说是非常良心了

# TL;DR
Transformer由于自回归的特性，每个时间片的预测都需要从头开始，这样的推理速度限制了它在很多场景的应用。Transformer-XL提出的递归机制，使得推理过程以段为单位，段的长度越长，无疑提速越明显，从实验结果来看，Transformer-XL提速了300-1800倍，为Transformer-XL的使用提供了基础支撑。同时递归机制增加了Transformer-XL可建模的长期依赖的长度（O(NL)，这对提升模型的泛化能力也是很有帮助的。

仿照RPR，Transformer-XL提出了自己的相对位置编码算法，此编码方法对比Transformer和RPR都有了性能的提升，而且从理论角度也有了可解释性。

# Transformer-XL




