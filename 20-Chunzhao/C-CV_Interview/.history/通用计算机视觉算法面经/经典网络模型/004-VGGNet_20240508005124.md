VGGNet: 是牛津大学计算机视觉组在2014年提出的模型, 通过多个卷积层和池化层的堆叠来构建深度网络, 以提高网络的性能. VGGNet的网络结构非常简单, 但是却非常有效, 在ILSVRC2014比赛中取得了第二名的成绩. 

## 网络结构

VGGNet的网络结构如下图所示:

<img src=https://s2.loli.net/2024/05/07/Hq2CDiUhsfw3npW.png width='50%'>
<img src=https://s2.loli.net/2024/05/07/RELfSi85KzJa1Nj.png width='50%'>

其中VGG16和VGG19效果较好, 分别为D和E. 这6种网络结构相似，都是由5层卷积层、3层全连接层组成，区别在于每个卷积层的子层数量不同，从A至E依次增加，总的网络深度从11层到19层。表格中的卷积层参数表示为“conv（感受野大小）-通道数”，例如con3-64，表示使用3x3的卷积核，通道数为64；最大池化表示为maxpool，层与层之间使用maxpool分开；全连接层表示为“FC-神经元个数”，例如FC-4096表示包含4096个神经元的全连接层；最后是softmax层。

以VGG16为例(D): 第1层卷积层由2个conv3-64组成，第2层卷积层由2个conv3-128组成，第3层卷积层由3个conv3-256组成，第4层卷积层由3个conv3-512组成，第5层卷积层由3个conv3-512组成，然后是2个FC4096，1个FC1000。总共16层，这也就是VGG16名字的由来。

1. 输入层
224x224x3
2. 第一层 卷积层
第1层卷积层由2个conv3-64组成。
- 卷积: 3x3x3 64个, padding=1, stride=1
    -> 224x224x64
- Relu
- 卷积: 3x3x64 64个, padding=1, stride=1
    -> 224x224x64
- Relu
- 池化: 2x2 stride=2, 最大池化
    -> 112x112x64

3. 第二层 卷积层
- 卷积: 输入是112x112x64，使用128个3x3x64的卷积核进行卷积，padding=1，stride=1,
    -> 112x112x128
- Relu
- 卷积: 输入是112x112x128，使用128个3x3x128的卷积核进行卷积，padding=1，stride=1，根据公式
    -> 112x112x128
- Relu
- 池化: 使用2x2，stride=2的池化单元, 最大池化
    -> 56x56x128
4. 第三层 卷积层
- 卷积：输入是56x56x128，使用256个3x3x128的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(56+2*1-3)/1+1=56

得到输出是56x56x256。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 卷积：输入是56x56x256，使用256个3x3x256的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(56+2*1-3)/1+1=56

得到输出是56x56x256。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 池化：使用2x2，stride=2的池化单元进行最大池化操作（max pooling）。根据公式：

(56+2*0-2)/2+1=28

每组得到的输出为28x28x256。

5. 第四层 卷积层
- 卷积：输入是28x28x256，使用512个3x3x256的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(28+2*1-3)/1+1=28

得到输出是28x28x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 卷积：输入是28x28x512，使用512个3x3x512的卷积核进行卷积，padding=1，stride=1，根据公式：
(input_size + 2 * padding - kernel_size) / stride + 1=(28+2*1-3)/1+1=28

得到输出是28x28x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 卷积：输入是28x28x512，使用512个3x3x512的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(28+2*1-3)/1+1=28

得到输出是28x28x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。
  
- 池化：使用2x2，stride=2的池化单元进行最大池化操作（max pooling）。根据公式：

(28+2*0-2)/2+1=14

每组得到的输出为14x14x512。

6. 第五层 卷积层
- 卷积：输入是14x14x512，使用512个3x3x512的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(14+2*1-3)/1+1=14

得到输出是14x14x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 卷积：输入是14x14x512，使用512个3x3x512的卷积核进行卷积，padding=1，stride=1，根据公式：
  
(input_size + 2 * padding - kernel_size) / stride + 1=(14+2*1-3)/1+1=14

得到输出是14x14x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 卷积：输入是14x14x512，使用512个3x3x512的卷积核进行卷积，padding=1，stride=1，根据公式：

(input_size + 2 * padding - kernel_size) / stride + 1=(14+2*1-3)/1+1=14

得到输出是14x14x512。

- ReLU：将卷积层输出的FeatureMap输入到ReLU函数中。

- 池化：使用2x2，stride=2的池化单元进行最大池化操作（max pooling）。根据公式：

(14+2*0-2)/2+1=7

每组得到的输出为7x7x512。

7. 第六层 全连接层

- FC1：输入是7x7x512=25088，神经元个数为4096，将输入的数据与权重进行矩阵相乘，得到输出为4096。
- ReLU：将全连接层的输出输入到ReLU函数中。
- Dropout：使用Dropout方法防止过拟合。

8. 第七层 全连接层
- FC2：输入是4096，神经元个数为4096，将输入的数据与权重进行矩阵相乘，得到输出为4096。
- ReLU：将全连接层的输出输入到ReLU函数中。
- Dropout：使用Dropout方法防止过拟合。

9. 第八层 全连接层
- FC3：输入是4096，神经元个数为1000，将输入的数据与权重进行矩阵相乘，得到输出为1000。

10. 第九层 Softmax层
- Softmax：将全连接层的输出输入到Softmax函数中，得到最终的分类结果。

## 全卷积网络

全卷积网络是指卷积神经网络中只包含卷积层和池化层，不包含全连接层的网络。全卷积网络的优点是可以接受任意尺寸的输入，输出也是一个特征图，可以用于图像分割等任务。

VGG16 在训练的时候使用了全连接层，但是在预测的时候可以去掉全连接层，只保留卷积层和池化层，得到一个全卷积网络。

具体做法:
1. 第一层全连接层
输入7x7x512的FeatureMap, 使用4096个7x7x512的卷积核进行卷积, 由于卷积核尺寸与输入尺寸相同, 卷积后得到的FeatureMap尺寸为1x1x4096。 相当于全连接层的神经元个数为4096。

2. 第二层全连接层
输入1x1x4096的FeatureMap, 使用4096个1x1x4096的卷积核进行卷积, 由于卷积核尺寸与输入尺寸相同, 卷积后得到的FeatureMap尺寸为1x1x4096。 相当于全连接层的神经元个数为4096。

3. 第三层全连接层
输入1x1x4096的FeatureMap, 使用1000个1x1x4096的卷积核进行卷积, 由于卷积核尺寸与输入尺寸相同, 卷积后得到的FeatureMap尺寸为1x1x1000。 相当于全连接层的神经元个数为1000。

得到1x1x1000的输出之后, 输入到Softmax层中, 得到最终的分类结果。