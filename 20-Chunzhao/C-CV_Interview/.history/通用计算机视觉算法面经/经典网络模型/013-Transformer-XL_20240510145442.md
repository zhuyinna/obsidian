**前言**
目前在NLP领域中，处理语言建模问题有两种最先进的架构：RNN和Transformer。RNN按照序列顺序逐个学习输入的单词或字符之间的关系，而Transformer则接收一整段序列，然后使用self-attention机制来学习它们之间的依赖关系。这两种架构目前来看都取得了令人瞩目的成就，但它们都局限在捕捉长期依赖性上。

为了解决这一问题，CMU联合Google Brain在2019年1月推出的一篇新论文《Transformer-XL：Attentive Language Models beyond a Fixed-Length Context》同时结合了RNN序列建模和Transformer自注意力机制的优点，在输入数据的每个段上使用Transformer的注意力模块，并使用循环机制来学习连续段之间的依赖关系。Transformer-XL在多种语言建模数据集（如单词级别的enwik8和字符级别的text8）上实现了目前的SoTA效果，且该模型在推理阶段速度更快，比之前最先进的利用Transformer进行语言建模的方法快300～1800倍。 同时，该论文也放出了其配套源码（包括TensorFlow和PyTorch的）、预训练模型及在各个数据集上训练的超参数，可以说是非常良心了

# TL;DR
Transformer由于自回归的特性，每个时间片的预测都需要从头开始，这样的推理速度限制了它在很多场景的应用。Transformer-XL提出的递归机制，使得推理过程以段为单位，段的长度越长，无疑提速越明显，从实验结果来看，Transformer-XL提速了300-1800倍，为Transformer-XL的使用提供了基础支撑。同时递归机制增加了Transformer-XL可建模的长期依赖的长度(O(NL))，这对提升模型的泛化能力也是很有帮助的。

仿照RPR，Transformer-XL提出了自己的相对位置编码算法，此编码方法对比Transformer和RPR都有了性能的提升，而且从理论角度也有了可解释性。

# Transformer-XL

Transformer-XL的提出旨在解决上面所列出的Transformer的三个问题，为了解决上下文碎片和推理速度慢的问题，作者推出了片段递归机制，为了解决长期依赖，作者对绝对位置编码进行了改进，并推出了相对位置编码机制。下面分别详细介绍两个优化点。

## 片段递归
和Transformer一样，Transformer-XL在训练的时候也是以固定长度的片段的形式进行输入的，不同的是Transformer-XL的上一个片段的状态会被缓存下来然后在计算当前段的时候再重复使用上个时间片的隐层状态。因为上个片段的特征在当前片段进行了重复使用，这也就赋予了Transformer-XL建模更长期的依赖的能力。

## 相对位置编码

最先介绍相对位置编码的是论文《self-attention with relative positional representation》(后面简称RPR)。

Transformer-XL的相对位置编码参考了RPR中把相对位置编码加入到self-attention中的思想

