ResNet是2015年由微软亚洲研究院的何恺明等人提出的深度卷积神经网络。ResNet的核心思想是残差学习，通过引入残差单元解决了深度卷积神经网络训练过程中的梯度消失和梯度爆炸问题，使得可以训练更深的网络。ResNet在ILSVRC2015比赛中获得了冠军，同时也是当时ImageNet上最好的模型之一。

## 残差路径
可以分为两种, 一种有**bottleneck**结构, 即右图中的1x1卷积层, 用于先降维, 再升维, 主要出于降低计算复杂度的现实考虑, 称之为"bottleneck block"; 另一种没有, 称之为"basicblock"

<img src=https://s2.loli.net/2024/05/08/uXAyMEo4KNitwWd.png width='50%'>

<img src=https://s2.loli.net/2024/05/08/VxDcU6XquShFbNf.png width='50%'>

## 残差原理
咱们要求解的映射为：H(x)
现在咱们将这个问题转换为求解网络的残差映射函数，也就是F(x)，其中F(x) = H(x)-x。

残差：观测值与估计值之间的差。
这里H(x)就是观测值，x就是估计值（也就是上一层ResNet输出的特征映射）。
我们一般称x为identity Function，它是一个跳跃连接；称F(x)为ResNet Function。

那么咱们要求解的问题变成了H(x) = F(x)+x

>有小伙伴可能会疑惑，咱们干嘛非要经过F(x)之后在求解H(x)啊！整这么麻烦干嘛！
>咱们开始看图说话：如果是采用一般的卷积神经网络的化，原先咱们要求解的是H(x) = F(x)这个值对不？那么，我们现在假设，在我的网络达到某一个深度的时候，咱们的网络已经达到最优状态了，也就是说，此时的错误率是最低的时候，再往下加深网络的化就会出现退化问题（错误率上升的问题）。咱们现在要更新下一层网络的权值就会变得很麻烦，权值得是一个让下一层网络同样也是最优状态才行。对吧？
>但是采用残差网络就能很好的解决这个问题。还是假设当前网络的深度能够使得错误率最低，如果继续增加咱们的ResNet，为了保证下一层的网络状态仍然是最优状态，咱们只需要把令F(x)=0就好啦！因为x是当前输出的最优解，为了让它成为下一层的最优解也就是希望咱们的输出H(x)=x的话，是不是只要让F(x)=0就行了？
>当然上面提到的只是理想情况，咱们在真实测试的时候x肯定是很难达到最优的，但是总会有那么一个时刻它能够无限接近最优解。采用ResNet的话，也只用小小的更新F(x)部分的权重值就行啦！不用像一般的卷积层一样大动干戈！

两层结构的公式:
$a^{[l+2]}=Relu(W^{[l+2]}(Relu(W^{[l+1]}a{[l]}+b^{[l+1]})+b^{[l+2]}+a^{[l]})$

## 结构
34层, 用AvePool替代了FC，

<img src=https://s2.loli.net/2024/05/08/mNhi1RIbtUDvBVM.png width='400%'>