# Transformer
Transformer架构最早是由谷歌在 2017 年的论文《Attention Is All You Need》中引入的，抛弃了以往深度学习任务里面使用到的CNN和RNN。它受欢迎的主要原因是其架构引入了并行化，Transformer 利用了强大的 TPU 和并行训练，从而减少了训练时间。此后在此基础上又出现了GPT、Bert等优秀模型，这些优秀模型都是在Transformer的基础上衍生出来的。
————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/Morganfs/article/details/124373914

## Transformer中的三种注意力
<img src=https://s2.loli.net/2024/05/09/CHh3I2QxrvTyLDE.png width='100%'>