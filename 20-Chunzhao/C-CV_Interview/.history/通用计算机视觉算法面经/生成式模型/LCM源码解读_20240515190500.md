
# preliminary
## CM: consistency model
训练consistency model的方式——蒸馏预训练好的score model $s_{\phi}(\mathrm{x}, t)$
假设采样轨迹的时间序列为 $t_1=\epsilon<t_2<...<t_N=T$
通过运行数值ODE求解器的一个离散化步骤从$X_{t_{n+1}}$到$X_{t_{n}}$
![alt text](assets/LCM源码解读/image-13.png)
其中![alt text](assets/LCM源码解读/image-14.png)
![alt text](assets/LCM源码解读/image-15.png)
沿着ODE轨迹的分布进行第一次采样$x~p_{data}$, 然后添加高斯噪声, 生成一对在PF ODE轨迹上相邻的数据点![alt text](assets/LCM源码解读/image-16.png)
通过最小化这一对的输出差异来训练一致性模型，作者遵循一致性蒸馏损失来训练一致性模型，就有如下的consistency distillation loss：
![alt text](assets/LCM源码解读/image-17.png)
在蒸馏的过程中，作者用预训练模型来估计得分.
采用EMA来更新模型会提高训练的稳定性，并且性能会更好
![alt text](assets/LCM源码解读/image-18.png)

**CM train from scratch**: CM不仅能从预训练模型蒸馏, 也能单独训练. 因为预训练是借助教师模型来估计分数, 因此原作者提出了得分函数的无偏估计:
![alt text](assets/LCM源码解读/image-19.png)

![alt text](assets/LCM源码解读/image-20.png)
![alt text](assets/LCM源码解读/image-21.png)

## LCM中targetUnet作用

在这个模型蒸馏过程中，涉及了三个模型：`teacher_unet`、`unet`（学生模型）和 `target_unet`。`target_unet` 在这个过程中起到了一个特殊的作用。下面详细解释 `target_unet` 的作用及其对增强模型泛化性的原理。

### 作用

1. **教师模型 (`teacher_unet`)**：
   - **作用**：提供学习目标和指导信号。它通常是一个预训练的、性能良好的模型。在训练过程中，它的参数保持不变（通过 `requires_grad_(False)`），仅用于生成目标信号。
   
2. **学生模型 (`unet`)**：
   - **作用**：这个模型是实际进行训练的模型，它的参数会通过反向传播和优化器进行更新。学生模型从教师模型的输出中学习，以提高自己的性能。

3. **目标模型 (`target_unet`)**：
   - **作用**：`target_unet` 使用指数移动平均（Exponential Moving Average，EMA）更新，来跟踪学生模型参数的平滑版本。它不会直接参与梯度更新，而是通过聚合学生模型的历史参数来提供一个更加稳定和泛化性更好的参考模型。
   
### 目标模型的工作原理

1. **指数移动平均 (EMA)**：
   - **EMA 更新**：在每次训练更新时，`target_unet` 的参数是通过一个小的步长（通常是 0.99 或 0.999）向当前学生模型参数移动。例如，对于每个参数 θ_target_unet 和 θ_unet，有：
     \[
     \theta_{\text{target\_unet}} = \alpha \cdot \theta_{\text{target\_unet}} + (1 - \alpha) \cdot \theta_{\text{unet}}
     \]
     其中，α 是一个接近 1 的常数（如 0.999）。
   
2. **参数平滑**：
   - **平滑效应**：EMA 平滑了训练过程中学生模型参数的波动，尤其是在训练初期。由于参数更新是逐渐变化的，这种平滑有助于减少由于小批量（batch）波动或高学习率引起的噪音，从而使模型更加稳定。
   
3. **增强泛化性**：
   - **稳定的目标信号**：`target_unet` 提供了一个更加稳定的目标信号，可以用于在训练后期指导学生模型，从而提高其泛化性能。
   - **对抗过拟合**：由于 EMA 平滑的特性，`target_unet` 在训练过程中捕捉了学生模型在不同训练阶段的平均表现。这种平均化处理有助于对抗过拟合，提高模型在未见数据上的表现。

### 具体应用

在代码中，`target_unet` 的初始化和设置如下：
```python
# 8. Create online (`unet`) student U-Nets. This will be updated by the optimizer (e.g. via backpropagation.)
# Add `time_cond_proj_dim` to the student U-Net if `teacher_unet.config.time_cond_proj_dim` is None
if teacher_unet.config.time_cond_proj_dim is None:
    teacher_unet.config["time_cond_proj_dim"] = args.unet_time_cond_proj_dim
unet = StochasticDepthUNet(**teacher_unet.config)
# load teacher_unet weights into unet
unet.load_state_dict(teacher_unet.state_dict(), strict=False)
unet.train()

# 9. Create target (`ema_unet`) student U-Net parameters. This will be updated via EMA updates (polyak averaging).
# Initialize from unet
target_unet = StochasticDepthUNet(**teacher_unet.config)
target_unet.load_state_dict(unet.state_dict())
target_unet.train()  # 确保 Batch Normalization 和 Dropout 层的行为符合训练模式的预期
target_unet.requires_grad_(False)
```

在训练过程中，`target_unet` 的参数将不会被梯度更新，而是通过 EMA 从 `unet` 中获取更新。这确保了 `target_unet` 始终提供一个平滑且稳定的目标信号，以帮助 `unet` 提高泛化性能。

### 总结

`target_unet` 的主要作用是通过 EMA 技术，为学生模型提供一个平滑和稳定的参数参考。通过这种方式，可以减少模型训练过程中的波动，增强模型的稳定性和泛化能力。这种方法在许多高级机器学习应用中已被证明是有效的，尤其是在对抗过拟合和提高模型泛化性方面。



guidance_scale_embedding: 

```python
# From LatentConsistencyModel.get_guidance_scale_embedding
def guidance_scale_embedding(w, embedding_dim=512, dtype=torch.float32):
    """
    See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298

    Args:
        timesteps (`torch.Tensor`):
            generate embedding vectors at these timesteps
        embedding_dim (`int`, *optional*, defaults to 512):
            dimension of the embeddings to generate
        dtype:
            data type of the generated embeddings

    Returns:
        `torch.FloatTensor`: Embedding vectors with shape `(len(timesteps), embedding_dim)`
    """
    assert len(w.shape) == 1
    w = w * 1000.0

    half_dim = embedding_dim // 2
    emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)
    emb = w.to(dtype)[:, None] * emb[None, :]
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    if embedding_dim % 2 == 1:  # zero pad
        emb = torch.nn.functional.pad(emb, (0, 1))
    assert emb.shape == (w.shape[0], embedding_dim)
    return emb
```

# LCM 源码

## 配置参数

训练命令:

```sh
runwayml/stable-diffusion-v1-5
PROGRAM="train_lcm_distill_sd_wds.py \
    --pretrained_teacher_model=$MODEL_DIR \
    --output_dir=$OUTPUT_DIR \
    --mixed_precision=fp16 \
    --resolution=512 \      # 修改分辨率
    --learning_rate=1e-6 --loss_type="huber" --ema_decay=0.95 --adam_weight_decay=0.0 \     # 修改训练参数
    --max_train_steps=1000 \        # 最大训练步数
    --max_train_samples=4000000 \       # ???
    --dataloader_num_workers=8 \
    --train_shards_path_or_url='pipe:aws s3 cp s3://muse-datasets/laion-aesthetic6plus-min512-data/{00000..01210}.tar -' \
    --validation_steps=200 \
    --checkpointing_steps=200 --checkpoints_total_limit=10 \
    --train_batch_size=12 \
    --gradient_checkpointing --enable_xformers_memory_efficient_attention \
    --gradient_accumulation_steps=1 \
    --use_8bit_adam \       # ???
    --resume_from_checkpoint=latest \
    --report_to=wandb \
    --seed=453645634 \
    --push_to_hub \
```


## 源码分析

### 0. accelerator, logging 准备

```python
    ## 0. 准备： logging和accelerator
    logging_dir = Path(args.output_dir, args.logging_dir)

    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)

    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_config=accelerator_project_config,
        split_batches=True,  # It's important to set this to True when using webdataset to get the right number of steps for lr scheduling. If set to False, the number of steps will be devide by the number of processes assuming batches are multiplied by the number of processes
    )

    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        if args.push_to_hub:
            create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
                token=args.hub_token,
                private=True,
            ).repo_id
```

### 1. noise scheduler
noise scheduler中的 alpha_schedule和sigma_schedule用于x_0的预测
DDIMSolver用于通过z_{n+k}z_{n}的

```python
    # 1. Create the noise scheduler and the desired noise schedule.
    noise_scheduler = DDPMScheduler.from_pretrained(
        args.pretrained_teacher_model, subfolder="scheduler", revision=args.teacher_revision
    )

    # The scheduler calculates the alpha and sigma schedule for us
    alpha_schedule = torch.sqrt(noise_scheduler.alphas_cumprod)
    sigma_schedule = torch.sqrt(1 - noise_scheduler.alphas_cumprod)
    solver = DDIMSolver(
        noise_scheduler.alphas_cumprod.numpy(),
        timesteps=noise_scheduler.config.num_train_timesteps,
        ddim_timesteps=args.num_ddim_timesteps,
    )
```

具体的DDIMsolver如下:
该段代码对应的是Denoising Diffusion Implicit Models论文中的(12)式：
![alt text](assets/LCM源码解读/image-6.png)
注意上式中的$\alpha_t$对应的是DDPM论文中的$\overline{\alpha_t}$
因此alpha_schedule = torch.sqrt(noise_scheduler.alphas_cumprod)对应的是$\alpha_t$

```python
def extract_into_tensor(a, t, x_shape):
    b, *_ = t.shape  # *_: ignore the rest of the dimensions    
    out = a.gather(-1, t)  
    return out.reshape(b, *((1,) * (len(x_shape) - 1)))


class DDIMSolver:
    def __init__(self, alpha_cumprods, timesteps=1000, ddim_timesteps=50):
        # DDIM sampling parameters
        step_ratio = timesteps // ddim_timesteps
        self.ddim_timesteps = (np.arange(1, ddim_timesteps + 1) * step_ratio).round().astype(np.int64) - 1
        self.ddim_alpha_cumprods = alpha_cumprods[self.ddim_timesteps]
        self.ddim_alpha_cumprods_prev = np.asarray(
            [alpha_cumprods[0]] + alpha_cumprods[self.ddim_timesteps[:-1]].tolist()
        )
        # convert to torch tensors
        self.ddim_timesteps = torch.from_numpy(self.ddim_timesteps).long()
        self.ddim_alpha_cumprods = torch.from_numpy(self.ddim_alpha_cumprods)
        self.ddim_alpha_cumprods_prev = torch.from_numpy(self.ddim_alpha_cumprods_prev)

    def to(self, device):
        self.ddim_timesteps = self.ddim_timesteps.to(device)
        self.ddim_alpha_cumprods = self.ddim_alpha_cumprods.to(device)
        self.ddim_alpha_cumprods_prev = self.ddim_alpha_cumprods_prev.to(device)
        return self

    def ddim_step(self, pred_x0, pred_noise, timestep_index):
        alpha_cumprod_prev = extract_into_tensor(self.ddim_alpha_cumprods_prev, timestep_index, pred_x0.shape)
        dir_xt = (1.0 - alpha_cumprod_prev).sqrt() * pred_noise
        x_prev = alpha_cumprod_prev.sqrt() * pred_x0 + dir_xt
        return x_prev

```

### 2-6 Load所需的模块并Freeze无需训练的模块
其中teacher_Unet导入预训练权重

```python

# 2. Load tokenizers from SD-XL checkpoint.
tokenizer = AutoTokenizer.from_pretrained(
    args.pretrained_teacher_model, subfolder="tokenizer", revision=args.teacher_revision, use_fast=False
)

# 3. Load text encoders from SD-1.5 checkpoint.
# import correct text encoder classes
text_encoder = CLIPTextModel.from_pretrained(
    args.pretrained_teacher_model, subfolder="text_encoder", revision=args.teacher_revision
)

# 4. Load VAE from SD-XL checkpoint (or more stable VAE)
vae = AutoencoderKL.from_pretrained(
    args.pretrained_teacher_model,
    subfolder="vae",
    revision=args.teacher_revision,
)

# 5. Load teacher U-Net from SD-XL checkpoint
teacher_unet = UNet2DConditionModel.from_pretrained(
    args.pretrained_teacher_model, subfolder="unet", revision=args.teacher_revision
)

# 6. Freeze teacher vae, text_encoder, and teacher_unet
vae.requires_grad_(False)
text_encoder.requires_grad_(False)
teacher_unet.requires_grad_(False)
```


### 8-9 创建online student Unet和target Unet(EMA_Unet)
```python
# 8. Create online (`unet`) student U-Nets. This will be updated by the optimizer (e.g. via backpropagation.)
# Add `time_cond_proj_dim` to the student U-Net if `teacher_unet.config.time_cond_proj_dim` is None
if teacher_unet.config.time_cond_proj_dim is None:
    teacher_unet.config["time_cond_proj_dim"] = args.unet_time_cond_proj_dim
unet = UNet2DConditionModel(**teacher_unet.config)
# load teacher_unet weights into unet
unet.load_state_dict(teacher_unet.state_dict(), strict=False)
unet.train()

# 9. Create target (`ema_unet`) student U-Net parameters. This will be updated via EMA updates (polyak averaging).
# Initialize from unet
target_unet = UNet2DConditionModel(**teacher_unet.config)
target_unet.load_state_dict(unet.state_dict())
target_unet.train()
target_unet.requires_grad_(False)

# Check that all trainable models are in full precision
low_precision_error_string = (
    " Please make sure to always have all model weights in full float32 precision when starting training - even if"
    " doing mixed precision training, copy of the weights should still be float32."
)

if accelerator.unwrap_model(unet).dtype != torch.float32:
    raise ValueError(
        f"Controlnet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}"
    )
```

### 10-11.precision和device对齐，处理保存和加载checkpoints

```python
# 10. Handle mixed precision and device placement
# For mixed precision training we cast all non-trainable weigths to half-precision
# as these weights are only used for inference, keeping weights in full precision is not required.
weight_dtype = torch.float32
if accelerator.mixed_precision == "fp16":
    weight_dtype = torch.float16
elif accelerator.mixed_precision == "bf16":
    weight_dtype = torch.bfloat16

# Move unet, vae and text_encoder to device and cast to weight_dtype
# The VAE is in float32 to avoid NaN losses.
vae.to(accelerator.device)
if args.pretrained_vae_model_name_or_path is not None:
    vae.to(dtype=weight_dtype)
text_encoder.to(accelerator.device, dtype=weight_dtype)

# Move teacher_unet to device, optionally cast to weight_dtype
target_unet.to(accelerator.device)
teacher_unet.to(accelerator.device)
if args.cast_teacher_unet:
    teacher_unet.to(dtype=weight_dtype)

# Also move the alpha and sigma noise schedules to accelerator.device.
alpha_schedule = alpha_schedule.to(accelerator.device)
sigma_schedule = sigma_schedule.to(accelerator.device)
solver = solver.to(accelerator.device)

# 11. Handle saving and loading of checkpoints
# `accelerate` 0.16.0 will have better support for customized saving
if version.parse(accelerate.__version__) >= version.parse("0.16.0"):
    # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            target_unet.save_pretrained(os.path.join(output_dir, "unet_target"))

            for i, model in enumerate(models):
                model.save_pretrained(os.path.join(output_dir, "unet"))

                # make sure to pop weight so that corresponding model is not saved again
                weights.pop()
    def load_model_hook(models, input_dir):
        load_model = UNet2DConditionModel.from_pretrained(os.path.join(input_dir, "unet_target"))
        target_unet.load_state_dict(load_model.state_dict())
        target_unet.to(accelerator.device)
        del load_model

        for i in range(len(models)):
            # pop models so that they are not loaded again
            model = models.pop()

            # load diffusers style into model
            load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder="unet")
            model.register_to_config(**load_model.config)

            model.load_state_dict(load_model.state_dict())
            del load_model

    accelerator.register_save_state_pre_hook(save_model_hook)
    accelerator.register_load_state_pre_hook(load_model_hook)
```







**开始训练**

- 数据处理

```python

    for epoch in range(first_epoch, args.num_train_epochs):
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(unet):  # Enable gradient accumulation
                image, text, _, _ = batch

                image = image.to(accelerator.device, non_blocking=True)
                encoded_text = compute_embeddings_fn(text)

                pixel_values = image.to(dtype=weight_dtype)
                if vae.dtype != weight_dtype:
                    vae.to(dtype=weight_dtype)

                # encode pixel values with batch size of at most 32
                latents = []
                for i in range(0, pixel_values.shape[0], 32):  # 32 is the maximum batch size for encoding
                    latents.append(vae.encode(pixel_values[i : i + 32]).latent_dist.sample())
                latents = torch.cat(latents, dim=0)

                latents = latents * vae.config.scaling_factor
                latents = latents.to(weight_dtype)

                # Sample noise that we'll add to the latents
                noise = torch.randn_like(latents)
                bsz = latents.shape[0]

```

- 时间步
skip timestep策略: 

![alt text](assets/LCM源码解读/image.png)

```python

# Sample a random timestep for each image t_n ~ U[0, N - k - 1] without bias.
topk = noise_scheduler.config.num_train_timesteps // args.num_ddim_timesteps  
index = torch.randint(0, args.num_ddim_timesteps, (bsz,), device=latents.device).long()
start_timesteps = solver.ddim_timesteps[index]
timesteps = start_timesteps - topk
timesteps = torch.where(timesteps < 0, torch.zeros_like(timesteps), timesteps)

```

- cskip, cout: 为了保证fθ(x, ε) = x
> 一致性模型参数化
> 对于任意的一致性函数f ( ⋅ , ⋅ ) `f(\cdot, \cdot)f(⋅,⋅)`，用神经网络来拟合。但要满足两个条件：①同一个轨迹上的点输出一致；②在起始点f为一个对于x的恒等函数
> 第一种做法简单的参数化:
> ![alt text](assets/LCM源码解读/image-11.png)
> 第二种做法使用跳跃连接(CM和LCM使用的这个)
> ![alt text](assets/LCM源码解读/image-12.png)


来源于CM: 
![alt text](assets/LCM源码解读/image-1.png)
其中cskip和cout的计算公式如下(CM论文page25), LCM中因为最小步设为0因此有所改动，不过目的都是为了满足boundary condition:
$c_{skip}(0)=1$和$c_{cout}(0)=0$
![alt text](assets/LCM源码解读/image-7.png)



```python
# 20.4.4. Get boundary scalings for start_timesteps and (end) timesteps.
c_skip_start, c_out_start = scalings_for_boundary_conditions(start_timesteps)
c_skip_start, c_out_start = [append_dims(x, latents.ndim) for x in [c_skip_start, c_out_start]]   # 这里append_dims扩展的是哪个维度？
c_skip, c_out = scalings_for_boundary_conditions(timesteps)
c_skip, c_out = [append_dims(x, latents.ndim) for x in [c_skip, c_out]]
```

其中维度对齐:
```python
def append_dims(x, target_dims):
    """Appends dimensions to the end of a tensor until it has target_dims dimensions."""
    dims_to_append = target_dims - x.ndim
    if dims_to_append < 0:
        raise ValueError(f"input has {x.ndim} dims but target_dims is {target_dims}, which is less")
    return x[(...,) + (None,) * dims_to_append]
```
$c_{skip},c_{out}$ : torch.Size([32,1,1,1])

```python
# From LCMScheduler.get_scalings_for_boundary_condition_discrete
def scalings_for_boundary_conditions(timestep, sigma_data=0.5, timestep_scaling=10.0):
    c_skip = sigma_data**2 / ((timestep / 0.1) ** 2 + sigma_data**2)
    c_out = (timestep / 0.1) / ((timestep / 0.1) ** 2 + sigma_data**2) ** 0.5
    return c_skip, c_out
```

- 加噪
20.4.5对应扩散前向加噪过程，得到的noisy_model_input为含噪程度不同的noisy latents
```python
# 20.4.5. Add noise to the latents according to the noise magnitude at each timestep
# (this is the forward diffusion process) [z_{t_{n + k}} in Algorithm 1]
noisy_model_input = noise_scheduler.add_noise(latents, noise, start_timesteps)
```


- CFG: 增加了w和c, 其中w是从[args.w_max - args.w_min]中随机选择
> 疑问: 这里w_max和w_min设置为多少? 不是一个确定值吗?
20.4.6对应guidance scale的随机采样过程并将其进行embedding操作转为w_embedding
![alt text](assets/LCM源码解读/image-2.png)


```python
# 20.4.6. Sample a random guidance scale w from U[w_min, w_max] and embed it
w = (args.w_max - args.w_min) * torch.rand((bsz,)) + args.w_min
w_embedding = guidance_scale_embedding(w, embedding_dim=args.unet_time_cond_proj_dim)
w = w.reshape(bsz, 1, 1, 1)
# Move to U-Net device and dtype
w = w.to(device=latents.device, dtype=latents.dtype)
w_embedding = w_embedding.to(device=latents.device, dtype=latents.dtype)
```

20.4.8将之前prompt embeddings相应的值赋值给变量prompt_embeds
```python
# 20.4.8. Prepare prompt embeds and unet_added_conditions
prompt_embeds = encoded_text.pop("prompt_embeds")
```

- 预测噪声, 预测x_0

```python
# 20.4.9. Get online LCM prediction on z_{t_{n + k}}, w, c, t_{n + k}
noise_pred = unet(
    noisy_model_input,
    start_timesteps,
    timestep_cond=w_embedding,
    encoder_hidden_states=prompt_embeds.float(),
    added_cond_kwargs=encoded_text,
).sample

pred_x_0 = predicted_origin(
    noise_pred,
    start_timesteps,
    noisy_model_input,
    noise_scheduler.config.prediction_type,
    alpha_schedule,
    sigma_schedule,
)
```

其中预测x_0的原理如下:
来源:https://www.zhangzhenhu.com/aigc/ddim.html
![alt text](assets/LCM源码解读/image-4.png)
![alt text](assets/LCM源码解读/image-5.png)

原论文:LCM
![alt text](assets/LCM源码解读/image-8.png)

```python
# Compare LCMScheduler.step, Step 4
def predicted_origin(model_output, timesteps, sample, prediction_type, alphas, sigmas):
    if prediction_type == "epsilon":
        sigmas = extract_into_tensor(sigmas, timesteps, sample.shape)
        alphas = extract_into_tensor(alphas, timesteps, sample.shape)
        pred_x_0 = (sample - sigmas * model_output) / alphas
    elif prediction_type == "v_prediction":
        sigmas = extract_into_tensor(sigmas, timesteps, sample.shape)
        alphas = extract_into_tensor(alphas, timesteps, sample.shape)
        pred_x_0 = alphas * sample - sigmas * model_output
    else:
        raise ValueError(f"Prediction type {prediction_type} currently not supported.")

    return pred_x_0


def extract_into_tensor(a, t, x_shape):
    b, *_ = t.shape  # *_: ignore the rest of the dimensions    
    out = a.gather(-1, t)  
    return out.reshape(b, *((1,) * (len(x_shape) - 1)))
```


- 得到最终模型输出, 当t为0时, model_pred即为x_0

![alt text](assets/LCM源码解读/image-3.png)

```python
model_pred = c_skip_start * noisy_model_input + c_out_start * pred_x_0
```

- teacher Unet

![alt text](assets/LCM源码解读/image-9.png)

其中20.4.12中target对应的是
![alt text](assets/LCM源码解读/image-10.png)

```python
# 20.4.10. Use the ODE solver to predict the kth step in the augmented PF-ODE trajectory after
# noisy_latents with both the conditioning embedding c and unconditional embedding 0
# Get teacher model prediction on noisy_latents and conditional embedding
with torch.no_grad():
    with torch.autocast("cuda"):
        cond_teacher_output = teacher_unet(
            noisy_model_input.to(weight_dtype),
            start_timesteps,
            encoder_hidden_states=prompt_embeds.to(weight_dtype),
        ).sample
        cond_pred_x0 = predicted_origin(
            cond_teacher_output,
            start_timesteps,
            noisy_model_input,
            noise_scheduler.config.prediction_type,
            alpha_schedule,
            sigma_schedule,
        )

        # Get teacher model prediction on noisy_latents and unconditional embedding
        uncond_teacher_output = teacher_unet(
            noisy_model_input.to(weight_dtype),
            start_timesteps,
            encoder_hidden_states=uncond_prompt_embeds.to(weight_dtype),
        ).sample
        uncond_pred_x0 = predicted_origin(
            uncond_teacher_output,
            start_timesteps,
            noisy_model_input,
            noise_scheduler.config.prediction_type,
            alpha_schedule,
            sigma_schedule,
        )

        # 20.4.11. Perform "CFG" to get x_prev estimate (using the LCM paper's CFG formulation)
        pred_x0 = cond_pred_x0 + w * (cond_pred_x0 - uncond_pred_x0)
        pred_noise = cond_teacher_output + w * (cond_teacher_output - uncond_teacher_output)
        x_prev = solver.ddim_step(pred_x0, pred_noise, index)      

```

```python
                # 20.4.12. Get target LCM prediction on x_prev, w, c, t_n
                with torch.no_grad():
                    with torch.autocast("cuda", dtype=weight_dtype):
                        target_noise_pred = target_unet(
                            x_prev.float(),
                            timesteps,
                            timestep_cond=w_embedding,
                            encoder_hidden_states=prompt_embeds.float(),
                        ).sample
                    pred_x_0 = predicted_origin(
                        target_noise_pred,
                        timesteps,
                        x_prev,
                        noise_scheduler.config.prediction_type,
                        alpha_schedule,
                        sigma_schedule,
                    )
                    target = c_skip * x_prev + c_out * pred_x_0

                # 20.4.13. Calculate loss
                if args.loss_type == "l2":
                    loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
                elif args.loss_type == "huber":
                    loss = torch.mean(
                        torch.sqrt((model_pred.float() - target.float()) ** 2 + args.huber_c**2) - args.huber_c
                    )

                # 20.4.14. Backpropagate on the online student model (`unet`)
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=True)
```


### 梯度反传,checkpoint保存, tagetUnet(EMA)更新

```python
             # Checks if the accelerator has performed an optimization step behind the scenes
            if accelerator.sync_gradients:
                # 20.4.15. Make EMA update to target student model parameters
                update_ema(target_unet.parameters(), unet.parameters(), args.ema_decay)
                progress_bar.update(1)
                global_step += 1

                if accelerator.is_main_process:
                    if global_step % args.checkpointing_steps == 0:
                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                        if args.checkpoints_total_limit is not None:
                            checkpoints = os.listdir(args.output_dir)
                            checkpoints = [d for d in checkpoints if d.startswith("checkpoint")]
                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[1]))

                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
                            if len(checkpoints) >= args.checkpoints_total_limit:
                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
                                removing_checkpoints = checkpoints[0:num_to_remove]

                                logger.info(
                                    f"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints"
                                )
                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")

                                for removing_checkpoint in removing_checkpoints:
                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                    shutil.rmtree(removing_checkpoint)

                        save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                        accelerator.save_state(save_path)
                        logger.info(f"Saved state to {save_path}")

                    if global_step % args.validation_steps == 0:
                        log_validation(vae, target_unet, args, accelerator, weight_dtype, global_step, "target")
                        log_validation(vae, unet, args, accelerator, weight_dtype, global_step, "online")

            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)

            if global_step >= args.max_train_steps:
                break

    # Create the pipeline using using the trained modules and save it.
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        unet = accelerator.unwrap_model(unet)
        unet.save_pretrained(os.path.join(args.output_dir, "unet"))

        target_unet = accelerator.unwrap_model(target_unet)
        target_unet.save_pretrained(os.path.join(args.output_dir, "unet_target"))

    accelerator.end_training()
```