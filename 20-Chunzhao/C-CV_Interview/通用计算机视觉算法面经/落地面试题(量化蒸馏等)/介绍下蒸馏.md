深度学习在计算机视觉、语音识别、自然语言处理等内的众多领域中均取得了令人难以置信的性能。但是，大多数模型在计算上过于昂贵，无法在移动端或嵌入式设备上运行。因此需要对模型进行压缩，且知识蒸馏是模型压缩中重要的技术之一。

**1. 提升模型精度**

如果对目前的网络模型A的精度不是很满意，那么可以先训练一个更高精度的teacher模型B（通常参数量更多，时延更大），然后用这个训练好的teacher模型B对student模型A进行知识蒸馏，得到一个更高精度的A模型。

**2. 降低模型时延，压缩网络参数**

如果对目前的网络模型A的时延不满意，可以先找到一个时延更低，参数量更小的模型B，通常来讲，这种模型精度也会比较低，然后通过训练一个更高精度的teacher模型C来对这个参数量小的模型B进行知识蒸馏，使得该模型B的精度接近最原始的模型A，从而达到降低时延的目的。

**3. 标签之间的域迁移**

假如使用狗和猫的数据集训练了一个teacher模型A，使用香蕉和苹果训练了一个teacher模型B，那么就可以用这两个模型同时蒸馏出一个可以识别狗、猫、香蕉以及苹果的模型，将两个不同域的数据集进行集成和迁移。

因此，在工业界中对知识蒸馏和迁移学习也有着非常强烈的需求。





知识蒸馏被广泛的用于模型压缩和迁移学习当中。开山之作应该是[”Distilling the Knowledge in a Neural Network“](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531)。这篇文章中，作者的motivation是找到一种方法，把多个模型的知识提炼给单个模型。

文章的标题是Distilling the Knowledge in a Neural Network，那么说明是神经网络的知识呢？一般认为模型的参数保留了模型学到的知识，因此最常见的迁移学习的方式就是在一个大的数据集上先做预训练，然后使用预训练得到的参数在一个小的数据集上做微调（两个数据集往往领域不同或者任务不同）。例如先在Imagenet上做预训练，然后在COCO数据集上做检测。在这篇论文中，作者认为可以将模型看成是黑盒子，知识可以看成是输入到输出的映射关系。因此，我们可以先训练好一个teacher网络，然后将teacher的网络的输出结果 ![[公式]](https://www.zhihu.com/equation?tex=q) 作为student网络的目标，训练student网络，使得student网络的结果 ![[公式]](https://www.zhihu.com/equation?tex=p) 接近 ![[公式]](https://www.zhihu.com/equation?tex=q) ，因此，我们可以将损失函数写成 ![[公式]](https://www.zhihu.com/equation?tex=L%3DCE%28y%2C+p%29%2B%5Calpha+CE%28q%2C+p%29) 。这里CE是交叉熵（Cross Entropy），y是真实标签的onehot编码，q是teacher网络的输出结果，p是student网络的输出结果。

但是，直接使用teacher网络的softmax的输出结果 ![[公式]](https://www.zhihu.com/equation?tex=q) ，可能不大合适。因此，一个网络训练好之后，对于正确的答案会有一个很高的置信度。例如，在MNIST数据中，对于某个2的输入，对于2的预测概率会很高，而对于2类似的数字，例如3和7的预测概率为 ![[公式]](https://www.zhihu.com/equation?tex=10%5E%7B-6%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=10%5E%7B-9%7D) 。这样的话，teacher网络学到数据的相似信息（例如数字2和3，7很类似）很难传达给student网络。由于它们的概率值接近0。因此，文章提出了softmax-T，公式如下所示：

![[公式]](https://www.zhihu.com/equation?tex=q_i%3D%5Cfrac%7Bexp%28z_i%2FT%29%7D%7B%5Csum_j+exp%28z_j%2FT%29%7D) 

这里 ![[公式]](https://www.zhihu.com/equation?tex=q_i) 是student网络学习的对象（soft targets），![[公式]](https://www.zhihu.com/equation?tex=z_i) 是神经网络softmax前的输出logit。如果将T取1，这个公式就是softmax，根据logit输出各个类别的概率。如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。

