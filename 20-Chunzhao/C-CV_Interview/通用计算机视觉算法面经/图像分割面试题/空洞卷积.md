## 空洞卷积

空洞卷积（dilated convolution）是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。利用添加空洞扩大感受野，让原本 $3\times 3$的卷积核，在相同参数量的情况下，用于$5 \times 5(dilated rate=2)$或者更大的感受野，无须下采样。

扩张卷积（Dilated Convolution）又称空洞卷积（Atrous Convolution），向卷积层引入了一个称为 “扩张率(dilation rate)”的新参数，该参数定义了卷积核处理数据时各值的间距。换句话说，相比原来的标准卷积，扩张卷积多了一个超参数称之为dilation rate（扩张率），指的是kernel各点之间的间隔数量，正常的卷积核的扩张率为1。

![空洞卷积](https://files.mdnice.com/user/15197/dfcc549f-ca3f-47ac-86aa-556c775d78b5.png)



![空洞卷积动态采样展示](https://files.mdnice.com/user/15197/9e53b17d-edb5-40c2-bc31-8d652ae6a0c3.gif)

上图是一个扩张率为2，尺寸为 $3×3$ 的空洞卷积，感受野与$5×5$的卷积核相同，而且仅需要9个参数。你可以把它想象成一个5×5的卷积核，每隔一行或一列删除一行或一列。在相同的计算条件下，空洞卷积提供了更大的感受野。空洞卷积经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。

我们来探究一下感受野


![](https://files.mdnice.com/user/15197/359144a5-8558-4bc8-8774-f64396baf8af.png)

第一层的一个$5×5$大小的区域经过2次3×3的标准卷积之后，变成了一个点。也就是说从size上来讲，2层$3*3$卷积转换相当于1层$5*5$卷积。从以上图的演化也可以看出，一个5×5的卷积核是可以由2次连续的3×3的卷积代替。

![](https://files.mdnice.com/user/15197/ec94e17e-217b-4f78-8c55-edcee39bed02.png)

但对于$dilated=2$，$3 \times 3$ 的扩张卷积核。可以看到第一层$13\times 13$的区域，经过2次$3 \times 3$的扩张卷积之后，变成了一个点。即从size上来讲，连续2层的$3 \times 3$空洞卷积转换相当于1层$13 \times 13$卷积。


意义与问题
- 最早出现在DeeplLab系列中，作用：可以在不改变特征图尺寸的同时增大感受野，摈弃了pool的做法（丢失信息）；
- Dilation convolution(扩张卷积)的原理其实也比较简单，就是在kernel各个像素点之间加入0值像素点，变向的增大核的尺寸从而增大感受野。

经过dilation rate放大后，卷积核尺寸为：$d \times (k-1)+1$；

扩张卷积的输入和输出特征图的尺寸关系如下：
$$
{\color{Purple}W_{2} = \frac{ W_{1} + 2p -d(k-1)-1}{s} + 1 }
$$

当在s=1,k=3时，令d = p，则输出特征图尺寸不变；

**空洞卷积存在的问题：**
![](https://files.mdnice.com/user/15197/b18b3c11-3d0d-436a-8fb9-c3b1a513b6cd.png)

**Gridding效应**

- 局部信息丢失：由于空洞卷积的计算方式类似于棋盘格式，某一层得到的卷积结果，来自上一层的独立的集合，没有相互依赖，因此该层的卷积结果之间没有相关性，即局部信息丢失。

通过图a解释了空洞卷积存在的问题，三层卷积均为r=2的空洞卷积,可以看出红色像素的感受野为13。这种空洞卷积核并不连续，所以不是所有的pixel都用来计算，且参与实际计算的只有$75 \%$，损失了信息的连续性，这对像素级密集预测任务来说事不适用的。

多次叠加多个具有相同空洞率的卷积核会造成格网中有一些像素自始至终都没有参与运算，不起任何作用，这对于像素级别的预测是不友好的。

**Long-ranged information might be not relevant.**

- 远距离获取的信息没有相关性：由于空洞卷积稀疏的采样输入信号，用来获取远距离信息。但是这种信息之间没有相关性，同时当对大物体分割时，会有一定的效果，但是对于小物体来说，有弊无利。

我们从空洞卷积的设计背景来看就能推测出这样的设计是用来获取长距离信息。然而光采用大采样率的信息或许只对一些大物体分割有效果，而对小物体来说可能则有弊无利了。如何同时处理不同大小的物体的关系，则是设计好 dilated convolution 网络的关键。

总结：简单来说，就是空洞卷积虽然在参数不变的情况下保证了更大的感受野，但是对于一些很小的物体，本身就不要那么大的感受野来说，这是不友好的。


**图森组提出HDC的方案解决该问题。**
1. 第一个特性是，叠加卷积的空洞率不能有大于1的公约数。比如 [2, 4, 6] 则不是一个好的三层卷积，依然会出现 gridding effect。
2. 第二个特性是，我们将空洞率设计成锯齿状结构，例如 $[1, 2, 5, 1, 2, 5]$ 循环结构。
3. 第三个特性是，我们需要满足一下这个式子： 
$$
M_i = max \begin{bmatrix} M_{i+1} -2r_i, \  M_{i+1} - 2(M_{i+1} - r_i), \ r_i \end{bmatrix}
$$

其中 $r_i$ 是 $i$ 层的 空洞率，而 $M_i$ 是指在 $i$ 层的最大空洞率，那么假设总共有 $n$ 层的话，默认 $M_n = r_n$ 。假设我们应用于 kernel 为 $k \times k$ 的话，我们的目标则是 $M_2 <= k$  ，这样我们至少可以用空洞率为1，即标准卷积的方式来覆盖掉所有洞。

一个简单的例子:  dilation rate [1, 2, 5] with 3 x 3 kernel (可行的方案)

![](https://files.mdnice.com/user/15197/2c98a6b3-14eb-427a-85a1-4bbb3a5efc51.png)


而这样的锯齿状本身的性质就比较好的来同时满足小物体大物体的分割要求(小 dilation rate 来关心近距离信息，大 dilation rate 来关心远距离信息)。这样我们的卷积依然是连续的也就依然能满足VGG组观察的结论，大卷积是由小卷积的 regularisation 的 叠加。以下的对比实验可以明显看出，一个良好设计的 dilated convolution 网络能够有效避免 gridding effect.

![](https://files.mdnice.com/user/15197/f04d80dd-7cd3-4cf0-9de4-a00debcedf92.png)







### 参考

- https://www.zhihu.com/question/54149221/answer/323880412