设有一个两层（一层隐含层，一层输出层）的深度学习回归模型，我们的激活函数设置为relu，我们的损失函数用均方误差，模型构造方程如下：



![img](https://pic4.zhimg.com/v2-41fcf71dae5281b72f1630bc0ffc0d5f_b.png)



那么此时，如果我们初始化参数全部为0的话，上述模型的前向传播结果为：



![img](https://pic2.zhimg.com/v2-e590cec9a9cea18452de03fa695a7c25_b.png)



然后，我们再对原方程进行反向传播计算。

我们先得到Relu函数的导数：



![img](https://pic3.zhimg.com/v2-37e7dd8173100dbfac1b53c7efbedcea_b.png)



然后使用链式求导计算：



![img](https://pic2.zhimg.com/v2-5f481c28adef719d833ed973af98eed9_b.png)



由于此处偏导已经为0，那么后续的偏导数求解均为0，即（此处不再详细推导）：



![img](https://pic3.zhimg.com/v2-fea2cac73977bcc4963b41a027e47efe_b.png)



也就是说，我们的参数是无法更新的，那么结果也就导致我们的模型时无法很好的拟合，这个现象类似于“梯度弥散”，即当我们的偏导数为0时，我们无法对模型参数进行求解，也就导致模型训练失败（其他激活函数，大家可以试着按照上面的解析过程自行推导）。

因此，在进行深度学习模型训练时，我们在进行参数初始化一定要切记不能初始化为0，否则后果很严重。

