作者：13472722199
链接：https://zhuanlan.zhihu.com/p/77466700
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



卷积神经网络是在空间上挖掘特征，例如对象检测大多数使用卷积神经网络，一个对象在图像中移动，对象里相邻点像素具有一定固有特征。循环神经网络是在时序的数据中挖掘特征，例如语言模型中经常使用LSTM，GRU来处理文字序列特征。如果不谈训练准确率，时空是可以变换的，在卷积神经网络能使用的地方，常常也能使用循环神经网络来训练。1D的卷积可以训练语言模型。图像也可以使用循环神经网络来训练。

**1.RNN**

**1.1 RNN网络结构**

下图是RNN图示，简单说明是：有三个参数矩阵 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bxh%7D%2Cw_%7Bhh%7D%2Cw_%7Bhy%7D) ，三层网络结构(输入层，隐藏层，输出层)，隐藏输出带入下次输入。

![img](https://pic1.zhimg.com/v2-7411a9a1883ea5c1af2ae57aae6e3770_b.jpg)![img](https://pic1.zhimg.com/80/v2-7411a9a1883ea5c1af2ae57aae6e3770_1440w.jpg)

设t时刻输入![[公式]](https://www.zhihu.com/equation?tex=X_t+%5Cin+R%5E%7Bn%2Ad%7D) ，输入矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bxh%7D+%5Cin+R%5E%7Bd%2Ah%7D) ，隐藏层矩阵为： ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bhh%7D+%5Cin+R%5E%7Bh%2Ah%7D) ,隐藏层输入 ![[公式]](https://www.zhihu.com/equation?tex=H_%7Bt-1%7D+%5Cin+R%5E%7Bn%2Ah%7D) , ![[公式]](https://www.zhihu.com/equation?tex=X_tW_%7Bxh%7D+%5Cin+R%5E%7Bn%2Ah%7D+) , ![[公式]](https://www.zhihu.com/equation?tex=H_%7Bt-1%7D%2AW_%7Bhh%7D+%5Cin+R%5E%7Bn%2Ah%7D) ，输出矩阵为 ![[公式]](https://www.zhihu.com/equation?tex=W_%7Bhy%7D+%5Cin+R%5E%7Bh%2Aq%7D) , 输出为：![[公式]](https://www.zhihu.com/equation?tex=H_tW_%7Bhy%7D+%5Cin+R%5E%7Bn%2Aq%7D) 

![[公式]](https://www.zhihu.com/equation?tex=H_t+%3D+X_tW_%7Bxh%7D+%2B+H_%7Bt-1%7DW_%7Bhh%7D) 当然经常写成 ![[公式]](https://www.zhihu.com/equation?tex=H_t+%3D+tanh%28X_tW_%7Bxh%7D+%2B+H_%7Bt-1%7DW_%7Bhh%7D+%2B+b_h%29) 

称 ![[公式]](https://www.zhihu.com/equation?tex=H_t) 为隐藏层。

![[公式]](https://www.zhihu.com/equation?tex=y_t+%3D+H_tW_%7Bhy%7D+%2B+b_y) 

![[公式]](https://www.zhihu.com/equation?tex=y_t) 通常使用softmax进行输出。

**1.2. RNN的梯度反向传播(BP)**

**1.2.1不带时间序列的神经网络计算梯度**

下图是一个神经网络的一部分。

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D%2Ca%5E%7B%28l%2B1%29%7D%2Ca%5E%7B%28l%2B2%29%7D) 计算需要假定每层的 ![[公式]](https://www.zhihu.com/equation?tex=w) 已知。数据通过网络后，根据loss，来更新每层的 ![[公式]](https://www.zhihu.com/equation?tex=w) 来减少loss。因此每层的输出 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D%2Ca%5E%7B%28l%2B1%29%7D%2Ca%5E%7B%28l%2B2%29%7D) 是由网络的forward计算的， ![[公式]](https://www.zhihu.com/equation?tex=w) 是由误差向后BP计算的。

![img](https://pic3.zhimg.com/v2-3c5e6d1441a4ebe16a5a079663d54c36_b.jpg)![img](https://pic3.zhimg.com/80/v2-3c5e6d1441a4ebe16a5a079663d54c36_1440w.jpg)

根据链式求导法则，有下式成立。

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28a%5E%7B%28l%29%7D_%7B%7D%29%7D+%3D+%28w%5E%7B%28l%2B1%29%7D%29%5ET%5Ccdot+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28a%5E%7B%28l%2B1%29%7D_%7B%7D%29%7D+%3D+%28w%5E%7B%28l%2B1%29%7D%29%5ET%5Ccdot+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28f%5E%7B%28l%2B1%29%7D_%7B%7D%28a%5E%7B%28l%2B1%29%7D%29%29%7D%2A%5Cfrac%7B%5Cpartial%28f%5E%7B%28l%2B1%29%7D%28a%5E%7B%28l%2B1%29%7D%29%29%7D%7B%5Cpartial%28a%5E%7B%28l%2B1%29%7D_%7B%7D%29%7D)  ....(1)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28a%5E%7B%28l%29%7D_%7B%7D%29%7D+%3D+%28w%5E%7B%28l%2B1%29%7D%29%5ET%5Ccdot+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28a%5E%7B%27%28l%2B1%29%7D%29%7D%2Af%5E%7B%27%28n%2B1%29%7D)         ...........(2)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28w%5E%7B%28l%2B1%29%7D_%7B%7D%29%7D+%3D+a%5E%7B%28l%29%7D+%2A+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28a%5E%7B%28l%2B1%29%7D_%7B%7D%29%7D)                           ..........  (3)

**1.2.2 带时间序列的神经网络梯度**

假设一个RNN网络如下图。

![img](https://pic1.zhimg.com/v2-1e657f4c46cee5a590445bf971e87a38_b.jpg)![img](https://pic1.zhimg.com/80/v2-1e657f4c46cee5a590445bf971e87a38_1440w.jpg)

loss函数：求 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28W_%7Bhh%7D%29%7D+%3D+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28y_0%29%7D%2A%5Cfrac%7B%5Cpartial%28y_0%29%7D%7B%5Cpartial%28W_%7Bhh%7D%29%7D+%2B+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28y_1%29%7D%2A%5Cfrac%7B%5Cpartial%28y_1%29%7D%7B%5Cpartial%28W_%7Bhh%7D%29%7D+%2B+%5Cfrac%7B%5Cpartial%28L%29%7D%7B%5Cpartial%28y_2%29%7D%2A%5Cfrac%7B%5Cpartial%28y_2%29%7D%7B%5Cpartial%28W_%7Bhh%7D%29%7D)

![img](https://pic4.zhimg.com/v2-cd397cac626784cb28c21ef88eaa058b_b.jpg)![img](https://pic4.zhimg.com/80/v2-cd397cac626784cb28c21ef88eaa058b_1440w.jpg)

根据链式法则对 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bhh%7D%E6%B1%82%E5%AF%BC%E5%BE%97%EF%BC%9A) 

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%28y_2%29%7D%7B%5Cpartial%28W_%7Bhh%7D%29%7D+%3D) ![[公式]](https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cpartial%28y_2%29%7D%7B%5Cpartial%28h_%7B2%7D%29%7D%2A%5Cfrac%7B%5Cpartial%28h_2%29%7D%7B%5Cpartial%28w_%7Bhh%7D%29%7D) = ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bhy%7D%5ET%5Ccdot+%5B%5Cfrac%7B%5Cpartial%28h2%29%7D%7B%5Cpartial%28w_%7Bhh%7D%29%7D%2B+w_%7Bhh%7D+%5Ccdot%5B%5Cfrac%7B%5Cpartial%28h1%29%7D%7B%5Cpartial%28w_%7Bhh%7D%29%7D%2B+w_%7Bhh%7D%5Ccdot+%5Cfrac%7B%5Cpartial%28h0%29%7D%7B%5Cpartial%28w_%7Bhh%7D%29%7D+%5D%5D+) 

梯度是t的指数函数，如果 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw_%7Bhh%7D%7C%7C+%3E+1) ,连续乘积后会出现梯度爆炸，反之当 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw_%7Bhh%7D%7C%7C+%3C+1) 会出现梯度消失问题。

梯度爆炸现在已不是很严重的问题，可以通过梯度修剪（Gradient clipping）来避免。梯度消失问题是RNN的通过LSTM来解决梯度消失问题。

**2. LSTM神经网络**

**2.1 LSTM神经网络结构**

![img](https://pic4.zhimg.com/v2-b04b67157aac2c0e666a14be57384edf_b.jpg)![img](https://pic4.zhimg.com/80/v2-b04b67157aac2c0e666a14be57384edf_1440w.jpg)

三个门 ![[公式]](https://www.zhihu.com/equation?tex=F_t%2CI_t%2CO_t) 分别是遗忘门，输入门和输出门:

![[公式]](https://www.zhihu.com/equation?tex=F_t+%3D+%5Csigma%28X_tWxf+%2B+H_%7Bhf%7DH_%7Bt-1%7D+%2B+b_f%29) 

![[公式]](https://www.zhihu.com/equation?tex=I_t+%3D+%5Csigma%28X_tWxi+%2B+H_%7Bhi%7DH_%7Bt-1%7D+%2B+b_i%29) 

![[公式]](https://www.zhihu.com/equation?tex=O_t+%3D+%5Csigma%28X_tWxo+%2B+H_%7Bho%7DH_%7Bt-1%7D+%2B+b_o%29) 

三个输入： ![[公式]](https://www.zhihu.com/equation?tex=X_t) , ![[公式]](https://www.zhihu.com/equation?tex=H_%7Bt-1%7D) , ![[公式]](https://www.zhihu.com/equation?tex=C_%7Bt-1%7D) 输入，上次隐藏层输出以及上次记忆细胞值。

候选记忆细胞 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BC_t%7D+%3D+tanh%28X_tW_%7Bxc%7D+%2B+H_%7Bt-1%7DW_%7Bhc%7D+%2B+b_c%29) 

记忆细胞输出 ![[公式]](https://www.zhihu.com/equation?tex=C_t+%3D+F_t%5Codot+C_%7Bt-1%7D+%2B+I_t%5Codot%5Ctilde%7BC_t%7D) 

![[公式]](https://www.zhihu.com/equation?tex=F_t%2CI_t) 的值分布在[0,1]之间，当前记忆细胞组合了当前输入和前次记忆细胞状态。如果遗忘 ⻔⼀直近似1且输⼊⻔⼀直近似0，过去的记忆细胞将⼀直通过时间保存并传递⾄当前时间步。这 个设计可以应对循环神经⽹络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较⼤的依 赖关系。

隐藏层输出为 ![[公式]](https://www.zhihu.com/equation?tex=H_t+%3D+O_%7Bt%7D%5Codot+tanh%28C_t%29) 

输出 ![[公式]](https://www.zhihu.com/equation?tex=Y_t+%3D+H_t%2AW_%7Bhq%7D+%2B+b_q) 

**3. 门控循环单元GRU**

**3.1 门控循环单元GRU神经网络结构**

![img](https://pic1.zhimg.com/v2-6a03fb17e6940cca2ed8885281b3cd8c_b.jpg)![img](https://pic1.zhimg.com/80/v2-6a03fb17e6940cca2ed8885281b3cd8c_1440w.jpg)

两个门重置门，更新门： ![[公式]](https://www.zhihu.com/equation?tex=R_t%2CZ_t) 。

![[公式]](https://www.zhihu.com/equation?tex=R_t+%3D+%5Csigma%28X_tW_%7Bxr%7D+%2B+H_%7Bt-1%7DW_%7Bhr%7D+%2B+br%29) 

![[公式]](https://www.zhihu.com/equation?tex=Z_t+%3D+%5Csigma%28X_tW_%7Bxz%7D+%2B+H_%7Bt-1%7DW_%7Bhz%7D+%2B+b_z%29) 

候选隐藏层： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7BH_t%7D+%3D+tanh%28X_tWxh+%2B+%28R_t%5Codot+H_%7Bt-1%7D%29W_%7Bhh%7D+%2B+b_h%29) 

隐藏层： ![[公式]](https://www.zhihu.com/equation?tex=H_t+%3D+Z_t%5Codot+H_%7Bt-1%7D+%2B+%281-+Z_t%29%5Codot+%5Ctilde%7BH%7D) 

输出层： ![[公式]](https://www.zhihu.com/equation?tex=Y_t+%3D+H_tW_%7Bhq%7D+%2B+b_q) 
