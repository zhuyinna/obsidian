# Motion CLIP

![[Pasted image 20230919105011.png]]


## Framework
![[Pasted image 20230919105309.png]]

CLIP è®©å›¾åƒå’Œæ–‡æœ¬ä¸¤ä¸ªä¸åŒæ¨¡æ€åˆ†åˆ«å¯¹åº”çš„ç‰¹å¾ï¼Œå…±äº«åŒä¸€ä¸ªç‰¹å¾ç©ºé—´ï¼Œä»è€Œå»ºç«‹ä¸¤æ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚motionclip åˆ™å¸Œæœ› human motion çš„ç‰¹å¾ç©ºé—´å’Œ CLIP çš„ç‰¹å¾ç©ºé—´å¯¹é½ï¼Œä»è€Œå»ºç«‹æ–‡æœ¬å’Œ human motion ä¹‹é—´çš„å¯¹åº”æ˜ å°„å…³ç³»ã€‚

ä¸€æ—¦å»ºç«‹æ˜ å°„å…³ç³»ï¼Œå°±å¯ä»¥é€šè¿‡æ–‡æœ¬å»æŸ¥è¯¢å¯¹åº”çš„ human motionï¼ŒåŒæ—¶å€ŸåŠ©æ–‡æœ¬è¡¨è¾¾çš„ä¸°å¯Œæ€§ä»¥åŠ CLIP ç‰¹å¾ç©ºé—´çš„æ³›åŒ–æ€§ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬å¾—åˆ°é™¤äº†è®­ç»ƒæ•°æ®é›†ä»¥å¤–çš„ human motionã€‚

![[Pasted image 20230919161751.png]]
### Encoder

Maps a motion sequence ğ‘1:ğ‘‡ to its latent representation ğ‘§ğ‘
ğ‘§ğ‘ = ğ¸ (ğ‘§ğ‘¡ğ‘˜, ğ‘1:ğ‘‡ )
### Decoder
the query sequence is simply the positional encoding of 1 : ğ‘‡
We further use a differentiable SMPL layer to get the mesh vertices locations, Ë† ğ‘£1:ğ‘‡ .

### Losses
ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š
![[Pasted image 20230919162339.png]]
Lreconï¼šè¯¥è‡ªåŠ¨ç¼–ç å™¨ç»è¿‡è®­ç»ƒï¼Œé€šè¿‡å…³èŠ‚æ–¹å‘ã€å…³èŠ‚é€Ÿåº¦å’Œé¡¶ç‚¹ä½ç½®ä¸Šçš„é‡å»º L2 æŸå¤±æ¥è¡¨ç¤ºè¿åŠ¨ã€‚æ˜ç¡®åœ°è¯´
![[Pasted image 20230919162358.png]]
![[Pasted image 20230919162412.png]]

### settingç»†èŠ‚
![[Pasted image 20230919161851.png]]

## Result
- Action
  ä¸ JL2P æ¯”è¾ƒï¼šå› ä¸ºä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ‰€ä»¥å»ºç«‹äº†æ–°çš„æ ·æœ¬é›†ï¼Œç”¨æˆ·è¯„åˆ¤ç”Ÿæˆåºåˆ—è´¨é‡
  æ•ˆæœå¥½äº JL2P
  ![[Pasted image 20230919165446.png|525]]
- style
  é£æ ¼è¿ç§»æ–¹é¢ï¼Œæ•ˆæœä¸å¦‚ Alberman
![[Pasted image 20230919164825.png|500]]
![[Pasted image 20230919164745.png|500]]
- abstract language
  é€šè¿‡ CLIP å¯ä»¥ç”Ÿæˆè®­ç»ƒé›†ä»¥å¤–çš„åŠ¨ä½œ

## Applications
- Latent-Based Editing
  å¯ä»¥åœ¨å­¦ä¹ å¾—åˆ°çš„ç‰¹å¾ç©ºé—´ä¸Šè¿›è¡Œç‰¹å¾å‘é‡åŠ å‡æ“ä½œå®ç°åŠ¨ä½œç»„åˆï¼Œé£æ ¼è¿ç§»ä»¥åŠæ’å€¼ç­‰æ•ˆæœ
- Action Recognition
  å¯ä»¥çœ‹æˆ motion-textï¼Ÿä½†æ˜¯ motionclip è®¾è®¡ä¸æ˜¯ä¸ºäº† recognitionï¼Œæ‰€ä»¥ç²¾åº¦ä¸è¶³
![[Pasted image 20230919170220.png]]


## Related work
- CVAE
  Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems 28 (2015).
- seq2seq RNN
  Matthias Plappert, Christian Mandery, and Tamim Asfour. 2018. Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks. Robotics and Autonomous Systems 109 (2018), 13â€“26.
- latent space+text and motion pairs
  Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. 2018. Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions. IEEE Robotics and Automation Letters 3, 4 (2018), 3441â€“3448.
- JL2P æ¨¡å‹åœ¨æ–‡æœ¬çš„ç»†å¾®æ¦‚å¿µï¼ˆå³é€Ÿåº¦ã€è½¨è¿¹å’ŒåŠ¨ä½œç±»å‹ï¼‰æ”¹è¿›çš„ç»“æœ
  Chaitanya Ahuja and Louis-Philippe Morency. 2019. Language2pose: Natural language grounded pose forecasting. In 2019 International Conference on 3D Vision (3DV). IEEE, 719â€“728
![[Pasted image 20230919154812.png]]

## ä¸è¶³
MotionCLIP (Tevet et al., 2022) could generate stylized motions, but **it is still limited to short text inputs and fails to handle complicated motion descriptions.** In addition, they (Petrovich et al., 2022; Tevet et al., 2022) typically only **accept a single text prompt, which greatly limits usersâ€™ creativity.**

