---
time: 2022-08-04
publish: ECCV'2022
---

# TM2T
2022-08-04
![[Pasted image 20230918190439.png]]


## Introduction

目标：集成 motion2text 和 text2motion

现有的工作主要集中在运动字幕 (motion2text) [13,43] 或基于语言的运动生成 (text2motion) [2,11,26] 的单向映射，只有两个 [37,56] 探索视觉 3D 运动的集成及其纹理描述。然而，当运动长度超过 3-4 秒时，这两项研究都倾向于产生静态姿势序列。两者都需要初始姿势和目标运动长度作为输入。它们也是确定性方法。也就是说，它们中的每一个总是从给定的文本脚本生成相同的动作。第一个无生命运动现象很大程度上归因于直接使用原始 3D 姿势作为其运动表示，这是不必要的冗余，但无法捕获底层运动动力学的局部上下文。第二个问题根源于它们的确定性动作生成过程，这与我们的日常体验相反，在我们的日常体验中，角色在相同的纹理脚本下执行时通常存在多种不同的动作风格。对初始状态和目标长度的调节进一步对实际可行施加了严格的约束。


## Approach overview
![[Pasted image 20230918194206.png]]

