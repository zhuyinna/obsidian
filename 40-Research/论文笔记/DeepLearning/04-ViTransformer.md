---
 Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, Illia Polosukhin
 Date: 2017
 Date Added: 2022-10-18
 Topics: Transformer
 Keywords: Self-Attention;
---
tags:  #æ·±åº¦å­¦ä¹ 

## Metadata
 Authors: "[[Ashish Vaswani]], [[Noam Shazeer]], [[Niki Parmar]], [[Jakob Uszkoreit]], [[Llion Jones]], [[Aidan N Gomez]], [[Åukasz Kaiser]], [[Illia Polosukhin]]"
 Date: [[2017]] 
 Date Added: [[2022-10-18]] 
 Topics: "[[Transformer]]"
 URL: [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)

## Zotero Links 
 PDF Attachments
	- [Full Text PDF](zotero://open-pdf/library/items/RI5HPDQN) 
 [Local library](zotero://select/items/1_4GB5ZVZW) 


## Abstract

The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.


## Summary
  Transformer æ˜¯ä¸€ä¸ªåˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥æé«˜æ¨¡å‹è®­ç»ƒé€Ÿåº¦çš„æ¨¡å‹ã€‚å…³äºæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å‚çœ‹ [[02-Seq2Seq]]ï¼Œtrasnformer å¯ä»¥è¯´æ˜¯å®Œå…¨åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå› ä¸ºå®ƒé€‚ç”¨äºå¹¶è¡ŒåŒ–è®¡ç®—ï¼Œå’Œå®ƒæœ¬èº«æ¨¡å‹çš„å¤æ‚ç¨‹åº¦å¯¼è‡´å®ƒåœ¨ç²¾åº¦å’Œæ€§èƒ½ä¸Šéƒ½è¦é«˜äºä¹‹å‰æµè¡Œçš„ RNN å¾ªç¯ç¥ç»ç½‘ç»œã€‚

```ad-note
ä¸seq2seqçš„åŒºåˆ«ï¼š
**The difference from the Seq2Seq model is that, at each timestep, we re-feed the entire output sequence generated thus far, rather than just the last word.**
```
## Conclusion
  In this work, we presented the Transformer, the first sequence transduction model based entirely on attention**, replacing the recurrent layers** most commonly used in encoder-decoder architectures with **multi-headed self-attention**.
  
  plan: plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. ğŸ”¤æˆ‘ä»¬è®¡åˆ’å°† Transformer æ‰©å±•åˆ°æ¶‰åŠé™¤æ–‡æœ¬ä»¥å¤–çš„è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼çš„é—®é¢˜ï¼Œå¹¶ç ”ç©¶å±€éƒ¨çš„ã€å—é™çš„æ³¨æ„åŠ›æœºåˆ¶ä»¥æœ‰æ•ˆå¤„ç†å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤§å‹è¾“å…¥å’Œè¾“å‡ºã€‚å‡å°‘ç”Ÿæˆé¡ºåºæ˜¯æˆ‘ä»¬çš„å¦ä¸€ä¸ªç ”ç©¶ç›®æ ‡ã€‚ğŸ”¤



## Background / Problem Statement
  ä½œè€…é‡‡ç”¨ Attention æœºåˆ¶çš„åŸå› æ˜¯è€ƒè™‘åˆ° RNNï¼ˆæˆ–è€… LSTMï¼ŒGRU ç­‰ï¼‰çš„è®¡ç®—é™åˆ¶ä¸ºæ˜¯é¡ºåºçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ RNN ç›¸å…³ç®—æ³•åªèƒ½ä»å·¦å‘å³ä¾æ¬¡è®¡ç®—æˆ–è€…ä»å³å‘å·¦ä¾æ¬¡è®¡ç®—ï¼Œè¿™ç§æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªé—®é¢˜ï¼š

1.  æ—¶é—´ç‰‡ $t$ çš„è®¡ç®—ä¾èµ–Â $t-1$ æ—¶åˆ»çš„è®¡ç®—ç»“æœï¼Œè¿™æ ·é™åˆ¶äº†æ¨¡å‹çš„å¹¶è¡Œèƒ½åŠ›ï¼›
2.  é¡ºåºè®¡ç®—çš„è¿‡ç¨‹ä¸­ä¿¡æ¯ä¼šä¸¢å¤±ï¼Œå°½ç®¡ LSTM ç­‰é—¨æœºåˆ¶çš„ç»“æ„ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†é•¿æœŸä¾èµ–çš„é—®é¢˜ï¼Œä½†æ˜¯å¯¹äºç‰¹åˆ«é•¿æœŸçš„ä¾èµ–ç°è±¡, LSTM ä¾æ—§æ— èƒ½ä¸ºåŠ›ã€‚


## Method (s)

### encoder
1. Encoder
     **QKVï¼Ÿ**
         Q æ˜¯ä¸€ç»„æŸ¥è¯¢è¯­å¥ï¼ŒV æ˜¯æ•°æ®åº“ï¼ŒK æ˜¯ä¸€ç»„é’¥åŒ™ï¼Œä»£è¡¨äº† V ä¸­æ¯ä¸€é¡¹çš„æŸç§æŸ¥è¯¢ç‰¹å¾ã€‚æ‰€ä»¥ K å’Œ V çš„æ•°é‡ä¸€å®šæ˜¯ç›¸ç­‰çš„ï¼Œç»´åº¦åˆ™æ²¡æœ‰ä¸¥æ ¼é™åˆ¶ã€‚åš attention æ—¶ç»´åº¦å’Œ Q ä¸€æ ·åªæ˜¯ä¸ºäº†åœ¨åšç‚¹ç§¯æ—¶æ–¹ä¾¿ï¼Œä¸è¿‡ä¹Ÿå­˜åœ¨ä¸ç”¨ç‚¹ç§¯çš„ attentionã€‚å¯¹äºæ¯ä¸€ä¸ª Q ä¸­çš„ qï¼Œæ±‚å’Œæ¯ä¸€ä¸ª k çš„ attentionï¼Œä½œä¸ºå¯¹åº” value çš„åŠ æƒç³»æ•°ï¼Œå¹¶ç”¨å®ƒæ¥åŠ æƒæ•°æ®åº“ä¸­ V ä¸­çš„æ¯ä¸€é¡¹ï¼Œå°±å¾—åˆ°äº† q æœŸæœ›çš„æŸ¥è¯¢ç»“æœã€‚
         e.g. 
             (1) å¯¹äºä¸€ä¸ªæ–‡æœ¬ï¼Œå¸Œæœ›æ‰¾åˆ°æŸå¼ å›¾ç‰‡ä¸­å’Œæ–‡æœ¬æè¿°ç›¸å…³çš„å±€éƒ¨å›¾åƒï¼šæ–‡æœ¬åš queryï¼Œå›¾åƒåš value
             (2) å¯¹äºä¸€ä¸ªå›¾åƒï¼Œå¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªæ–‡æœ¬ä¸­å’Œå›¾åƒæ‰€å«å†…å®¹æœ‰å…³çš„å±€éƒ¨æ–‡æœ¬ï¼šå›¾åƒåš queryï¼Œæ–‡æœ¬åš value
             (3) è‡ªæ³¨æ„åŠ›ï¼šå¥å­ä¸­æŸä¸ªè¯åœ¨æ•´ä¸ªå¥å­ä¸­çš„åˆ†é‡ï¼ˆæˆ–è€…ç›¸å…³æ–‡æœ¬ï¼‰ï¼šå¥å­æœ¬èº«ä¹˜ä»¥ä¸‰ä¸ªçŸ©é˜µå¾—åˆ° QKVï¼Œæ¯ä¸ªè¯å»æŸ¥å®Œæ•´çš„å¥å­
             (4) äº¤å‰æ³¨æ„åŠ›ï¼štransformer æ¨¡å‹çš„ decoder ä¸­ï¼Œç”± decoder çš„è¾“å…¥ç»è¿‡å˜æ¢ä½œä¸º queryï¼Œç”± encoder çš„è¾“å‡ºä½œä¸º key å’Œ valueï¼ˆæ•°æ®åº“ï¼‰ã€‚value å’Œ query æ¥è‡ªä¸åŒçš„åœ°æ–¹ï¼Œå°±æ˜¯äº¤å‰æ³¨æ„åŠ›ã€‚å¯ä»¥çœ‹åˆ° key å’Œ value ä¸€å®šæ˜¯ä»£è¡¨ç€åŒä¸€ä¸ªä¸œè¥¿ã€‚å³: $[Q,(K,V)]$
        $W_QW_KW_V$ çŸ©é˜µéšæœºåˆå§‹åŒ–å¾—åˆ°ï¼Œè®­ç»ƒè¿‡ç¨‹æ¢¯åº¦ä¸‹é™æ›´æ–°
    
    **Add&Norm**
        **Add**æŒ‡Â **X**+MultiHeadAttention (**X**)ï¼Œæ˜¯ä¸€ç§æ®‹å·®è¿æ¥ï¼Œé€šå¸¸ç”¨äºè§£å†³å¤šå±‚ç½‘ç»œè®­ç»ƒçš„é—®é¢˜ï¼Œå¯ä»¥è®©ç½‘ç»œåªå…³æ³¨å½“å‰å·®å¼‚çš„éƒ¨åˆ†ï¼Œåœ¨ ResNet ä¸­ç»å¸¸ç”¨åˆ°ï¼š![[Pasted image 20230322153328.png|500]]
        **Norm**æŒ‡ Layer Normalizationï¼Œé€šå¸¸ç”¨äº RNN ç»“æ„ï¼ŒLayer Normalization ä¼šå°†æ¯ä¸€å±‚ç¥ç»å…ƒçš„è¾“å…¥éƒ½è½¬æˆå‡å€¼æ–¹å·®éƒ½ä¸€æ ·çš„ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«æ”¶æ•›ã€‚
    ![[Pasted image 20230323105713.png|525]]
        
    **Self-Attention**
        ä¸¾ä¾‹ï¼šThe animal didn't cross the street because it was too tiredï¼Œæ€ä¹ˆåˆ¤æ–­ it æŒ‡ä»£ animal è¿˜æ˜¯ street
        **first step: create three vectors from each of the encoderâ€™s input vectors**
        ![[Pasted image 20230323102910.png|525]]
        **second step: calculate a score** (We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position)
        ![[Pasted image 20230323103417.png|525]]
        **third step: Â divide the scores by 8**(the square root of the dimension of the key vectors used in the paper â€“ 64. This leads to having more stable gradients. There could be other possible values here, but this is the default)
        **forth step: softmax operation**
        ![[Pasted image 20230323103707.png|525]]
        **fifth step: multiply each value vector by the softmax score**
        **sixth step: sum up the weighted value vectors**. This produces the output of the self-attention layer at this position (for the first word).
        
		![[Pasted image 20231007160636.png]]
		æ³¨æ„åŠ›è®¡ç®—å¾—åˆ†ï¼š
		![[Pasted image 20231007160623.png]]
		![[Pasted image 20231007160805.png]]
		![[Pasted image 20231007160957.png]]



    **Multi-Head Self Attention**
    Transformer å°†æ¯ä¸ªæ³¨æ„åŠ›è®¡ç®—å•å…ƒç§°ä¸ºæ³¨æ„åŠ›å¤´ï¼ˆAttention Head ï¼‰ã€‚å¤šä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œè¿ç®—ï¼Œå³æ‰€è°“çš„å¤šå¤´æ³¨æ„åŠ›--Multi-head Attentionã€‚å®ƒé€šè¿‡èåˆå‡ ä¸ªç›¸åŒçš„æ³¨æ„åŠ›è®¡ç®—ï¼Œä½¿æ³¨æ„åŠ›è®¡ç®—å…·æœ‰æ›´å¼ºå¤§çš„åˆ†è¾¨èƒ½åŠ›ã€‚
    æ•´ä½“æ¡†æ¶
        ![[Pasted image 20230323104326.png|500]]
    åˆ†è§£
        ![[Pasted image 20230323104144.png|500]]
        ![[Pasted image 20230323104150.png|500]]
        ![[Pasted image 20230323104230.png|500]]
    

### decoder

2. Decoder
     The output of the top encoder is then transformed into a set of attention vectors K and V
     
     Self-Attention
        å½“å‰ç¿»è¯‘å’Œå·²ç»ç¿»è¯‘çš„å‰æ–‡ä¹‹é—´çš„å…³ç³»
        ä¸åŒäº Encoder ä¸­çš„ attentionï¼Œthe self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by **masking future positions**
     Encoder-Decoder Attention
        å½“å‰ç¿»è¯‘å’Œç¼–ç çš„ç‰¹å¾å‘é‡ä¹‹é—´çš„å…³ç³»ï¼Œjust like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.
        è¾“å‡ºï¼ša vector of floats
    ![[transformer_decoding.gif|600]]
     ![[transformer_decoding_2.gif|650]]
    ![[Pasted image 20231007154700.png|500]]
    
        
    Linear & Softmax
        è¾“å…¥ï¼šÂ a vector of floats from the decoder
        è¾“å‡ºï¼šturn that into a word
        - linearï¼šsimple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a **logits vector.**ï¼ˆa vector of width *vocab_size*ï¼‰
        - Softmax: The softmax layer then turns those scores into **probabilities** (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.
    ![[Pasted image 20230323133559.png|525]]

   

### embedding 
input embeddingï¼š
	è¾“å…¥åºåˆ—->æ˜ å°„æˆè¯æ±‡è¡¨çš„å•è¯ ID çš„æ•°å­—åºåˆ—->æ¯ä¸ªæ•°å­—åºåˆ—å°„æˆä¸€ä¸ªåµŒå…¥å‘é‡
output embeddingï¼šå³ç§»ä¸€ä¸ªä½ç½®ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ª$<start>$
	è®­ç»ƒ-å¯¹ target è¿›è¡Œ embeddingï¼›æ¨ç†-å¯¹ encoder çš„è¾“å‡ºè¿›è¡Œå…¶ embeddingï¼›
![[Pasted image 20231007151434.png]]

### position encoding

![[Pasted image 20231007151126.png|350]]
![[Pasted image 20231007151133.png|375]]

### çŸ©é˜µç»´åº¦

![[Pasted image 20231007152928.png|500]]


## è®­ç»ƒ&æ¨ç†

### è®­ç»ƒ
Transformer è®­ç»ƒçš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹æºåºåˆ—ä¸ç›®æ ‡åºåˆ—çš„å­¦ä¹ ï¼Œç”Ÿæˆç›®æ ‡åºåˆ—ã€‚
![[Pasted image 20231007140238.png]]

![[Pasted image 20231007140400.png]]

### æ¨ç†
![[Pasted image 20231007140549.png]]
![[Pasted image 20231007140557.png]]
![[Pasted image 20231007140707.png]]


**ä¸ºä»€ä¹ˆè®­ç»ƒçš„æ—¶å€™ä¸åƒæ¨ç†ï¼štarget é€è¯è¾“å…¥ï¼Ÿ
åŸå› ï¼š**
![[Pasted image 20231007143338.png]]


## Model usage
1. Encoder-decoder
   è¿™é€šå¸¸ç”¨äºåºåˆ—åˆ°åºåˆ—å»ºæ¨¡ï¼ˆä¾‹å¦‚ç¥ç»æœºå™¨ç¿»è¯‘ï¼‰ã€‚
2. encoder only
   ä»…ä½¿ç”¨ç¼–ç å™¨ï¼Œå¹¶ä¸”ç¼–ç å™¨çš„è¾“å‡ºè¢«ç”¨ä½œè¾“å…¥åºåˆ—çš„è¡¨ç¤ºã€‚è¿™é€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ (NLU) ä»»åŠ¡ï¼ˆä¾‹å¦‚æ–‡æœ¬åˆ†ç±»å’Œåºåˆ—æ ‡è®°ï¼‰ã€‚
3. decoder only
   ä»…ä½¿ç”¨è§£ç å™¨ï¼Œå…¶ä¸­ç¼–ç å™¨è§£ç å™¨äº¤å‰æ³¨æ„æ¨¡å—ä¹Ÿè¢«åˆ é™¤ã€‚è¿™é€šå¸¸ç”¨äºåºåˆ—ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼Œè¯­è¨€å»ºæ¨¡ï¼‰ã€‚

## transformer å˜ä½“
### æ¦‚è§ˆ
![[Pasted image 20231008135139.png]]

![[Pasted image 20231008141434.png]]


### sparse attention
#### position-based sparse
- ç¨€ç–æ³¨æ„åŠ›
![[Pasted image 20231008145540.png]]

- å¤åˆç¨€ç–æ³¨æ„åŠ›
![[Pasted image 20231008162240.png]]

- æ³¨æ„åŠ›çš„é€‰æ‹©
  å¯¹äºå…·æœ‰å‘¨æœŸæ€§çš„å›¾åƒï¼šband+strided
  å¯¹äºæ²¡æœ‰å‘¨æœŸæ€§çš„æ–‡æœ¬ï¼šblock+global

- å…¶ä»–æ³¨æ„åŠ›ï¼ˆextended attentionï¼‰
![[Pasted image 20231008163213.png]]

#### content-based sparse

å¦ä¸€é¡¹å·¥ä½œæ˜¯æ ¹æ®è¾“å…¥å†…å®¹åˆ›å»ºç¨€ç–å›¾ï¼Œå³ç¨€ç–è¿æ¥ä»¥è¾“å…¥ä¸ºæ¡ä»¶ã€‚æ„å»ºåŸºäºå†…å®¹çš„ç¨€ç–å›¾çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯é€‰æ‹©é‚£äº›å¯èƒ½ä¸ç»™å®šæŸ¥è¯¢å…·æœ‰è¾ƒå¤§ç›¸ä¼¼åº¦åˆ†æ•°çš„é”®ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ„å»ºç¨€ç–å›¾ï¼Œæˆ‘ä»¬å¯ä»¥é€’å½’åˆ°æœ€å¤§å†…ç§¯æœç´¢ï¼ˆMIPSï¼‰é—®é¢˜ï¼Œå…¶ä¸­å°è¯•é€šè¿‡æŸ¥è¯¢æ‰¾åˆ°å…·æœ‰æœ€å¤§ç‚¹ç§¯çš„é”®ï¼Œè€Œä¸è®¡ç®—æ‰€æœ‰ç‚¹ç§¯é¡¹ã€‚

æ–¹æ³•ï¼šk å‡å€¼èšç±»

### Linearized attention

é™ä½ attention çš„å¤æ‚åº¦

![[Pasted image 20231010154800.png]]
### Query prototyping and memory compression
1. å‡å°‘æŸ¥è¯¢
2. å‡å°‘ key-value pairs--å†…å­˜å‹ç¼©
   æ–¹æ³•ï¼šå†…å­˜å‹ç¼©æ³¨æ„åŠ› MCAï¼šå¯ä»¥å¤„ç†æ›´é•¿åºåˆ—ï¼›sources. Set Transformer (Lee et al., 2019) and Luna (Ma et al., 2021)ï¼›Linformer (Wang et al., 2020a)ï¼›Poolingformer (Zhang et al., 2021)ï¼›





## å‚è€ƒåšå®¢
[å‚è€ƒåšå®¢ï¼šThe Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)


## æ”¹è¿›
### multi-attention
[Adaptive Attention Span in Transformers](https://browse.arxiv.org/pdf/1905.07799.pdf)
1. å‡ºå‘ç‚¹
   ä¸åŒ headï¼Œç‰¹å¾æå–æ–¹å¼ä¸åŒï¼Œæœ‰äº›å› ä¸ºæ³¨æ„åŠ›ç‰¹å¾çš„è¡°å‡è€Œäº§ç”Ÿå·®è·ï¼š
   ![[Pasted image 20231007165555.png|500]]
   ![[Pasted image 20231007165620.png|500]]
   ![[Pasted image 20231007165704.png|550]]
### self-attention
[Augmenting Self-attention with Persistent Memory](https://www.arxiv-vanity.com/papers/1907.01470/)

é€šè¿‡ä¼˜åŒ– FFN å±‚æ¥å‡å°‘æ¨¡å‹ç‰¹å¾çš„å‚æ•°æ•°é‡

![[Pasted image 20231007193511.png]]

