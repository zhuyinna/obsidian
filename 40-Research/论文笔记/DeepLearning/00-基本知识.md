

## Softmax 函数
  
  在数学，尤其是概率论和相关领域中，**Softmax 函数**，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的 K 维向量 $z$  “压缩”到另一个 K 维实向量 $\sigma(x)$ 中，使得每一个元素的范围都在(0,1) 之间，并且所有元素的和为1(也可视为一个 (k-1)维的 hyperplane 或 subspace)。该函数的形式通常按下面的式子给出：
  通常的意义：对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。
![[Pasted image 20230322151446.png|425]]

## 训练参数
#Todo
### batchsize

### epoch

### interval

### optimizer

### learning rate

## 迁移学习

迁移学习(Transfer learning) 顾名思义就是把已训练好的模型（预训练模型）参数迁移到新的模型来帮助新模型训练。考虑到大部分数据或任务都是存在相关性的，所以通过迁移学习我们可以将已经学到的[模型参数](https://www.zhihu.com/search?q=%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。

- **Transfer Learning**
    冻结预训练模型的全部卷积层，只训练自己定制的全连接层。
- **Extract Feature Vector**
    先计算出预训练模型的卷积层对所有训练和[测试数据](https://www.zhihu.com/search?q=%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)的[特征向量](https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)，然后抛开预训练模型，只训练自己定制的简配版[全连接网络](https://www.zhihu.com/search?q=%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)。
- **Fine-tuning**
    冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这些层保留了大量底层信息）甚至不冻结任何网络层，训练剩下的卷积层（通常是靠近输出的部分卷积层）和[全连接层](https://www.zhihu.com/search?q=%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)。
    原理：利用已知的网络结构和已知的网络参数，修改 output 层为我们自己的层，微调最后一层前的若干层的参数，这样就有效利用了[深度神经网络](https://www.zhihu.com/search?q=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)强大的[泛化能力](https://www.zhihu.com/search?q=%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A798566708%7D)，又免去了设计复杂的模型以及耗时良久的训练，所以 fine tuning 是当数据量不足时的一个比较合适的选择。
    意义：- 站在巨人肩膀上，不用重复造轮子；- 训练成本很低，甚至可以用 CPU；- 适用于小数据集；


# 网络组成

## backbone
这个主干网络大多时候指的是提取特征的网络，其作用就是提取图片中的信息，共后面的网络使用。这些网络经常使用的是 resnet VGG 等，而不是我们自己设计的网络，因为这些网络已经证明了在分类等问题上的特征提取能力是很强的。在用这些网络作为 backbone 的时候，都是直接加载官方已经训练好的模型参数，后面接着我们自己的网络。让网络的这两个部分同时进行训练，因为加载的 [backbone模型](https://www.zhihu.com/search?q=backbone%E6%A8%A1%E5%9E%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2328543924%7D)已经具有提取特征的能力了，在我们的训练过程中，会对他进行微调，使得其更适合于我们自己的任务。

## head
head 是获取网络输出内容的网络，利用之前提取的特征，head 利用这些特征，做出预测。

## neck
是放在 backbone 和 head 之间的，是为了更好的利用 backbone 提取的特征

## bottleneck
瓶颈的意思，通常指的是网网络输入的数据维度和输出的维度不同，输出的维度比输入的小了许多，就像脖子一样，变细了。经常设置的参数 bottle_num=256，指的是网络输出的数据的维度是256 ，可是输入进来的可能是1024维度的。

## GAP
Global Average Pool 全局平均池化。将某个通道的特征取平均值，经常使用 AdaptativeAvgpoold(1),在 pytorch 中，这个代表自适应性全局平均池化，说人话就是将某个通道的特征取平均值

## embedding
深度学习方法都是利用使用线性和非线性转换对复杂的数据进行自动特征抽取，并将特征表示为“向量”


# 其他

## pretext task
用于预训练的任务被称为[前置/代理任务](https://www.zhihu.com/search?q=%E5%89%8D%E7%BD%AE%2F%E4%BB%A3%E7%90%86%E4%BB%BB%E5%8A%A1&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2328543924%7D) (pretext task)

## downstream task
用于微调的任务被称为下游任务(downstream task)

## warm up
热身 Warm up。Warm up 指的是用一个小的学习率先训练几个 [epoch](https://www.zhihu.com/search?q=epoch&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2328543924%7D)，这是因为网络的参数是随机初始化的，一开始就采用较大的学习率容易数值不稳定。

## end-to-end
输入一张图，输出想要的结果。算法细节和学习过程全部丢给了神经网络。


## 聚类算法
### K-means
1. 首先选择 _K_ 个随机的点，称为聚类中心（Cluster centroids）；
2. 对于数据集中的每一个数据，分别计算其与 _K_ 个中心点的距离，选择距离最近的中心点。将该数据与此中心点关联起来。所有与同一个中心点关联的所有点聚成一类。
3. 计算每一组的平均值，将该组所关联的中心点移动到平均值的位置。
