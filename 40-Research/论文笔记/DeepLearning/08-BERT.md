
## 总体评价

BERT 是 18 年提出的全新的**预训练语言模型**，使用了 transformer 模型的 encoder 层来进行特征的提取，采用了预训练+fine-tuning 的训练模式，通过**Masked LM 任务和 Next Sentence Prediction 任务**来学习深度单词级和句子级的特征，在不同的下游任务上通过 fine-tuning 的方式训练和测试，以此得到最终的模型和实验结果。

Bert 对 NLP 来说有非常重要的意义，它作为一个**骨架级**的模型，采用大量的无标签数据充分的训练学习了字符级、单词级、句子级甚至句间关系的特征，以至于在不同的下游 NLP 任务中，只需要去为 bert 在特定任务中**定制一个非常轻量级的输出层（比如一个单层 MLP）** 就可以取得很好的结果。

## 其他语言模型

1. EMLO
     - feature-based


2. GPT