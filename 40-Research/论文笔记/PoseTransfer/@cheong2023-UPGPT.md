---
Authors: Soon Yau Cheong; Armin Mustafa; Andrew Gilbert
Date: 2023-07-26
Topics: pose-transfer
DOI: 10.48550/arXiv.2304.08870
Keywords:
---
tags: #论文笔记 

# UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer


## Abstract
Text-to-image models (T2I) such as StableDiffusion have been used to generate high quality images of people. However, due to the random nature of the generation process, the person has a different appearance e.g. pose, face, and clothing, despite using the same text prompt. The appearance inconsistency makes T2I unsuitable for pose transfer. We address this by proposing a multimodal diffusion model that accepts text, pose, and visual prompting. Our model is the first unified method to perform all person image tasks - generation, pose transfer, and mask-less edit. We also pioneer using small dimensional 3D body model parameters directly to demonstrate new capability - simultaneous pose and camera view interpolation while maintaining the person's appearance.

## Files and Links
- **Url**: [Open online](http://arxiv.org/abs/2304.08870)
- **zotero entry**: [Zotero](zotero://select/library/items/48DPHD2D)
- **open pdf**: [arXiv.org Snapshot](zotero://open-pdf/library/items/ADXM7EBM); [Cheong et al_2023_UPGPT.pdf](zotero://open-pdf/library/items/8MW2XX48)

## Comments


---

## Summary

  
## Research Objective(s)

![[Pasted image 20240117150850.png|450]]


## Background / Problem Statement


## Method(s)

![[Pasted image 20240117150832.png]]


## Evaluation

### Text-Pose Guided Image Generation
![[Pasted image 20240117151103.png]]

### Pose Transfer

- DeepFashion[53] In-shop Clothes Retrieval dataset 
- 256×176
- 性能不如PIDM
![[Pasted image 20240117151133.png|475]]


## Conclusion


## Notes


----

## Extracted Annotations

