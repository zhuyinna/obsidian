---
Authors: Yufei Xu; Jing Zhang; Qiming Zhang; Dacheng Tao
Date: 2022-10-12
Topics: 2D; HPE
DOI: 10.48550/arXiv.2204.12484
Keywords:
---
tags: #è®ºæ–‡ç¬”è®° 

# ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation


## Abstract
Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose.

## Files and Links
- **Url**: [Open online](http://arxiv.org/abs/2204.12484)
- **zotero entry**: [Zotero](zotero://select/library/items/L38J526Y)
- **open pdf**: [arXiv.org Snapshot](zotero://open-pdf/library/items/RH2NY9CT); [Xu et al_2022_ViTPose.pdf](zotero://open-pdf/library/items/FTNRCR4L)

## Comments


---

## Summary

  
## Research Objective(s)


## Background / Problem Statement


## Method(s)


## Evaluation


## Conclusion


## Notes


----

## Extracted Annotations

æ³¨é‡Š(2023/4/19 ä¸‹åˆ9:09:34)

<mark style="background: green;">Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation.</mark> [(Xu ç­‰, 2022, p. 1)](zotero://open-pdf/library/items/FTNRCR4L?page=1&annotation=YSWKGTTW)   
> ğŸ”¤å…·ä½“æ¥è¯´ï¼ŒViTPose ä½¿ç”¨æ™®é€šå’Œéåˆ†å±‚è§†è§‰è½¬æ¢å™¨ä½œä¸ºä¸»å¹²æ¥æå–ç»™å®šäººç‰©å®ä¾‹çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§è§£ç å™¨è¿›è¡Œå§¿åŠ¿ä¼°è®¡ã€‚ğŸ”¤Vison Transformer åœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­æ•ˆæœä¼˜ç§€ï¼Œåœ¨è¯†åˆ«ä½†è¿˜æ²¡æœ‰äººåœ¨å§¿æ€ä¼°è®¡ä»»åŠ¡ä¸ŠéªŒè¯è¿™ç§ç»“æ„çš„æœ‰æ•ˆæ€§ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†åä¸ºVitPoseçš„ç”¨äºå§¿æ€ä¼°è®¡çš„Transformerç½‘ç»œï¼Œä½¿ç”¨æ™®é€šViTç»“æ„ä½œä¸ºBackboneï¼Œç»“åˆä¸€ä¸ªè½»é‡çº§çš„Decoderï¼Œå°±èƒ½åœ¨MS COCO å…³é”®ç‚¹ä¼°è®¡bechmarkä¸Šè¾¾åˆ°SOTAã€‚  

<mark style="background: yellow;">To this end, we simply append several decoder layers after the transformer backbone to estimate the heatmaps w.r.t. the keypoints, as shown in Fig. 2 (a)</mark> [(Xu ç­‰, 2022, p. 3)](zotero://open-pdf/library/items/FTNRCR4L?page=3&annotation=TE5REUDQ)   
> ğŸ”¤ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç®€å•åœ°åœ¨å˜å‹å™¨ä¸»å¹²ä¹‹åé™„åŠ å‡ ä¸ªè§£ç å™¨å±‚æ¥ä¼°è®¡çƒ­å›¾ w.r.tã€‚å…³é”®ç‚¹ï¼Œå¦‚å›¾2ï¼ˆaï¼‰æ‰€ç¤ºğŸ”¤  

<mark style="background: yellow;">e do not adopt skip-connections or cross-attentions in the decoder layers but simple deconvolution layers and a prediction layer,</mark> [(Xu ç­‰, 2022, p. 3)](zotero://open-pdf/library/items/FTNRCR4L?page=3&annotation=X39FUPKP)   
> ğŸ”¤e åœ¨è§£ç å±‚ä¸­ä¸é‡‡ç”¨è·³è·ƒè¿æ¥æˆ–äº¤å‰æ³¨æ„ï¼Œè€Œæ˜¯é‡‡ç”¨ç®€å•çš„åå·ç§¯å±‚å’Œé¢„æµ‹å±‚ï¼ŒğŸ”¤  

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/RYZ39JBS%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=3&annotation=CMLRZGDB)


> embedding:é™é‡‡æ ·då€æ•°(default:16)ï¼Œé€šé“æ•°3-&gt;C;transformer blockï¼šåŒ…å«multi-head self-attention(MHSA)å’Œä¸€ä¸ªfeed-forward network(FFN); [ ](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=XYT259F9) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/UHIB2ESF%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=XYT259F9)

<mark style="background: yellow;">The first one is the classic decoder. It is composed of two deconvolution blocks, each of which contains one deconvolution layer followed by batch normalization [19] and ReLU</mark> [(Xu ç­‰, 2022, p. 4)](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=BQT2TV5U)   
> ğŸ”¤ç¬¬ä¸€ä¸ªæ˜¯ç»å…¸è§£ç å™¨ã€‚å®ƒç”±ä¸¤ä¸ªåå·ç§¯å—ç»„æˆï¼Œæ¯ä¸ªåå·ç§¯å—åŒ…å«ä¸€ä¸ªåå·ç§¯å±‚ï¼Œç„¶åæ˜¯æ‰¹é‡å½’ä¸€åŒ– [19] å’Œ ReLUğŸ”¤  

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/FILZKQFA%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=N4BWTNJ3)

<mark style="background: yellow;">try another simpler decoder in ViTPose, which is proved effective thanks to the strong representation ability of the vision transformer backbone. Specifically, we directly upsample the feature maps by 4 times with bilinear interpolation, followed by a ReLU and a convolution layer with the kernel size 3 Ã— 3 to get the heatmaps</mark> [(Xu ç­‰, 2022, p. 4)](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=TL76VXZ6)   
> ğŸ”¤åœ¨ ViTPose ä¸­å°è¯•å¦ä¸€ä¸ªæ›´ç®€å•çš„è§£ç å™¨ï¼Œç”±äº vision transformer backbone çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ï¼Œå®ƒè¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åŒçº¿æ€§æ’å€¼å¯¹ç‰¹å¾å›¾è¿›è¡Œ 4 å€ä¸Šé‡‡æ ·ï¼Œç„¶åä½¿ç”¨ ReLU å’Œå†…æ ¸å¤§å°ä¸º 3 Ã— 3 çš„å·ç§¯å±‚æ¥è·å¾—çƒ­å›¾ğŸ”¤  


> ä¸æè¿°ä¸ç¬¦ã€‚æè¿°ä¸ºRELUå±‚åœ¨Bilinearå±‚åé¢ã€‚å…·ä½“çœ‹ä»£ç  [ ](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=EQVFS6UR) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/UKAFYCLR%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=4&annotation=EQVFS6UR)

<mark style="background: yellow;">We empirically demonstrate that with the MHSA module frozen, ViTPose obtains comparable performance to the fully finetuning setting.</mark> [(Xu ç­‰, 2022, p. 5)](zotero://open-pdf/library/items/FTNRCR4L?page=5&annotation=NZQTXVFE)   
> ğŸ”¤æˆ‘ä»¬å‡­ç»éªŒè¯æ˜ï¼Œåœ¨ MHSA æ¨¡å—å†»ç»“çš„æƒ…å†µä¸‹ï¼ŒViTPose è·å¾—äº†ä¸å®Œå…¨å¾®è°ƒè®¾ç½®ç›¸å½“çš„æ€§èƒ½ã€‚ğŸ”¤  

<mark style="background: green;">One common method to improve the performance of smaller models is to transfer the knowledge from larger ones, i.e., knowledge distillation [17, 14]. Specifically, given a teacher network T and student network S, a simple distillation method is to add an output distillation loss Lod tâ†’s to force the student networkâ€™s output imitating the teacher networkâ€™s output,</mark> [(Xu ç­‰, 2022, p. 5)](zotero://open-pdf/library/items/FTNRCR4L?page=5&annotation=3TR3YBYI)   
> ğŸ”¤æé«˜è¾ƒå°æ¨¡å‹æ€§èƒ½çš„ä¸€ç§å¸¸ç”¨æ–¹æ³•æ˜¯ä»è¾ƒå¤§æ¨¡å‹è½¬ç§»çŸ¥è¯†ï¼Œå³çŸ¥è¯†è’¸é¦ [17ã€14]ã€‚å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªæ•™å¸ˆç½‘ç»œTå’Œå­¦ç”Ÿç½‘ç»œSï¼Œä¸€ä¸ªç®€å•çš„è’¸é¦æ–¹æ³•æ˜¯æ·»åŠ ä¸€ä¸ªè¾“å‡ºè’¸é¦æŸå¤±Lod tâ†’sæ¥å¼ºåˆ¶å­¦ç”Ÿç½‘ç»œçš„è¾“å‡ºæ¨¡ä»¿æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºï¼ŒğŸ”¤è¿™ç¯‡è®ºæ–‡æ¯”è¾ƒæœ‰æ„æ€çš„ä¸€ä¸ªç‚¹æ˜¯æå‡ºäº†ä¸€ä¸ªåŸºäºTransformerçš„è’¸é¦æ–¹æ³•ï¼Œä¸å¸¸è§çš„ç”¨lossæ¥ç›‘ç£Teacherå’ŒStudentç½‘ç»œçš„æ€è·¯ä¸å¤ªä¸€æ ·ï¼Œå…·ä½“å¦‚ä¸‹: 1. åœ¨å¤§æ¨¡å‹çš„patch embeddingåçš„visual tokenåé¢å¢åŠ ä¸€ä¸ªçŸ¥è¯†tokenæ¨¡å—ï¼Œå¹¶è¿›è¡Œéšæœºåˆå§‹åŒ– 2. å›ºå®šå¤§æ¨¡å‹çš„å‚æ•°ï¼Œåªè®­ç»ƒçŸ¥è¯†tokenæ¨¡å— 3. å°†è®­ç»ƒå¥½çš„çŸ¥è¯†tokenæ¨¡å—æ¥åˆ°å°æ¨¡å‹çš„visual tokenåé¢ï¼Œä¸”å›ºå®šçŸ¥è¯†tokençš„å‚æ•°ï¼Œåªè®­ç»ƒå°æ¨¡å‹çš„å…¶ä»–å‚æ•°ã€‚é€šè¿‡è¿™æ ·çš„æµç¨‹ï¼Œå°†æ‰€æœ‰çš„çŸ¥è¯†éƒ½èåˆåˆ°äº†çŸ¥è¯†tokenæ¨¡å—çš„å‚æ•°é‡Œé¢ï¼Œå¹¶ä¸”ä»å¤§æ¨¡å‹ä¼ é€’åˆ°å°æ¨¡å‹  


> ä¸¤ç§decoderå¯¹æ¯”:1. ç»å…¸Decoderç»“æ„ï¼Œä¸¤ä¸ªDeconvï¼ˆ+BN+ReLU) + 1ä¸ª1x1 convï¼Œæ¯ä¸ªdeconvä¸Šé‡‡æ ·2å€ï¼Œæœ€ç»ˆè¾“å‡ºfeature mapå¤§å°ä¸ºè¾“å…¥çš„1/4å€2. åŒçº¿æ€§å·®å€¼ä¸Šé‡‡æ ·4å€ï¼Œç„¶åæ˜¯ReLU+3x3convï¼Œä¸è¿‡è®ºæ–‡ä¸­å…¬å¼ä¸æè¿°ä¸ç¬¦ï¼ŒReLUåœ¨åŒçº¿æ€§ä¸Šé‡‡æ ·ä¹‹å‰ï¼Œéœ€è¦çœ‹ä»£ç å®ç°å…·ä½“æ˜¯å“ªä¸€ç§æ–¹æ¡ˆ1éçº¿æ€§æ›´é«˜ï¼Œå› æ­¤åœ¨CNNçš„ç»“æ„ä¸­ä½¿ç”¨æ¯”è¾ƒå¤šã€‚è€Œè¿™ç¯‡è®ºæ–‡ä¹ŸéªŒè¯äº†ç”±äºTransformerå¼ºå¤§çš„å­¦ä¹ èƒ½åŠ›ï¼Œå³ä½¿åƒæ–¹æ¡ˆ2è¿™æ ·çš„çš„ç®€å•decoderï¼Œä¹Ÿèƒ½è¾¾åˆ°å¾ˆé«˜çš„ç²¾åº¦å¯ä»¥çœ‹åˆ°ï¼ŒResNetç³»åˆ—åœ¨æ–¹æ¡ˆ1ä¸Šçš„ç»“æœè¿œé«˜äºæ–¹æ¡ˆ2ï¼Œè¯´æ˜CNNç»“æ„çš„å­¦ä¹ èƒ½åŠ›éœ€è¦å¼ºæœ‰åŠ›çš„decoderæ¥è¿›ä¸€æ­¥åŠ å¼ºï¼Œè€ŒVitPoseç»“æ„åˆ™ä¸éœ€è¦ï¼Œè¿™éœ€è¦å½’åŠŸäºViTç»“æ„çš„å¼ºå¤§å­¦ä¹ èƒ½åŠ› [ ](zotero://open-pdf/library/items/FTNRCR4L?page=6&annotation=QPVE248H) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/W5ZD87B6%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=6&annotation=QPVE248H)


> é¢„è®­ç»ƒä¸Šçš„çµæ´»æ€§ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=EML6686N) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/D3YJEP27%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=EML6686N)


> åˆ†è¾¨ç‡ä¸Šçš„çµæ´»æ€§ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=64J9JWWR) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/XP5XS2IU%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=64J9JWWR)


> attention typeä¸Šçš„çµæ´»æ€§ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=C3SS7R6P) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/IQ4XERDW%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=7&annotation=C3SS7R6P)


> finetuneçš„çµæ´»æ€§ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=9RLQYTSA) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/YQETBGUX%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=9RLQYTSA)


> å¤šä»»åŠ¡ä¸Šçš„çµæ´»æ€§ï¼šä½œè€…è¿˜å°è¯•äº†è¿™æ ·ä¸€ä¸ªå®éªŒï¼Œé‡‡ç”¨åŒä¸€ä¸ªbackboneï¼Œå¤šä¸ªdecoderï¼Œæ¯ä¸ªdecoderå¯¹åº”ä¸€ä¸ªæ•°æ®é›†çš„ä»»åŠ¡ï¼Œå®éªŒéªŒè¯ä¸€æ¬¡è®­ç»ƒï¼Œå¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœéƒ½èƒ½æ¯”è¾ƒå¥½ï¼Œä¸”æ¯”å•ä¸ªæ•°æ®é›†ç²¾åº¦æœ‰æå‡ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=68FHNYQW) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/TJGZ7G7L%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=68FHNYQW)


> çŸ¥è¯†è’¸é¦æ–¹é¢çš„æ¶ˆèå®éªŒ [ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=RDEIUYAS) 

![](file://C:/Users/%E7%A5%9D%E9%93%B6%E9%82%A3/Documents/Obsidian%20Vault/.hidden/zotero/storage/KPYMT4KG%5Cimage.png)[ ](zotero://open-pdf/library/items/FTNRCR4L?page=8&annotation=RDEIUYAS)

<mark style="background: red;">complex decoders or FPN structures</mark> [(Xu ç­‰, 2022, p. 10)](zotero://open-pdf/library/items/FTNRCR4L?page=10&annotation=998UWF8N)   
> ğŸ”¤å¤æ‚è§£ç å™¨æˆ– FPN ç»“æ„ğŸ”¤  

- [ ] <mark style="background: blue;">In addition, we believe ViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45] and face keypoint detection</mark> [(Xu ç­‰, 2022, p. 10)](zotero://open-pdf/library/items/FTNRCR4L?page=10&annotation=U8DYKFRJ)  ğŸ”¤æ­¤å¤–ï¼Œæˆ‘ä»¬ç›¸ä¿¡ ViTPose ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–å§¿åŠ¿ä¼°è®¡æ•°æ®é›†ï¼Œä¾‹å¦‚åŠ¨ç‰©å§¿åŠ¿ä¼°è®¡ [47, 9, 45] å’Œäººè„¸å…³é”®ç‚¹æ£€æµ‹ğŸ”¤ 



