

# 生成模型大类

## 自回归模型(Autoregressive Models)
自回归模型是一种生成模型，它通过对数据的先前部分进行建模来预测下一个数据点。典型的自回归模型包括RNN和Transformer.

## 生成对抗网络(Generative Adversarial Networks，GANS)
由生成器和判别器组成，通过对抗训练的方式学习生成数据。生成器试图生成逼真的样本，而判别器则试图区分生成的样本和真实样本。Generator接收一个随机噪声向量作为输入，并通过网络结构将其映射到生成样本空间。噪声向量可以被视为生成器的输入信号，它在训练过程中被用来生成样本使生成器能够学习生成与真实样本相似的数据。

## 变分自编码器(Variational Autoencoders，VAES)
是一种基于概率编码和解码的生成模型。编码器将输入数据转换为潜在变量(通常是高斯分布)的均值和方差参数，并从潜在变量中采样来生成样本。

## 流模型(Flow Models)
通过定义一个可逆的变换将简单的先验分布映射到复杂的后验分布，从而实现样本的生成。流模型具有可解析的概率密度函数，可以进行高效的样本生成和概率推断。

## 概率扩散模型(Diffusion Probabilistic Models，DPMs)
通过迭代地应用概率扩散过程来生成样本如DDPM和常用于Stable Diffusion的DDIM与DPM++。DPMs也使用噪声作为输入。DPMS通过迭代的方式对声进行降，从而生成更准确的样本。噪声在这里被视为对真实样本的干扰，通过去除噪声来逐步恢复真实样本的信号。

### DDPM

1. **DDPM**
DDPM是一种概率扩散模型，它通过迭代地应用概率扩散过程来生成样本。DDPM使用噪声作为输入，并通过迭代的方式对噪声进行降，从而生成更准确的样本。噪声在这里被视为对真实样本的干扰，通过去除噪声来逐步恢复真实样本的信号。

2. **DDPM的数学形式**

2.1 加噪

$$
x_t=\alpha x_{t-1}+\beta\epsilon_t
$$

其中$\alpha_t^2+\beta_t^2=1$
    
迭代:
$$
\begin{aligned}
x_t=&\alpha_t x_{t-1}+\beta_t\epsilon_t \\
   =&\alpha_{t}(\alpha_{t-1}x_{t-2}+\beta_{t-1}\epsilon_{t-1})+\beta_{t}\epsilon_t \\
   =& \dots \\
   =&(\alpha_1\dots\alpha_1)x_0+(\alpha_1\dots\alpha_2)\beta_1\epsilon_1+\dots+\alpha_{t}\beta_{t-1}\epsilon_{t-1}+\beta_t\epsilon_t
\end{aligned}   
$$
由于
$$
\alpha_t^2+\beta_t^2=1
$$
那么 
$$(\alpha_t\dots\alpha_2)\beta_1\epsilon_1+\dots+\alpha_{t}\beta_{t-1}\epsilon_{t-1}+\beta_t\epsilon_t$$
即为多个独立正态噪声之和, 均值为0,  方差为 
$$(\alpha_t\dots\alpha_2)^2\beta_1^2$$ 
$$(\alpha_t\dots\alpha_3)^2\beta_2^2$$
$$\dots$$
$$\beta_t^2$$

叠加这些正态分布, 最终得到的是均值为0,   方差为
$$(\alpha_t\dots\alpha_2)^2\beta_1^2+(\alpha_t\dots\alpha_3)^2\beta_2^2+\dots+\beta_t^2$$
的正态分布, 由于
$$\alpha_t^2+\beta_t^2=1$$
可以得到:
$$(\alpha_t\dots\alpha_1)^2+(\alpha_t\dots\alpha_2)^2\beta_1^2+(\alpha_t\dots\alpha_3)^2\beta_2^2+\dots+\beta_t^2$$
注意上式多了最开始的一项

所以最终方差和为:
$$(\alpha_t\dots\alpha_2)^2\beta_1^2+(\alpha_t\dots\alpha_3)^2\beta_2^2+\dots+\beta_t^2 = 1 - (\alpha_t\dots\alpha_1)^2$$

相当于有:

$$
x_t = (\alpha_1\dots\alpha_1)x_0 + \sqrt{1 - (\alpha_t\dots\alpha_1)^2}\overline\epsilon_t
$$

2.2 去噪

虽然可以直接通过x0得到xt, 并对xt通过网络得到相应噪声, 但是如果直接一步计算得到xt, 需要用到$\overline{\epsilon_t}$, 而$\epsilon_t$已经通过采样得到, $\epsilon_t$和$\overline\epsilon_{t}$并不是相互独立的, 所以不能同时采样两个(否则就变成采样两个独立变量了), 所以如果用了$\epsilon_t$就不能再直接用$\overline\epsilon_{t}$来计算xt, 只能通过xt-1加噪来得到
$$
\begin{aligned}
x_t = & \alpha_t x_{t-1}+\beta_t\epsilon_t \\
    = & \alpha_t (\overline{\alpha_{t-1}} x_0+\overline{\beta_{t-1}}\overline{\epsilon_{t-1}})+ \beta_t\epsilon_t\\
    = & \overline{\alpha_t}x_0 + \alpha_t\overline{\beta_{t-1}}\overline{\epsilon_{t-1}}+\beta_t\epsilon_t \\
\end{aligned}
$$

损失函数:

$$
||\epsilon_t-\epsilon_{\theta}(\overline{\alpha_t}x_0 + \alpha_t\overline{\beta_{t-1}}\overline{\epsilon_{t-1}}+\beta_t\epsilon_t,t) ||^2
$$

上式包含了4个需要采样的随机变量:
-  从所有训练样本中采样x0
-  从正态分布采样$\overline{\epsilon_{t-1}}$, $\epsilon_{t}$
-  从1~T中采样一个t

采样的随机变量越多,反差越大, 收敛慢, 所以实际通过积分技巧来将$\overline{\epsilon_{t-1}}$, $\epsilon_{t}$进行合并成单个正太随机变量

(太复杂了..省略)

最终得到DDPM的损失函数为:

$$
||\epsilon-\frac{\overline{\beta_t}}{\beta_t}\epsilon(\overline{\alpha}_tx_0+\overline{\beta}_t\epsilon,t)||
$$

3. **递归生成**

$$
x_{t-1}=\frac{1}{\alpha_t}(x_t-\beta_t\epsilon_\theta(x_{t},t))
$$

如果要进行random sample,需要补上噪声项:
$$
x_{t-1}=\frac{1}{\alpha_t}(x_t-\beta_t\epsilon_\theta(x_{t},t)) + \sigma_tz
$$

上式显式的给了t, 是因为原则上不同的t处理的是不同层次的对象, 所以应该用不同的重构模型, 即应该有T个不同的重构模型才对, 所以我们共享了所有重构模型的参数, 将t作为条件传入. 论文中, t是转换为transformer中类似的位置编码后, 直接加到残差模块

4. **为什么T=1000? 以及$\alpha_t$的选择?**
- 使用欧式距离来衡量两张图片, 会导致模糊, 所以尽可能两张图片要相近, 那么最开始加噪的图片比较接近原始图片, 所以alpha尽可能大, 随着图片逐渐接近噪声, alpha可以开始减小
$$
\alpha_t = \sqrt{1-\frac{0.02t}{T}}
$$

$$
log\overline{\alpha_t}=\sum_{t=1}^{T}log\alpha_t=\frac{1}{2}\sum_{t=1}^{T}log(1-\frac{0.02t}{T})<\frac{1}{2}\sum_{t=1}^{T}(-\frac{0.02t}{T})=-0.005(T+1)
$$

带入T=1000, 大致$\overline{\alpha}_t\approx e^{-5}$, $\approx 0$, 如果从头到尾$\alpha$都很大, 那么T也相应要很大

5. 缺陷
在对$p_\theta(x_{t-1}|x_t,x_0)$的推导中, 使用了一阶markov假设, 使得$p_\theta(x_{t-1}|x_t,x_0)=p_\theta(x_{t-1}|x_t)$, 因此重建步很长, 速度慢. 为什么不能跳步? 因为在分布均值的方法用的是贝叶斯公式:
$$
P(x_{t-1}|x_t,x_0) = \frac{P(x_t|x_{t-1})P(x_{t-1}|x_0)}{P(x_t|x_0)}
$$
其中用到了马尔可夫性质, 所以采样必须严格遵守马尔可夫, 一步一步来

那为什么不能直接通过x_t和epsilon直接得到x0? 也不行, 理由同上

### DDIM

不再限制一阶markov, 使得重建步可以很短, 速度快, 所以贝叶斯公式要改写为:

$$
P(x_{t-1}|x_t,x_0)=\frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}
$$
这个时候等号右边三个分布我们都不知道了.


1. 推导
$$
x_{t-1}=\sqrt{\alpha_t}(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta^{(t)}(x_t)}{\sqrt{\alpha_t}})+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\epsilon_\theta^{(t)}(x_t)+\sigma_t\epsilon_t
$$
其中第一项为预测的x0, 第二项为direction poiting to x_t, 第三项为噪声项, 噪声想中的$\epsilon_t$是从标准正态分布中采样的, 与x_t无关, 而$\epsilon_\theta^{(t)}(x_t)$是从网络中采样的, 两者是不同的, 但是由于网络的参数是固定的, 所以$\epsilon_\theta^{(t)}(x_t)$是确定的, 所以可以将两者合并成一个正态分布, 从而减少采样的随机变量. 
- 如果$\sigma_t^2=\beta_t$, 采样过程和DDPM完全一样
- 如果$\sigma_t^2=0$, 采样过程没有随机噪声, 是一个确定性过程, 也称为DDIM, 即一旦给定初始噪声x_T, DDIM的最终生成的结果也确定



2. 加速采样
- 如何去马尔科夫化?
假设$P(x_{t-1}|x_t,x_0)\sim N(kx_0+mx_t,\sigma^2I)$, 于是有
$$
x_{t-1} = kx_0+mx_t+\sigma\epsilon
$$
由于假设仍然成立, 即:

$$
x_t = \sqrt{\overline{{\alpha_t}}}x_{0}+\sqrt{1-\overline{\alpha_t}}\epsilon^{'}
$$



$$
\left\{
\begin{aligned}
x_t = & \sqrt{\overline{{\alpha_t}}}x_{0}+\sqrt{1-\overline{\alpha_t}}\epsilon^{'} \\
x_{t-1} = & kx_0+m(\sqrt{\overline{{\alpha_t}}}x_{0}+\sqrt{1-\overline{\alpha_t}}\epsilon^{'})+\sigma\epsilon
\end{aligned}
\right.
$$














